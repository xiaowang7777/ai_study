{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de44f01c-a8a6-436b-b48b-0dd5104d5e9e",
   "metadata": {},
   "source": [
    "# PyTorch 线性模型实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bd6a74-07cc-4b53-8c9c-149ed4dfdd7f",
   "metadata": {},
   "source": [
    "有一组对未知温度单位到摄氏温度的映射数据，但当中存在一些噪声数据，需要排除这些噪声数据，并预测出近似的刻度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9657bee-0b55-4279-ad27-99764e56bd16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "t_c = [0.5, 14.0, 15.0, 28.0, 11.0, 8.0, 3.0, -4.0, 6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47758414-d2f7-43a2-b7e7-78d768afa7d1",
   "metadata": {},
   "source": [
    "在这里， `t_c` 值是以摄氏度为单位的温度，而 `t_u` 值是未知的单位。可以预期的是两组数据中的噪声来自设备本身和近似的刻度值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba8937-781c-4557-9d9d-2aeb59734450",
   "metadata": {},
   "source": [
    "## 模型准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d3343-cc78-4f3c-a102-17572f226994",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 可视化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaa24d4-21f0-4358-a091-3fe15de6dd5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (3.7.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (4.39.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (9.5.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.0.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.23.4)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/wangjunfeng/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed6419d-2f88-44d5-9af5-84952ac3b199",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1292259f0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2560x1920 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(dpi=400)\n",
    "plt.xlabel(\"Temperture(Celsius)\")\n",
    "plt.ylabel(\"Temperture(Unknown)\")\n",
    "plt.plot(t_c.numpy(), t_u.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487f45c-8fb6-4d3b-928e-c871b2571214",
   "metadata": {},
   "source": [
    "从可视化数据出的图像可以看出数据整体是有规律的，但是存在噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502d967d-001c-40c3-bce4-6c4cd5cbe24c",
   "metadata": {},
   "source": [
    "### 选择模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0133aa51-680e-4334-8072-d51429316c03",
   "metadata": {},
   "source": [
    "在缺乏进一步了解的情况下，按照之前的可视化数据结果可以推测两个数据集是线性相关的————也就是说，将 `t_u` 乘一个因子，再加上一个长度就可以得到摄氏温度（忽略一定的误差）。\n",
    "\n",
    "`t_c = w * t_u + b`\n",
    "\n",
    "在上面的线性模型中，分别将权重和偏置名为 `w` 和 `b` ，现在就需要基于现有的数据来评估模型中的 `w` 和 `b` 参数。\n",
    "\n",
    "目前我们有一个带有一些未知参数的模型，我们需要估计这些参数，使预测值和测量值之间的误差尽可能的小。预测值和测量值之间的误差也需要精确的定义，这种函数一般都被称为**损失函数**，如果误差很大，则说明损失函数的值大了。理想情况下应尽可能使损失函数的值交小，这样预测值和测量值二者可以完美匹配。因此，我们的优化过程应该是以找到 `w` 和 `b` 为目标，使损失函数的值处于最小值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0323c7-1fcf-4491-87c8-9aa5f78824f7",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448f83e-9842-41ac-aec8-9b529aaf5316",
   "metadata": {},
   "source": [
    "**损失函数**（或代价函数）是一个计算单个数值的函数，学习过程将试图使其值最小化。损失的计算通常涉及一些*训练样本的期望输出*与*输入这些样本时模型实际产生的输出*之间的差值。在我们的这个个例子中，它将是模型输出的预测温度 `t_p` 与实际测量值之间的差值，即 `t_p-t_c`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27696f29-af7e-4389-a64f-c4db3824c34b",
   "metadata": {},
   "source": [
    "### 选取合适的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e2d7d-dfe2-4126-a4bd-7016e20134d0",
   "metadata": {},
   "source": [
    "我们需要确保损失函数在 `t_p` 大于或小于真正的 `t_c` 时损失都为正，因为我们的目标时让 `t_p` 匹配 `t_c` (`t_p` 和 `t_c` 的差值尽可能的小)。目前有两个最直接的选择： `|t_p - t_c|` 和 `(t_p - t_c)^2` 。这两个损失函数在零点处都有一个明显的最小值，并且随着预测值在两个方向上远离真实值而单调递增。另外绝对值在收敛的地方存在一个不明确的导数，虽然这并不是什么大问题，但目前还是选在第二个平方差作为损失函数。另外，平方差比绝对差对错误结果的惩罚更大，通常有更多轻微错误的结果比有少量严重错误的结果更好。\n",
    "\n",
    "接下来先定义模型函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b544c77b-90af-49a3-82b2-944280c3e0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(t_u, w, b):\n",
    "    return w * t_u + b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb9660e-58a5-43e4-ae9f-cc6b0849e220",
   "metadata": {},
   "source": [
    "我们期望 `t_u` 、 `w` 和 `b` 分别作为输入张量、权重参数和偏置参数。在我们的模型中，参数是 `PyTorch` 标量（也称为 0 维张量），通过乘积运算和广播生成返回的张量。接下来就要确定损失函数了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18eef18-1fdc-485f-91c6-a5cc15562e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c) ** 2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ee339-3cfa-4e58-b3a6-e9acd65c7b48",
   "metadata": {},
   "source": [
    "在这个损失函数中，先构建了一个差分张量，然后对其值进行平方处理，最后通过对得到的张量中的所有元素求一次平均值得到一个标量损失函数，即均方损失函数。\n",
    "\n",
    "我们现在可以初始化参数，调用模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0031506-a434-41ab-a313-4a16a993c45a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(())\n",
    "b = torch.zeros(())\n",
    "\n",
    "t_p = model(t_u, w, b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86e688b-817d-4755-84eb-a4d99a179c1d",
   "metadata": {},
   "source": [
    "检查损失的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f069cb-14c0-43fd-aff3-754d8d077333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd638edf-08d9-42e3-a9dc-1198b6ce06f1",
   "metadata": {},
   "source": [
    "## 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4924dcd6-339d-40b5-9686-ca3e154223cd",
   "metadata": {},
   "source": [
    "### 更新步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276c5814-9368-4217-b282-c8843106173d",
   "metadata": {},
   "source": [
    "梯度下降的基本思想是计算各参数的损失变化率，并在减小损失变化率的方向上修改各参数。我们可以通过在 `w` 和 `b` 上加一个小数字来估计变化率，然后看看损失在这附近的变化有多大。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a836240-df36-4690-9796-a71daf1cb64c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "loss_rate_of_change_w = (loss_fn(model(t_u, w + delta, b), t_c) - loss_fn(model(t_u, w - delta, b), t_c)) / (2.0 * delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af958f4-fd3c-499b-8946-9d20917ad89b",
   "metadata": {},
   "source": [
    "上面代码的含义是：在当前的 `w` 和 `b` 的值附近， `w` 的变化会导致损失的一些变化。如果变化是负的，那么就需要增加 `w` 的值，如果变化是正的，那么就需要减小 `w` 的值。那么值具体增加或减少多少呢？对 `w` 运用一个与损失成比例的变化是一个很好的主意。一般来说，缓慢的改变参数也是明智的，因为在距离当前 `w` 值邻域很远的地方，损失变化的速率可能显著不同，因此，我们通常用一个很小的比例因子来衡量变化率，这个因子有很多名称，在机器学习中被称为学习率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd14c3c0-3e24-4c06-990a-a152babdd9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1183a784-d9a2-4c73-910a-96ca80409337",
   "metadata": {},
   "source": [
    "我们可以对 `b` 采用与 `w` 相同的处理方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f1ea65-1333-4dc3-a01e-12f281af96cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_rate_of_change_b = (loss_fn(model(t_u, w, b + delta), t_c) - loss_fn(model(t_u, w, b - delta), t_c)) / (2.0 * delta)\n",
    "b = b - learning_rate * loss_rate_of_change_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0c251f-1aa6-43ed-b666-2706638718e1",
   "metadata": {},
   "source": [
    "以上操作表示梯度下降基本参数的更新步骤。重复上面的步骤，我们将收敛到给定数据上使损失最小的参数的最优值。但这个方法相当粗糙，接下来看看更好的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ca33d-6f89-48d4-b38a-6c1525929efb",
   "metadata": {},
   "source": [
    "### 更好的梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895283ad-b46c-47d8-832b-8b128c82757c",
   "metadata": {},
   "source": [
    "通过对模型和损失的重复评估来探测损失函数在 `w` 和 `b` 邻域的行为，计算变化率，这在参数较多的模型中不太合适。此外，我们也不总是清楚邻域又多大，在这个例子中，我们选择了 `delta = 0.1` ，但这完全取决于由 `w` 和 `b` 构成的损失函数的形状。如果与 `delta` 相比，损失太快，那么就无法很好的知道损失在哪个方向上减小的最快。\n",
    "\n",
    "如果我们能使邻域无限小，那就是最好的情况，这也正是分析所得到的损失对参数的导数时所发生的。在有 2 个或 2 个以上参数的模型中，我们计算每个参数的损失函数，并将它们放到一个导数向量中，即梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427426fb-0ea6-4441-ba46-d959f1d5f807",
   "metadata": {},
   "source": [
    "为了计算损失对一个参数的导数，我们可以使用链式法则，先计算损失对其输入（模型输出）的导数，再乘模型对参数的导数导数。例：\n",
    "\n",
    "`d(loss_fn) / d(w) = (d(loss_fn) / d(t_p)) * (d(t_p) / d(w))`\n",
    "\n",
    "由于我们的这个模型使用的是线性函数，损失为平方差，据此算出导数为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8513851d-77b6-4580-ba11-c7b0f44518f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2 * (t_p - t_c) / t_p.size(0)\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c2dac-659f-4d57-bcf9-818dc546f8fb",
   "metadata": {},
   "source": [
    "由模型可以得到以下导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b5f12e-7ffa-41b0-a13a-b3ed6fc7de6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型对参数 w 的导数\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c5c203-25f6-43ea-af64-06211bea8b76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型对参数 b 的导数\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492c5d0-0222-4593-98c6-adcbf5ae1209",
   "metadata": {},
   "source": [
    "把这些所有都放一起，返回关于 `w` 和 `b` 的损失梯度的函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b180a5d-0c0e-4760-9525-fbc8948cd25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)\n",
    "    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n",
    "    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982a458-eb80-41f2-a2a9-105357cf4f41",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f4d00-9a5e-44cc-b999-e10d692487ad",
   "metadata": {},
   "source": [
    "现在我们已经做好了优化参数的准备。从某参数的假定值开始，我们可以对它应用更新，进行固定次数的迭代，也可以直到 `w` 和 `b` 停止变化为止。\n",
    "\n",
    "我们一般把训练迭代称为迭代周期，在这个迭代周期，我们更新所有训练样本的参数。完整的训练循环如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eac1aed-f5d9-4bf0-bbf2-46a32ff5ad5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        w, b = params\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        \n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch %d,Loss %f'%(epoch,float(loss)))\n",
    "        print('\\tParams: %s'%params)\n",
    "        print('\\tGrad:%s'%grad)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f5c7a-169f-4a84-ba8a-e042e4946a47",
   "metadata": {},
   "source": [
    "接下来调用训练循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77c83920-1b78-4778-9472-79ef3e3c8b77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Loss 1763.884766\n",
      "\tParams: tensor([-44.1730,  -0.8260])\n",
      "\tGrad:tensor([4517.2964,   82.6000])\n",
      "Epoch 2,Loss 5802484.500000\n",
      "\tParams: tensor([2568.4011,   45.1637])\n",
      "\tGrad:tensor([-261257.4062,   -4598.9702])\n",
      "Epoch 3,Loss 19408029696.000000\n",
      "\tParams: tensor([-148527.7344,   -2616.3931])\n",
      "\tGrad:tensor([15109614.0000,   266155.6875])\n",
      "Epoch 4,Loss 64915905708032.000000\n",
      "\tParams: tensor([8589999.0000,  151310.8906])\n",
      "\tGrad:tensor([-8.7385e+08, -1.5393e+07])\n",
      "Epoch 5,Loss 217130525461053440.000000\n",
      "\tParams: tensor([-4.9680e+08, -8.7510e+06])\n",
      "\tGrad:tensor([5.0539e+10, 8.9023e+08])\n",
      "Epoch 6,Loss 726257583152928129024.000000\n",
      "\tParams: tensor([2.8732e+10, 5.0610e+08])\n",
      "\tGrad:tensor([-2.9229e+12, -5.1486e+10])\n",
      "Epoch 7,Loss 2429183416467662896627712.000000\n",
      "\tParams: tensor([-1.6617e+12, -2.9270e+10])\n",
      "\tGrad:tensor([1.6904e+14, 2.9776e+12])\n",
      "Epoch 8,Loss 8125122549611731432050262016.000000\n",
      "\tParams: tensor([9.6102e+13, 1.6928e+12])\n",
      "\tGrad:tensor([-9.7764e+15, -1.7221e+14])\n",
      "Epoch 9,Loss 27176882120842590626938030653440.000000\n",
      "\tParams: tensor([-5.5580e+15, -9.7903e+13])\n",
      "\tGrad:tensor([5.6541e+17, 9.9596e+15])\n",
      "Epoch 10,Loss 90901105189019073810297959556841472.000000\n",
      "\tParams: tensor([3.2144e+17, 5.6621e+15])\n",
      "\tGrad:tensor([-3.2700e+19, -5.7600e+17])\n",
      "Epoch 11,Loss inf\n",
      "\tParams: tensor([-1.8590e+19, -3.2746e+17])\n",
      "\tGrad:tensor([1.8912e+21, 3.3313e+19])\n",
      "Epoch 12,Loss inf\n",
      "\tParams: tensor([1.0752e+21, 1.8939e+19])\n",
      "\tGrad:tensor([-1.0937e+23, -1.9266e+21])\n",
      "Epoch 13,Loss inf\n",
      "\tParams: tensor([-6.2181e+22, -1.0953e+21])\n",
      "\tGrad:tensor([6.3256e+24, 1.1142e+23])\n",
      "Epoch 14,Loss inf\n",
      "\tParams: tensor([3.5962e+24, 6.3346e+22])\n",
      "\tGrad:tensor([-3.6584e+26, -6.4441e+24])\n",
      "Epoch 15,Loss inf\n",
      "\tParams: tensor([-2.0798e+26, -3.6636e+24])\n",
      "\tGrad:tensor([2.1158e+28, 3.7269e+26])\n",
      "Epoch 16,Loss inf\n",
      "\tParams: tensor([1.2028e+28, 2.1188e+26])\n",
      "\tGrad:tensor([-1.2236e+30, -2.1554e+28])\n",
      "Epoch 17,Loss inf\n",
      "\tParams: tensor([-6.9566e+29, -1.2254e+28])\n",
      "\tGrad:tensor([7.0769e+31, 1.2466e+30])\n",
      "Epoch 18,Loss inf\n",
      "\tParams: tensor([4.0233e+31, 7.0869e+29])\n",
      "\tGrad:tensor([-4.0929e+33, -7.2095e+31])\n",
      "Epoch 19,Loss inf\n",
      "\tParams: tensor([-2.3268e+33, -4.0987e+31])\n",
      "\tGrad:tensor([2.3671e+35, 4.1695e+33])\n",
      "Epoch 20,Loss inf\n",
      "\tParams: tensor([1.3457e+35, 2.3704e+33])\n",
      "\tGrad:tensor([-1.3690e+37, -2.4114e+35])\n",
      "Epoch 21,Loss inf\n",
      "\tParams: tensor([       -inf, -1.3709e+35])\n",
      "\tGrad:tensor([       inf, 1.3946e+37])\n",
      "Epoch 22,Loss inf\n",
      "\tParams: tensor([nan, inf])\n",
      "\tGrad:tensor([-inf, -inf])\n",
      "Epoch 23,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 24,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 25,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 26,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 27,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 28,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 29,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 30,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 31,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 32,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 33,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 34,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 35,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 36,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 37,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 38,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 39,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 40,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 41,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 42,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 43,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 44,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 45,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 46,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 47,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 48,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 49,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 50,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 51,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 52,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 53,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 54,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 55,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 56,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 57,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 58,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 59,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 60,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 61,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 62,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 63,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 64,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 65,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 66,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 67,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 68,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 69,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 70,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 71,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 72,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 73,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 74,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 75,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 76,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 77,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 78,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 79,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 80,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 81,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 82,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 83,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 84,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 85,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 86,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 87,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 88,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 89,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 90,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 91,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 92,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 93,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 94,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 95,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 96,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 97,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 98,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 99,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n",
      "Epoch 100,Loss nan\n",
      "\tParams: tensor([nan, nan])\n",
      "\tGrad:tensor([nan, nan])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs = 100, learning_rate = 1e-2, params = torch.tensor([1.0, 0]), t_u = t_u, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415d73fb-6754-4927-b279-cc5d0519b6b7",
   "metadata": {},
   "source": [
    "由上面的结果可以看出训练过程奔溃了，导致损失变成了无穷。导致奔溃的原因是参数接收到的更新太大了，他们的值开始来回波动，因为每次更新过度修正，就会导致下一次更新更加过度。这个过程是发散的而不是收敛的，而我们希望的是一个收敛的优化。解决的方法也很简单，选择一个更小的学习率即可。事实上当训练进行得不像我们希望的那样好时，我们通常会改变学习率，所以可以尝试使用 1e-3 或 1e-4 ，现在试试 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64f73ec6-f148-4060-b349-76927c1911fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Loss 1763.884766\n",
      "\tParams: tensor([ 0.5483, -0.0083])\n",
      "\tGrad:tensor([4517.2964,   82.6000])\n",
      "Epoch 2,Loss 323.090515\n",
      "\tParams: tensor([ 0.3623, -0.0118])\n",
      "\tGrad:tensor([1859.5493,   35.7843])\n",
      "Epoch 3,Loss 78.929634\n",
      "\tParams: tensor([ 0.2858, -0.0135])\n",
      "\tGrad:tensor([765.4666,  16.5122])\n",
      "Epoch 4,Loss 37.552845\n",
      "\tParams: tensor([ 0.2543, -0.0143])\n",
      "\tGrad:tensor([315.0790,   8.5787])\n",
      "Epoch 5,Loss 30.540283\n",
      "\tParams: tensor([ 0.2413, -0.0149])\n",
      "\tGrad:tensor([129.6733,   5.3127])\n",
      "Epoch 6,Loss 29.351154\n",
      "\tParams: tensor([ 0.2360, -0.0153])\n",
      "\tGrad:tensor([53.3495,  3.9682])\n",
      "Epoch 7,Loss 29.148884\n",
      "\tParams: tensor([ 0.2338, -0.0156])\n",
      "\tGrad:tensor([21.9304,  3.4148])\n",
      "Epoch 8,Loss 29.113848\n",
      "\tParams: tensor([ 0.2329, -0.0159])\n",
      "\tGrad:tensor([8.9964, 3.1869])\n",
      "Epoch 9,Loss 29.107145\n",
      "\tParams: tensor([ 0.2325, -0.0162])\n",
      "\tGrad:tensor([3.6721, 3.0930])\n",
      "Epoch 10,Loss 29.105247\n",
      "\tParams: tensor([ 0.2324, -0.0166])\n",
      "\tGrad:tensor([1.4803, 3.0544])\n",
      "Epoch 11,Loss 29.104168\n",
      "\tParams: tensor([ 0.2323, -0.0169])\n",
      "\tGrad:tensor([0.5781, 3.0384])\n",
      "Epoch 12,Loss 29.103222\n",
      "\tParams: tensor([ 0.2323, -0.0172])\n",
      "\tGrad:tensor([0.2066, 3.0318])\n",
      "Epoch 13,Loss 29.102295\n",
      "\tParams: tensor([ 0.2323, -0.0175])\n",
      "\tGrad:tensor([0.0537, 3.0291])\n",
      "Epoch 14,Loss 29.101379\n",
      "\tParams: tensor([ 0.2323, -0.0178])\n",
      "\tGrad:tensor([-0.0093,  3.0279])\n",
      "Epoch 15,Loss 29.100466\n",
      "\tParams: tensor([ 0.2323, -0.0181])\n",
      "\tGrad:tensor([-0.0353,  3.0274])\n",
      "Epoch 16,Loss 29.099548\n",
      "\tParams: tensor([ 0.2323, -0.0184])\n",
      "\tGrad:tensor([-0.0459,  3.0272])\n",
      "Epoch 17,Loss 29.098631\n",
      "\tParams: tensor([ 0.2323, -0.0187])\n",
      "\tGrad:tensor([-0.0502,  3.0270])\n",
      "Epoch 18,Loss 29.097717\n",
      "\tParams: tensor([ 0.2323, -0.0190])\n",
      "\tGrad:tensor([-0.0520,  3.0270])\n",
      "Epoch 19,Loss 29.096796\n",
      "\tParams: tensor([ 0.2323, -0.0193])\n",
      "\tGrad:tensor([-0.0528,  3.0269])\n",
      "Epoch 20,Loss 29.095881\n",
      "\tParams: tensor([ 0.2323, -0.0196])\n",
      "\tGrad:tensor([-0.0531,  3.0268])\n",
      "Epoch 21,Loss 29.094959\n",
      "\tParams: tensor([ 0.2323, -0.0199])\n",
      "\tGrad:tensor([-0.0533,  3.0268])\n",
      "Epoch 22,Loss 29.094049\n",
      "\tParams: tensor([ 0.2323, -0.0202])\n",
      "\tGrad:tensor([-0.0533,  3.0267])\n",
      "Epoch 23,Loss 29.093134\n",
      "\tParams: tensor([ 0.2323, -0.0205])\n",
      "\tGrad:tensor([-0.0533,  3.0267])\n",
      "Epoch 24,Loss 29.092216\n",
      "\tParams: tensor([ 0.2323, -0.0208])\n",
      "\tGrad:tensor([-0.0533,  3.0266])\n",
      "Epoch 25,Loss 29.091301\n",
      "\tParams: tensor([ 0.2323, -0.0211])\n",
      "\tGrad:tensor([-0.0533,  3.0266])\n",
      "Epoch 26,Loss 29.090385\n",
      "\tParams: tensor([ 0.2323, -0.0214])\n",
      "\tGrad:tensor([-0.0533,  3.0265])\n",
      "Epoch 27,Loss 29.089464\n",
      "\tParams: tensor([ 0.2323, -0.0217])\n",
      "\tGrad:tensor([-0.0533,  3.0265])\n",
      "Epoch 28,Loss 29.088551\n",
      "\tParams: tensor([ 0.2323, -0.0220])\n",
      "\tGrad:tensor([-0.0532,  3.0264])\n",
      "Epoch 29,Loss 29.087635\n",
      "\tParams: tensor([ 0.2323, -0.0223])\n",
      "\tGrad:tensor([-0.0533,  3.0264])\n",
      "Epoch 30,Loss 29.086714\n",
      "\tParams: tensor([ 0.2323, -0.0226])\n",
      "\tGrad:tensor([-0.0533,  3.0263])\n",
      "Epoch 31,Loss 29.085804\n",
      "\tParams: tensor([ 0.2324, -0.0229])\n",
      "\tGrad:tensor([-0.0532,  3.0262])\n",
      "Epoch 32,Loss 29.084888\n",
      "\tParams: tensor([ 0.2324, -0.0232])\n",
      "\tGrad:tensor([-0.0533,  3.0262])\n",
      "Epoch 33,Loss 29.083967\n",
      "\tParams: tensor([ 0.2324, -0.0235])\n",
      "\tGrad:tensor([-0.0533,  3.0261])\n",
      "Epoch 34,Loss 29.083057\n",
      "\tParams: tensor([ 0.2324, -0.0238])\n",
      "\tGrad:tensor([-0.0533,  3.0261])\n",
      "Epoch 35,Loss 29.082142\n",
      "\tParams: tensor([ 0.2324, -0.0241])\n",
      "\tGrad:tensor([-0.0532,  3.0260])\n",
      "Epoch 36,Loss 29.081221\n",
      "\tParams: tensor([ 0.2324, -0.0244])\n",
      "\tGrad:tensor([-0.0533,  3.0260])\n",
      "Epoch 37,Loss 29.080309\n",
      "\tParams: tensor([ 0.2324, -0.0247])\n",
      "\tGrad:tensor([-0.0533,  3.0259])\n",
      "Epoch 38,Loss 29.079390\n",
      "\tParams: tensor([ 0.2324, -0.0250])\n",
      "\tGrad:tensor([-0.0532,  3.0259])\n",
      "Epoch 39,Loss 29.078474\n",
      "\tParams: tensor([ 0.2324, -0.0253])\n",
      "\tGrad:tensor([-0.0533,  3.0258])\n",
      "Epoch 40,Loss 29.077562\n",
      "\tParams: tensor([ 0.2324, -0.0256])\n",
      "\tGrad:tensor([-0.0533,  3.0258])\n",
      "Epoch 41,Loss 29.076649\n",
      "\tParams: tensor([ 0.2324, -0.0259])\n",
      "\tGrad:tensor([-0.0533,  3.0257])\n",
      "Epoch 42,Loss 29.075731\n",
      "\tParams: tensor([ 0.2324, -0.0262])\n",
      "\tGrad:tensor([-0.0532,  3.0257])\n",
      "Epoch 43,Loss 29.074812\n",
      "\tParams: tensor([ 0.2324, -0.0265])\n",
      "\tGrad:tensor([-0.0533,  3.0256])\n",
      "Epoch 44,Loss 29.073895\n",
      "\tParams: tensor([ 0.2324, -0.0268])\n",
      "\tGrad:tensor([-0.0533,  3.0256])\n",
      "Epoch 45,Loss 29.072981\n",
      "\tParams: tensor([ 0.2324, -0.0271])\n",
      "\tGrad:tensor([-0.0533,  3.0255])\n",
      "Epoch 46,Loss 29.072069\n",
      "\tParams: tensor([ 0.2324, -0.0274])\n",
      "\tGrad:tensor([-0.0533,  3.0254])\n",
      "Epoch 47,Loss 29.071148\n",
      "\tParams: tensor([ 0.2324, -0.0277])\n",
      "\tGrad:tensor([-0.0533,  3.0254])\n",
      "Epoch 48,Loss 29.070234\n",
      "\tParams: tensor([ 0.2324, -0.0281])\n",
      "\tGrad:tensor([-0.0533,  3.0253])\n",
      "Epoch 49,Loss 29.069323\n",
      "\tParams: tensor([ 0.2325, -0.0284])\n",
      "\tGrad:tensor([-0.0533,  3.0253])\n",
      "Epoch 50,Loss 29.068401\n",
      "\tParams: tensor([ 0.2325, -0.0287])\n",
      "\tGrad:tensor([-0.0532,  3.0252])\n",
      "Epoch 51,Loss 29.067486\n",
      "\tParams: tensor([ 0.2325, -0.0290])\n",
      "\tGrad:tensor([-0.0533,  3.0252])\n",
      "Epoch 52,Loss 29.066566\n",
      "\tParams: tensor([ 0.2325, -0.0293])\n",
      "\tGrad:tensor([-0.0533,  3.0251])\n",
      "Epoch 53,Loss 29.065657\n",
      "\tParams: tensor([ 0.2325, -0.0296])\n",
      "\tGrad:tensor([-0.0533,  3.0251])\n",
      "Epoch 54,Loss 29.064741\n",
      "\tParams: tensor([ 0.2325, -0.0299])\n",
      "\tGrad:tensor([-0.0533,  3.0250])\n",
      "Epoch 55,Loss 29.063826\n",
      "\tParams: tensor([ 0.2325, -0.0302])\n",
      "\tGrad:tensor([-0.0532,  3.0250])\n",
      "Epoch 56,Loss 29.062910\n",
      "\tParams: tensor([ 0.2325, -0.0305])\n",
      "\tGrad:tensor([-0.0533,  3.0249])\n",
      "Epoch 57,Loss 29.061995\n",
      "\tParams: tensor([ 0.2325, -0.0308])\n",
      "\tGrad:tensor([-0.0532,  3.0249])\n",
      "Epoch 58,Loss 29.061079\n",
      "\tParams: tensor([ 0.2325, -0.0311])\n",
      "\tGrad:tensor([-0.0533,  3.0248])\n",
      "Epoch 59,Loss 29.060169\n",
      "\tParams: tensor([ 0.2325, -0.0314])\n",
      "\tGrad:tensor([-0.0533,  3.0248])\n",
      "Epoch 60,Loss 29.059248\n",
      "\tParams: tensor([ 0.2325, -0.0317])\n",
      "\tGrad:tensor([-0.0533,  3.0247])\n",
      "Epoch 61,Loss 29.058336\n",
      "\tParams: tensor([ 0.2325, -0.0320])\n",
      "\tGrad:tensor([-0.0533,  3.0247])\n",
      "Epoch 62,Loss 29.057415\n",
      "\tParams: tensor([ 0.2325, -0.0323])\n",
      "\tGrad:tensor([-0.0534,  3.0246])\n",
      "Epoch 63,Loss 29.056507\n",
      "\tParams: tensor([ 0.2325, -0.0326])\n",
      "\tGrad:tensor([-0.0533,  3.0245])\n",
      "Epoch 64,Loss 29.055586\n",
      "\tParams: tensor([ 0.2325, -0.0329])\n",
      "\tGrad:tensor([-0.0532,  3.0245])\n",
      "Epoch 65,Loss 29.054674\n",
      "\tParams: tensor([ 0.2325, -0.0332])\n",
      "\tGrad:tensor([-0.0533,  3.0244])\n",
      "Epoch 66,Loss 29.053761\n",
      "\tParams: tensor([ 0.2325, -0.0335])\n",
      "\tGrad:tensor([-0.0533,  3.0244])\n",
      "Epoch 67,Loss 29.052843\n",
      "\tParams: tensor([ 0.2325, -0.0338])\n",
      "\tGrad:tensor([-0.0533,  3.0243])\n",
      "Epoch 68,Loss 29.051929\n",
      "\tParams: tensor([ 0.2326, -0.0341])\n",
      "\tGrad:tensor([-0.0532,  3.0243])\n",
      "Epoch 69,Loss 29.051012\n",
      "\tParams: tensor([ 0.2326, -0.0344])\n",
      "\tGrad:tensor([-0.0533,  3.0242])\n",
      "Epoch 70,Loss 29.050098\n",
      "\tParams: tensor([ 0.2326, -0.0347])\n",
      "\tGrad:tensor([-0.0532,  3.0242])\n",
      "Epoch 71,Loss 29.049183\n",
      "\tParams: tensor([ 0.2326, -0.0350])\n",
      "\tGrad:tensor([-0.0533,  3.0241])\n",
      "Epoch 72,Loss 29.048273\n",
      "\tParams: tensor([ 0.2326, -0.0353])\n",
      "\tGrad:tensor([-0.0533,  3.0241])\n",
      "Epoch 73,Loss 29.047350\n",
      "\tParams: tensor([ 0.2326, -0.0356])\n",
      "\tGrad:tensor([-0.0532,  3.0240])\n",
      "Epoch 74,Loss 29.046442\n",
      "\tParams: tensor([ 0.2326, -0.0359])\n",
      "\tGrad:tensor([-0.0533,  3.0240])\n",
      "Epoch 75,Loss 29.045530\n",
      "\tParams: tensor([ 0.2326, -0.0362])\n",
      "\tGrad:tensor([-0.0532,  3.0239])\n",
      "Epoch 76,Loss 29.044611\n",
      "\tParams: tensor([ 0.2326, -0.0365])\n",
      "\tGrad:tensor([-0.0533,  3.0239])\n",
      "Epoch 77,Loss 29.043699\n",
      "\tParams: tensor([ 0.2326, -0.0368])\n",
      "\tGrad:tensor([-0.0533,  3.0238])\n",
      "Epoch 78,Loss 29.042784\n",
      "\tParams: tensor([ 0.2326, -0.0371])\n",
      "\tGrad:tensor([-0.0533,  3.0238])\n",
      "Epoch 79,Loss 29.041870\n",
      "\tParams: tensor([ 0.2326, -0.0374])\n",
      "\tGrad:tensor([-0.0533,  3.0237])\n",
      "Epoch 80,Loss 29.040955\n",
      "\tParams: tensor([ 0.2326, -0.0377])\n",
      "\tGrad:tensor([-0.0532,  3.0236])\n",
      "Epoch 81,Loss 29.040039\n",
      "\tParams: tensor([ 0.2326, -0.0380])\n",
      "\tGrad:tensor([-0.0534,  3.0236])\n",
      "Epoch 82,Loss 29.039122\n",
      "\tParams: tensor([ 0.2326, -0.0383])\n",
      "\tGrad:tensor([-0.0533,  3.0235])\n",
      "Epoch 83,Loss 29.038210\n",
      "\tParams: tensor([ 0.2326, -0.0386])\n",
      "\tGrad:tensor([-0.0532,  3.0235])\n",
      "Epoch 84,Loss 29.037294\n",
      "\tParams: tensor([ 0.2326, -0.0389])\n",
      "\tGrad:tensor([-0.0533,  3.0234])\n",
      "Epoch 85,Loss 29.036379\n",
      "\tParams: tensor([ 0.2326, -0.0392])\n",
      "\tGrad:tensor([-0.0533,  3.0234])\n",
      "Epoch 86,Loss 29.035463\n",
      "\tParams: tensor([ 0.2326, -0.0395])\n",
      "\tGrad:tensor([-0.0532,  3.0233])\n",
      "Epoch 87,Loss 29.034554\n",
      "\tParams: tensor([ 0.2327, -0.0398])\n",
      "\tGrad:tensor([-0.0533,  3.0233])\n",
      "Epoch 88,Loss 29.033636\n",
      "\tParams: tensor([ 0.2327, -0.0401])\n",
      "\tGrad:tensor([-0.0532,  3.0232])\n",
      "Epoch 89,Loss 29.032722\n",
      "\tParams: tensor([ 0.2327, -0.0405])\n",
      "\tGrad:tensor([-0.0533,  3.0232])\n",
      "Epoch 90,Loss 29.031811\n",
      "\tParams: tensor([ 0.2327, -0.0408])\n",
      "\tGrad:tensor([-0.0533,  3.0231])\n",
      "Epoch 91,Loss 29.030895\n",
      "\tParams: tensor([ 0.2327, -0.0411])\n",
      "\tGrad:tensor([-0.0532,  3.0231])\n",
      "Epoch 92,Loss 29.029976\n",
      "\tParams: tensor([ 0.2327, -0.0414])\n",
      "\tGrad:tensor([-0.0532,  3.0230])\n",
      "Epoch 93,Loss 29.029066\n",
      "\tParams: tensor([ 0.2327, -0.0417])\n",
      "\tGrad:tensor([-0.0533,  3.0230])\n",
      "Epoch 94,Loss 29.028151\n",
      "\tParams: tensor([ 0.2327, -0.0420])\n",
      "\tGrad:tensor([-0.0532,  3.0229])\n",
      "Epoch 95,Loss 29.027235\n",
      "\tParams: tensor([ 0.2327, -0.0423])\n",
      "\tGrad:tensor([-0.0533,  3.0229])\n",
      "Epoch 96,Loss 29.026323\n",
      "\tParams: tensor([ 0.2327, -0.0426])\n",
      "\tGrad:tensor([-0.0533,  3.0228])\n",
      "Epoch 97,Loss 29.025410\n",
      "\tParams: tensor([ 0.2327, -0.0429])\n",
      "\tGrad:tensor([-0.0532,  3.0227])\n",
      "Epoch 98,Loss 29.024492\n",
      "\tParams: tensor([ 0.2327, -0.0432])\n",
      "\tGrad:tensor([-0.0532,  3.0227])\n",
      "Epoch 99,Loss 29.023582\n",
      "\tParams: tensor([ 0.2327, -0.0435])\n",
      "\tGrad:tensor([-0.0533,  3.0226])\n",
      "Epoch 100,Loss 29.022667\n",
      "\tParams: tensor([ 0.2327, -0.0438])\n",
      "\tGrad:tensor([-0.0532,  3.0226])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2327, -0.0438])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs = 100, learning_rate = 1e-4, params = torch.tensor([1.0, 0]), t_u = t_u, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2d5e81-b9c5-4a03-b38b-f8b420392f79",
   "metadata": {},
   "source": [
    "现在优化过程已经稳定了，但还是有一个问题：参数接收到的更新非常小，所以损失下降的非常慢，最终停滞不前。我们可以通过学习率自适应来解决这个问题，但我们先处理另一个潜在的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4909ea-1ee1-425c-b179-041adcbc9ad3",
   "metadata": {},
   "source": [
    "### 归一化输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f085835-68c5-44e7-9092-bced7415938c",
   "metadata": {},
   "source": [
    "从之前的输出可以看到，在第一个迭代周期，权重的梯度大约是偏置梯度的 50 倍。这意味着权重和偏置存在于不同的比例空间中，在这种情况下，如果学习率足够大，能够有效更新其中一个参数，那么对另一个参数来说，学习率就会变得不稳定，而一个只适合于另一个参数的学习率也不足以有意义地改变前者。这意味着我们无法有效的更新参数，除非我们改变模型公式，对每个参数都有一个学习率，但是这样模型又太麻烦了。\n",
    "\n",
    "可以使用一中更简单的方法控制一切：改变输入，这样梯度就不会太大的不同。粗略地说，我们可以确保输入的范围不会偏离 -1.0～1.0太远。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e521c413-1c8e-4897-b847-bfcdb6d6ff90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_nu = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bab91f-4864-4fa4-98ab-bfd1bd3ba80c",
   "metadata": {},
   "source": [
    "现在我们可以使用归一化后的输入运行循环训练了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e093a491-c65c-4a15-ad88-fae6641b6443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Loss 80.364342\n",
      "\tParams: tensor([1.7761, 0.1064])\n",
      "\tGrad:tensor([-77.6140, -10.6400])\n",
      "Epoch 2,Loss 37.574913\n",
      "\tParams: tensor([2.0848, 0.1303])\n",
      "\tGrad:tensor([-30.8623,  -2.3864])\n",
      "Epoch 3,Loss 30.871077\n",
      "\tParams: tensor([2.2094, 0.1217])\n",
      "\tGrad:tensor([-12.4631,   0.8587])\n",
      "Epoch 4,Loss 29.756193\n",
      "\tParams: tensor([2.2616, 0.1004])\n",
      "\tGrad:tensor([-5.2218,  2.1327])\n",
      "Epoch 5,Loss 29.507153\n",
      "\tParams: tensor([2.2853, 0.0740])\n",
      "\tGrad:tensor([-2.3715,  2.6310])\n",
      "Epoch 6,Loss 29.392456\n",
      "\tParams: tensor([2.2978, 0.0458])\n",
      "\tGrad:tensor([-1.2492,  2.8241])\n",
      "Epoch 7,Loss 29.298828\n",
      "\tParams: tensor([2.3059, 0.0168])\n",
      "\tGrad:tensor([-0.8071,  2.8970])\n",
      "Epoch 8,Loss 29.208717\n",
      "\tParams: tensor([ 2.3122, -0.0124])\n",
      "\tGrad:tensor([-0.6325,  2.9227])\n",
      "Epoch 9,Loss 29.119415\n",
      "\tParams: tensor([ 2.3178, -0.0417])\n",
      "\tGrad:tensor([-0.5633,  2.9298])\n",
      "Epoch 10,Loss 29.030489\n",
      "\tParams: tensor([ 2.3232, -0.0710])\n",
      "\tGrad:tensor([-0.5355,  2.9295])\n",
      "Epoch 11,Loss 28.941877\n",
      "\tParams: tensor([ 2.3284, -0.1003])\n",
      "\tGrad:tensor([-0.5240,  2.9264])\n",
      "Epoch 12,Loss 28.853565\n",
      "\tParams: tensor([ 2.3336, -0.1295])\n",
      "\tGrad:tensor([-0.5190,  2.9222])\n",
      "Epoch 13,Loss 28.765553\n",
      "\tParams: tensor([ 2.3388, -0.1587])\n",
      "\tGrad:tensor([-0.5165,  2.9175])\n",
      "Epoch 14,Loss 28.677851\n",
      "\tParams: tensor([ 2.3439, -0.1878])\n",
      "\tGrad:tensor([-0.5150,  2.9126])\n",
      "Epoch 15,Loss 28.590431\n",
      "\tParams: tensor([ 2.3491, -0.2169])\n",
      "\tGrad:tensor([-0.5138,  2.9077])\n",
      "Epoch 16,Loss 28.503319\n",
      "\tParams: tensor([ 2.3542, -0.2459])\n",
      "\tGrad:tensor([-0.5129,  2.9028])\n",
      "Epoch 17,Loss 28.416498\n",
      "\tParams: tensor([ 2.3593, -0.2749])\n",
      "\tGrad:tensor([-0.5120,  2.8979])\n",
      "Epoch 18,Loss 28.329973\n",
      "\tParams: tensor([ 2.3644, -0.3038])\n",
      "\tGrad:tensor([-0.5111,  2.8930])\n",
      "Epoch 19,Loss 28.243742\n",
      "\tParams: tensor([ 2.3695, -0.3327])\n",
      "\tGrad:tensor([-0.5102,  2.8881])\n",
      "Epoch 20,Loss 28.157804\n",
      "\tParams: tensor([ 2.3746, -0.3615])\n",
      "\tGrad:tensor([-0.5093,  2.8832])\n",
      "Epoch 21,Loss 28.072151\n",
      "\tParams: tensor([ 2.3797, -0.3903])\n",
      "\tGrad:tensor([-0.5084,  2.8783])\n",
      "Epoch 22,Loss 27.986797\n",
      "\tParams: tensor([ 2.3848, -0.4190])\n",
      "\tGrad:tensor([-0.5076,  2.8734])\n",
      "Epoch 23,Loss 27.901728\n",
      "\tParams: tensor([ 2.3899, -0.4477])\n",
      "\tGrad:tensor([-0.5067,  2.8685])\n",
      "Epoch 24,Loss 27.816950\n",
      "\tParams: tensor([ 2.3949, -0.4763])\n",
      "\tGrad:tensor([-0.5059,  2.8636])\n",
      "Epoch 25,Loss 27.732464\n",
      "\tParams: tensor([ 2.4000, -0.5049])\n",
      "\tGrad:tensor([-0.5050,  2.8588])\n",
      "Epoch 26,Loss 27.648256\n",
      "\tParams: tensor([ 2.4050, -0.5335])\n",
      "\tGrad:tensor([-0.5042,  2.8539])\n",
      "Epoch 27,Loss 27.564344\n",
      "\tParams: tensor([ 2.4101, -0.5620])\n",
      "\tGrad:tensor([-0.5033,  2.8490])\n",
      "Epoch 28,Loss 27.480707\n",
      "\tParams: tensor([ 2.4151, -0.5904])\n",
      "\tGrad:tensor([-0.5024,  2.8442])\n",
      "Epoch 29,Loss 27.397362\n",
      "\tParams: tensor([ 2.4201, -0.6188])\n",
      "\tGrad:tensor([-0.5016,  2.8394])\n",
      "Epoch 30,Loss 27.314295\n",
      "\tParams: tensor([ 2.4251, -0.6471])\n",
      "\tGrad:tensor([-0.5007,  2.8346])\n",
      "Epoch 31,Loss 27.231512\n",
      "\tParams: tensor([ 2.4301, -0.6754])\n",
      "\tGrad:tensor([-0.4999,  2.8297])\n",
      "Epoch 32,Loss 27.149010\n",
      "\tParams: tensor([ 2.4351, -0.7037])\n",
      "\tGrad:tensor([-0.4990,  2.8249])\n",
      "Epoch 33,Loss 27.066790\n",
      "\tParams: tensor([ 2.4401, -0.7319])\n",
      "\tGrad:tensor([-0.4982,  2.8201])\n",
      "Epoch 34,Loss 26.984844\n",
      "\tParams: tensor([ 2.4450, -0.7600])\n",
      "\tGrad:tensor([-0.4973,  2.8153])\n",
      "Epoch 35,Loss 26.903175\n",
      "\tParams: tensor([ 2.4500, -0.7881])\n",
      "\tGrad:tensor([-0.4965,  2.8106])\n",
      "Epoch 36,Loss 26.821791\n",
      "\tParams: tensor([ 2.4550, -0.8162])\n",
      "\tGrad:tensor([-0.4957,  2.8058])\n",
      "Epoch 37,Loss 26.740679\n",
      "\tParams: tensor([ 2.4599, -0.8442])\n",
      "\tGrad:tensor([-0.4948,  2.8010])\n",
      "Epoch 38,Loss 26.659838\n",
      "\tParams: tensor([ 2.4649, -0.8722])\n",
      "\tGrad:tensor([-0.4940,  2.7963])\n",
      "Epoch 39,Loss 26.579279\n",
      "\tParams: tensor([ 2.4698, -0.9001])\n",
      "\tGrad:tensor([-0.4931,  2.7915])\n",
      "Epoch 40,Loss 26.498987\n",
      "\tParams: tensor([ 2.4747, -0.9280])\n",
      "\tGrad:tensor([-0.4923,  2.7868])\n",
      "Epoch 41,Loss 26.418974\n",
      "\tParams: tensor([ 2.4796, -0.9558])\n",
      "\tGrad:tensor([-0.4915,  2.7820])\n",
      "Epoch 42,Loss 26.339228\n",
      "\tParams: tensor([ 2.4845, -0.9836])\n",
      "\tGrad:tensor([-0.4906,  2.7773])\n",
      "Epoch 43,Loss 26.259754\n",
      "\tParams: tensor([ 2.4894, -1.0113])\n",
      "\tGrad:tensor([-0.4898,  2.7726])\n",
      "Epoch 44,Loss 26.180548\n",
      "\tParams: tensor([ 2.4943, -1.0390])\n",
      "\tGrad:tensor([-0.4890,  2.7679])\n",
      "Epoch 45,Loss 26.101616\n",
      "\tParams: tensor([ 2.4992, -1.0666])\n",
      "\tGrad:tensor([-0.4881,  2.7632])\n",
      "Epoch 46,Loss 26.022947\n",
      "\tParams: tensor([ 2.5041, -1.0942])\n",
      "\tGrad:tensor([-0.4873,  2.7585])\n",
      "Epoch 47,Loss 25.944544\n",
      "\tParams: tensor([ 2.5089, -1.1217])\n",
      "\tGrad:tensor([-0.4865,  2.7538])\n",
      "Epoch 48,Loss 25.866417\n",
      "\tParams: tensor([ 2.5138, -1.1492])\n",
      "\tGrad:tensor([-0.4856,  2.7491])\n",
      "Epoch 49,Loss 25.788549\n",
      "\tParams: tensor([ 2.5186, -1.1766])\n",
      "\tGrad:tensor([-0.4848,  2.7444])\n",
      "Epoch 50,Loss 25.710938\n",
      "\tParams: tensor([ 2.5235, -1.2040])\n",
      "\tGrad:tensor([-0.4840,  2.7398])\n",
      "Epoch 51,Loss 25.633600\n",
      "\tParams: tensor([ 2.5283, -1.2314])\n",
      "\tGrad:tensor([-0.4832,  2.7351])\n",
      "Epoch 52,Loss 25.556524\n",
      "\tParams: tensor([ 2.5331, -1.2587])\n",
      "\tGrad:tensor([-0.4823,  2.7305])\n",
      "Epoch 53,Loss 25.479700\n",
      "\tParams: tensor([ 2.5379, -1.2860])\n",
      "\tGrad:tensor([-0.4815,  2.7258])\n",
      "Epoch 54,Loss 25.403149\n",
      "\tParams: tensor([ 2.5428, -1.3132])\n",
      "\tGrad:tensor([-0.4807,  2.7212])\n",
      "Epoch 55,Loss 25.326851\n",
      "\tParams: tensor([ 2.5476, -1.3403])\n",
      "\tGrad:tensor([-0.4799,  2.7166])\n",
      "Epoch 56,Loss 25.250811\n",
      "\tParams: tensor([ 2.5523, -1.3675])\n",
      "\tGrad:tensor([-0.4791,  2.7120])\n",
      "Epoch 57,Loss 25.175035\n",
      "\tParams: tensor([ 2.5571, -1.3945])\n",
      "\tGrad:tensor([-0.4783,  2.7074])\n",
      "Epoch 58,Loss 25.099512\n",
      "\tParams: tensor([ 2.5619, -1.4216])\n",
      "\tGrad:tensor([-0.4775,  2.7028])\n",
      "Epoch 59,Loss 25.024248\n",
      "\tParams: tensor([ 2.5667, -1.4485])\n",
      "\tGrad:tensor([-0.4766,  2.6982])\n",
      "Epoch 60,Loss 24.949236\n",
      "\tParams: tensor([ 2.5714, -1.4755])\n",
      "\tGrad:tensor([-0.4758,  2.6936])\n",
      "Epoch 61,Loss 24.874483\n",
      "\tParams: tensor([ 2.5762, -1.5024])\n",
      "\tGrad:tensor([-0.4750,  2.6890])\n",
      "Epoch 62,Loss 24.799976\n",
      "\tParams: tensor([ 2.5809, -1.5292])\n",
      "\tGrad:tensor([-0.4742,  2.6845])\n",
      "Epoch 63,Loss 24.725737\n",
      "\tParams: tensor([ 2.5857, -1.5560])\n",
      "\tGrad:tensor([-0.4734,  2.6799])\n",
      "Epoch 64,Loss 24.651739\n",
      "\tParams: tensor([ 2.5904, -1.5828])\n",
      "\tGrad:tensor([-0.4726,  2.6753])\n",
      "Epoch 65,Loss 24.577986\n",
      "\tParams: tensor([ 2.5951, -1.6095])\n",
      "\tGrad:tensor([-0.4718,  2.6708])\n",
      "Epoch 66,Loss 24.504494\n",
      "\tParams: tensor([ 2.5998, -1.6361])\n",
      "\tGrad:tensor([-0.4710,  2.6663])\n",
      "Epoch 67,Loss 24.431252\n",
      "\tParams: tensor([ 2.6045, -1.6628])\n",
      "\tGrad:tensor([-0.4702,  2.6617])\n",
      "Epoch 68,Loss 24.358257\n",
      "\tParams: tensor([ 2.6092, -1.6893])\n",
      "\tGrad:tensor([-0.4694,  2.6572])\n",
      "Epoch 69,Loss 24.285505\n",
      "\tParams: tensor([ 2.6139, -1.7159])\n",
      "\tGrad:tensor([-0.4686,  2.6527])\n",
      "Epoch 70,Loss 24.212999\n",
      "\tParams: tensor([ 2.6186, -1.7423])\n",
      "\tGrad:tensor([-0.4678,  2.6482])\n",
      "Epoch 71,Loss 24.140741\n",
      "\tParams: tensor([ 2.6232, -1.7688])\n",
      "\tGrad:tensor([-0.4670,  2.6437])\n",
      "Epoch 72,Loss 24.068733\n",
      "\tParams: tensor([ 2.6279, -1.7952])\n",
      "\tGrad:tensor([-0.4662,  2.6392])\n",
      "Epoch 73,Loss 23.996971\n",
      "\tParams: tensor([ 2.6326, -1.8215])\n",
      "\tGrad:tensor([-0.4654,  2.6347])\n",
      "Epoch 74,Loss 23.925446\n",
      "\tParams: tensor([ 2.6372, -1.8478])\n",
      "\tGrad:tensor([-0.4646,  2.6302])\n",
      "Epoch 75,Loss 23.854168\n",
      "\tParams: tensor([ 2.6418, -1.8741])\n",
      "\tGrad:tensor([-0.4638,  2.6258])\n",
      "Epoch 76,Loss 23.783125\n",
      "\tParams: tensor([ 2.6465, -1.9003])\n",
      "\tGrad:tensor([-0.4631,  2.6213])\n",
      "Epoch 77,Loss 23.712328\n",
      "\tParams: tensor([ 2.6511, -1.9265])\n",
      "\tGrad:tensor([-0.4623,  2.6169])\n",
      "Epoch 78,Loss 23.641773\n",
      "\tParams: tensor([ 2.6557, -1.9526])\n",
      "\tGrad:tensor([-0.4615,  2.6124])\n",
      "Epoch 79,Loss 23.571455\n",
      "\tParams: tensor([ 2.6603, -1.9787])\n",
      "\tGrad:tensor([-0.4607,  2.6080])\n",
      "Epoch 80,Loss 23.501379\n",
      "\tParams: tensor([ 2.6649, -2.0047])\n",
      "\tGrad:tensor([-0.4599,  2.6035])\n",
      "Epoch 81,Loss 23.431538\n",
      "\tParams: tensor([ 2.6695, -2.0307])\n",
      "\tGrad:tensor([-0.4591,  2.5991])\n",
      "Epoch 82,Loss 23.361937\n",
      "\tParams: tensor([ 2.6741, -2.0566])\n",
      "\tGrad:tensor([-0.4584,  2.5947])\n",
      "Epoch 83,Loss 23.292570\n",
      "\tParams: tensor([ 2.6787, -2.0825])\n",
      "\tGrad:tensor([-0.4576,  2.5903])\n",
      "Epoch 84,Loss 23.223436\n",
      "\tParams: tensor([ 2.6832, -2.1084])\n",
      "\tGrad:tensor([-0.4568,  2.5859])\n",
      "Epoch 85,Loss 23.154541\n",
      "\tParams: tensor([ 2.6878, -2.1342])\n",
      "\tGrad:tensor([-0.4560,  2.5815])\n",
      "Epoch 86,Loss 23.085882\n",
      "\tParams: tensor([ 2.6923, -2.1600])\n",
      "\tGrad:tensor([-0.4553,  2.5771])\n",
      "Epoch 87,Loss 23.017447\n",
      "\tParams: tensor([ 2.6969, -2.1857])\n",
      "\tGrad:tensor([-0.4545,  2.5727])\n",
      "Epoch 88,Loss 22.949251\n",
      "\tParams: tensor([ 2.7014, -2.2114])\n",
      "\tGrad:tensor([-0.4537,  2.5684])\n",
      "Epoch 89,Loss 22.881283\n",
      "\tParams: tensor([ 2.7060, -2.2370])\n",
      "\tGrad:tensor([-0.4529,  2.5640])\n",
      "Epoch 90,Loss 22.813549\n",
      "\tParams: tensor([ 2.7105, -2.2626])\n",
      "\tGrad:tensor([-0.4522,  2.5597])\n",
      "Epoch 91,Loss 22.746044\n",
      "\tParams: tensor([ 2.7150, -2.2882])\n",
      "\tGrad:tensor([-0.4514,  2.5553])\n",
      "Epoch 92,Loss 22.678766\n",
      "\tParams: tensor([ 2.7195, -2.3137])\n",
      "\tGrad:tensor([-0.4506,  2.5510])\n",
      "Epoch 93,Loss 22.611717\n",
      "\tParams: tensor([ 2.7240, -2.3392])\n",
      "\tGrad:tensor([-0.4499,  2.5466])\n",
      "Epoch 94,Loss 22.544899\n",
      "\tParams: tensor([ 2.7285, -2.3646])\n",
      "\tGrad:tensor([-0.4491,  2.5423])\n",
      "Epoch 95,Loss 22.478306\n",
      "\tParams: tensor([ 2.7330, -2.3900])\n",
      "\tGrad:tensor([-0.4483,  2.5380])\n",
      "Epoch 96,Loss 22.411934\n",
      "\tParams: tensor([ 2.7374, -2.4153])\n",
      "\tGrad:tensor([-0.4476,  2.5337])\n",
      "Epoch 97,Loss 22.345793\n",
      "\tParams: tensor([ 2.7419, -2.4406])\n",
      "\tGrad:tensor([-0.4468,  2.5294])\n",
      "Epoch 98,Loss 22.279875\n",
      "\tParams: tensor([ 2.7464, -2.4658])\n",
      "\tGrad:tensor([-0.4461,  2.5251])\n",
      "Epoch 99,Loss 22.214186\n",
      "\tParams: tensor([ 2.7508, -2.4910])\n",
      "\tGrad:tensor([-0.4453,  2.5208])\n",
      "Epoch 100,Loss 22.148710\n",
      "\tParams: tensor([ 2.7553, -2.5162])\n",
      "\tGrad:tensor([-0.4446,  2.5165])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.7553, -2.5162])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs = 100, learning_rate = 1e-2, params = torch.tensor([1.0, 0]), t_u = t_nu, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b39cfb-267a-497d-9aa5-32828c55ed5d",
   "metadata": {},
   "source": [
    "可以看到即使我们将学习率调回到 `1e-2` ，参数也不会在迭代更新中爆炸。然后再看看梯度：他们大小相似，所以对 2 个参数使用一个学习率就可以了。当然我们可以选择更好的方式来进行归一化操作，但目前为止已经足够满足要求了，所以就继续使用它。\n",
    "\n",
    "在执行一次 5000 次的迭代循环："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aefa8ab6-fd9f-447c-a5ee-61239c245439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,Loss 80.364342\n",
      "\tParams: tensor([1.7761, 0.1064])\n",
      "\tGrad:tensor([-77.6140, -10.6400])\n",
      "Epoch 2,Loss 37.574913\n",
      "\tParams: tensor([2.0848, 0.1303])\n",
      "\tGrad:tensor([-30.8623,  -2.3864])\n",
      "Epoch 3,Loss 30.871077\n",
      "\tParams: tensor([2.2094, 0.1217])\n",
      "\tGrad:tensor([-12.4631,   0.8587])\n",
      "Epoch 4,Loss 29.756193\n",
      "\tParams: tensor([2.2616, 0.1004])\n",
      "\tGrad:tensor([-5.2218,  2.1327])\n",
      "Epoch 5,Loss 29.507153\n",
      "\tParams: tensor([2.2853, 0.0740])\n",
      "\tGrad:tensor([-2.3715,  2.6310])\n",
      "Epoch 6,Loss 29.392456\n",
      "\tParams: tensor([2.2978, 0.0458])\n",
      "\tGrad:tensor([-1.2492,  2.8241])\n",
      "Epoch 7,Loss 29.298828\n",
      "\tParams: tensor([2.3059, 0.0168])\n",
      "\tGrad:tensor([-0.8071,  2.8970])\n",
      "Epoch 8,Loss 29.208717\n",
      "\tParams: tensor([ 2.3122, -0.0124])\n",
      "\tGrad:tensor([-0.6325,  2.9227])\n",
      "Epoch 9,Loss 29.119415\n",
      "\tParams: tensor([ 2.3178, -0.0417])\n",
      "\tGrad:tensor([-0.5633,  2.9298])\n",
      "Epoch 10,Loss 29.030489\n",
      "\tParams: tensor([ 2.3232, -0.0710])\n",
      "\tGrad:tensor([-0.5355,  2.9295])\n",
      "Epoch 11,Loss 28.941877\n",
      "\tParams: tensor([ 2.3284, -0.1003])\n",
      "\tGrad:tensor([-0.5240,  2.9264])\n",
      "Epoch 12,Loss 28.853565\n",
      "\tParams: tensor([ 2.3336, -0.1295])\n",
      "\tGrad:tensor([-0.5190,  2.9222])\n",
      "Epoch 13,Loss 28.765553\n",
      "\tParams: tensor([ 2.3388, -0.1587])\n",
      "\tGrad:tensor([-0.5165,  2.9175])\n",
      "Epoch 14,Loss 28.677851\n",
      "\tParams: tensor([ 2.3439, -0.1878])\n",
      "\tGrad:tensor([-0.5150,  2.9126])\n",
      "Epoch 15,Loss 28.590431\n",
      "\tParams: tensor([ 2.3491, -0.2169])\n",
      "\tGrad:tensor([-0.5138,  2.9077])\n",
      "Epoch 16,Loss 28.503319\n",
      "\tParams: tensor([ 2.3542, -0.2459])\n",
      "\tGrad:tensor([-0.5129,  2.9028])\n",
      "Epoch 17,Loss 28.416498\n",
      "\tParams: tensor([ 2.3593, -0.2749])\n",
      "\tGrad:tensor([-0.5120,  2.8979])\n",
      "Epoch 18,Loss 28.329973\n",
      "\tParams: tensor([ 2.3644, -0.3038])\n",
      "\tGrad:tensor([-0.5111,  2.8930])\n",
      "Epoch 19,Loss 28.243742\n",
      "\tParams: tensor([ 2.3695, -0.3327])\n",
      "\tGrad:tensor([-0.5102,  2.8881])\n",
      "Epoch 20,Loss 28.157804\n",
      "\tParams: tensor([ 2.3746, -0.3615])\n",
      "\tGrad:tensor([-0.5093,  2.8832])\n",
      "Epoch 21,Loss 28.072151\n",
      "\tParams: tensor([ 2.3797, -0.3903])\n",
      "\tGrad:tensor([-0.5084,  2.8783])\n",
      "Epoch 22,Loss 27.986797\n",
      "\tParams: tensor([ 2.3848, -0.4190])\n",
      "\tGrad:tensor([-0.5076,  2.8734])\n",
      "Epoch 23,Loss 27.901728\n",
      "\tParams: tensor([ 2.3899, -0.4477])\n",
      "\tGrad:tensor([-0.5067,  2.8685])\n",
      "Epoch 24,Loss 27.816950\n",
      "\tParams: tensor([ 2.3949, -0.4763])\n",
      "\tGrad:tensor([-0.5059,  2.8636])\n",
      "Epoch 25,Loss 27.732464\n",
      "\tParams: tensor([ 2.4000, -0.5049])\n",
      "\tGrad:tensor([-0.5050,  2.8588])\n",
      "Epoch 26,Loss 27.648256\n",
      "\tParams: tensor([ 2.4050, -0.5335])\n",
      "\tGrad:tensor([-0.5042,  2.8539])\n",
      "Epoch 27,Loss 27.564344\n",
      "\tParams: tensor([ 2.4101, -0.5620])\n",
      "\tGrad:tensor([-0.5033,  2.8490])\n",
      "Epoch 28,Loss 27.480707\n",
      "\tParams: tensor([ 2.4151, -0.5904])\n",
      "\tGrad:tensor([-0.5024,  2.8442])\n",
      "Epoch 29,Loss 27.397362\n",
      "\tParams: tensor([ 2.4201, -0.6188])\n",
      "\tGrad:tensor([-0.5016,  2.8394])\n",
      "Epoch 30,Loss 27.314295\n",
      "\tParams: tensor([ 2.4251, -0.6471])\n",
      "\tGrad:tensor([-0.5007,  2.8346])\n",
      "Epoch 31,Loss 27.231512\n",
      "\tParams: tensor([ 2.4301, -0.6754])\n",
      "\tGrad:tensor([-0.4999,  2.8297])\n",
      "Epoch 32,Loss 27.149010\n",
      "\tParams: tensor([ 2.4351, -0.7037])\n",
      "\tGrad:tensor([-0.4990,  2.8249])\n",
      "Epoch 33,Loss 27.066790\n",
      "\tParams: tensor([ 2.4401, -0.7319])\n",
      "\tGrad:tensor([-0.4982,  2.8201])\n",
      "Epoch 34,Loss 26.984844\n",
      "\tParams: tensor([ 2.4450, -0.7600])\n",
      "\tGrad:tensor([-0.4973,  2.8153])\n",
      "Epoch 35,Loss 26.903175\n",
      "\tParams: tensor([ 2.4500, -0.7881])\n",
      "\tGrad:tensor([-0.4965,  2.8106])\n",
      "Epoch 36,Loss 26.821791\n",
      "\tParams: tensor([ 2.4550, -0.8162])\n",
      "\tGrad:tensor([-0.4957,  2.8058])\n",
      "Epoch 37,Loss 26.740679\n",
      "\tParams: tensor([ 2.4599, -0.8442])\n",
      "\tGrad:tensor([-0.4948,  2.8010])\n",
      "Epoch 38,Loss 26.659838\n",
      "\tParams: tensor([ 2.4649, -0.8722])\n",
      "\tGrad:tensor([-0.4940,  2.7963])\n",
      "Epoch 39,Loss 26.579279\n",
      "\tParams: tensor([ 2.4698, -0.9001])\n",
      "\tGrad:tensor([-0.4931,  2.7915])\n",
      "Epoch 40,Loss 26.498987\n",
      "\tParams: tensor([ 2.4747, -0.9280])\n",
      "\tGrad:tensor([-0.4923,  2.7868])\n",
      "Epoch 41,Loss 26.418974\n",
      "\tParams: tensor([ 2.4796, -0.9558])\n",
      "\tGrad:tensor([-0.4915,  2.7820])\n",
      "Epoch 42,Loss 26.339228\n",
      "\tParams: tensor([ 2.4845, -0.9836])\n",
      "\tGrad:tensor([-0.4906,  2.7773])\n",
      "Epoch 43,Loss 26.259754\n",
      "\tParams: tensor([ 2.4894, -1.0113])\n",
      "\tGrad:tensor([-0.4898,  2.7726])\n",
      "Epoch 44,Loss 26.180548\n",
      "\tParams: tensor([ 2.4943, -1.0390])\n",
      "\tGrad:tensor([-0.4890,  2.7679])\n",
      "Epoch 45,Loss 26.101616\n",
      "\tParams: tensor([ 2.4992, -1.0666])\n",
      "\tGrad:tensor([-0.4881,  2.7632])\n",
      "Epoch 46,Loss 26.022947\n",
      "\tParams: tensor([ 2.5041, -1.0942])\n",
      "\tGrad:tensor([-0.4873,  2.7585])\n",
      "Epoch 47,Loss 25.944544\n",
      "\tParams: tensor([ 2.5089, -1.1217])\n",
      "\tGrad:tensor([-0.4865,  2.7538])\n",
      "Epoch 48,Loss 25.866417\n",
      "\tParams: tensor([ 2.5138, -1.1492])\n",
      "\tGrad:tensor([-0.4856,  2.7491])\n",
      "Epoch 49,Loss 25.788549\n",
      "\tParams: tensor([ 2.5186, -1.1766])\n",
      "\tGrad:tensor([-0.4848,  2.7444])\n",
      "Epoch 50,Loss 25.710938\n",
      "\tParams: tensor([ 2.5235, -1.2040])\n",
      "\tGrad:tensor([-0.4840,  2.7398])\n",
      "Epoch 51,Loss 25.633600\n",
      "\tParams: tensor([ 2.5283, -1.2314])\n",
      "\tGrad:tensor([-0.4832,  2.7351])\n",
      "Epoch 52,Loss 25.556524\n",
      "\tParams: tensor([ 2.5331, -1.2587])\n",
      "\tGrad:tensor([-0.4823,  2.7305])\n",
      "Epoch 53,Loss 25.479700\n",
      "\tParams: tensor([ 2.5379, -1.2860])\n",
      "\tGrad:tensor([-0.4815,  2.7258])\n",
      "Epoch 54,Loss 25.403149\n",
      "\tParams: tensor([ 2.5428, -1.3132])\n",
      "\tGrad:tensor([-0.4807,  2.7212])\n",
      "Epoch 55,Loss 25.326851\n",
      "\tParams: tensor([ 2.5476, -1.3403])\n",
      "\tGrad:tensor([-0.4799,  2.7166])\n",
      "Epoch 56,Loss 25.250811\n",
      "\tParams: tensor([ 2.5523, -1.3675])\n",
      "\tGrad:tensor([-0.4791,  2.7120])\n",
      "Epoch 57,Loss 25.175035\n",
      "\tParams: tensor([ 2.5571, -1.3945])\n",
      "\tGrad:tensor([-0.4783,  2.7074])\n",
      "Epoch 58,Loss 25.099512\n",
      "\tParams: tensor([ 2.5619, -1.4216])\n",
      "\tGrad:tensor([-0.4775,  2.7028])\n",
      "Epoch 59,Loss 25.024248\n",
      "\tParams: tensor([ 2.5667, -1.4485])\n",
      "\tGrad:tensor([-0.4766,  2.6982])\n",
      "Epoch 60,Loss 24.949236\n",
      "\tParams: tensor([ 2.5714, -1.4755])\n",
      "\tGrad:tensor([-0.4758,  2.6936])\n",
      "Epoch 61,Loss 24.874483\n",
      "\tParams: tensor([ 2.5762, -1.5024])\n",
      "\tGrad:tensor([-0.4750,  2.6890])\n",
      "Epoch 62,Loss 24.799976\n",
      "\tParams: tensor([ 2.5809, -1.5292])\n",
      "\tGrad:tensor([-0.4742,  2.6845])\n",
      "Epoch 63,Loss 24.725737\n",
      "\tParams: tensor([ 2.5857, -1.5560])\n",
      "\tGrad:tensor([-0.4734,  2.6799])\n",
      "Epoch 64,Loss 24.651739\n",
      "\tParams: tensor([ 2.5904, -1.5828])\n",
      "\tGrad:tensor([-0.4726,  2.6753])\n",
      "Epoch 65,Loss 24.577986\n",
      "\tParams: tensor([ 2.5951, -1.6095])\n",
      "\tGrad:tensor([-0.4718,  2.6708])\n",
      "Epoch 66,Loss 24.504494\n",
      "\tParams: tensor([ 2.5998, -1.6361])\n",
      "\tGrad:tensor([-0.4710,  2.6663])\n",
      "Epoch 67,Loss 24.431252\n",
      "\tParams: tensor([ 2.6045, -1.6628])\n",
      "\tGrad:tensor([-0.4702,  2.6617])\n",
      "Epoch 68,Loss 24.358257\n",
      "\tParams: tensor([ 2.6092, -1.6893])\n",
      "\tGrad:tensor([-0.4694,  2.6572])\n",
      "Epoch 69,Loss 24.285505\n",
      "\tParams: tensor([ 2.6139, -1.7159])\n",
      "\tGrad:tensor([-0.4686,  2.6527])\n",
      "Epoch 70,Loss 24.212999\n",
      "\tParams: tensor([ 2.6186, -1.7423])\n",
      "\tGrad:tensor([-0.4678,  2.6482])\n",
      "Epoch 71,Loss 24.140741\n",
      "\tParams: tensor([ 2.6232, -1.7688])\n",
      "\tGrad:tensor([-0.4670,  2.6437])\n",
      "Epoch 72,Loss 24.068733\n",
      "\tParams: tensor([ 2.6279, -1.7952])\n",
      "\tGrad:tensor([-0.4662,  2.6392])\n",
      "Epoch 73,Loss 23.996971\n",
      "\tParams: tensor([ 2.6326, -1.8215])\n",
      "\tGrad:tensor([-0.4654,  2.6347])\n",
      "Epoch 74,Loss 23.925446\n",
      "\tParams: tensor([ 2.6372, -1.8478])\n",
      "\tGrad:tensor([-0.4646,  2.6302])\n",
      "Epoch 75,Loss 23.854168\n",
      "\tParams: tensor([ 2.6418, -1.8741])\n",
      "\tGrad:tensor([-0.4638,  2.6258])\n",
      "Epoch 76,Loss 23.783125\n",
      "\tParams: tensor([ 2.6465, -1.9003])\n",
      "\tGrad:tensor([-0.4631,  2.6213])\n",
      "Epoch 77,Loss 23.712328\n",
      "\tParams: tensor([ 2.6511, -1.9265])\n",
      "\tGrad:tensor([-0.4623,  2.6169])\n",
      "Epoch 78,Loss 23.641773\n",
      "\tParams: tensor([ 2.6557, -1.9526])\n",
      "\tGrad:tensor([-0.4615,  2.6124])\n",
      "Epoch 79,Loss 23.571455\n",
      "\tParams: tensor([ 2.6603, -1.9787])\n",
      "\tGrad:tensor([-0.4607,  2.6080])\n",
      "Epoch 80,Loss 23.501379\n",
      "\tParams: tensor([ 2.6649, -2.0047])\n",
      "\tGrad:tensor([-0.4599,  2.6035])\n",
      "Epoch 81,Loss 23.431538\n",
      "\tParams: tensor([ 2.6695, -2.0307])\n",
      "\tGrad:tensor([-0.4591,  2.5991])\n",
      "Epoch 82,Loss 23.361937\n",
      "\tParams: tensor([ 2.6741, -2.0566])\n",
      "\tGrad:tensor([-0.4584,  2.5947])\n",
      "Epoch 83,Loss 23.292570\n",
      "\tParams: tensor([ 2.6787, -2.0825])\n",
      "\tGrad:tensor([-0.4576,  2.5903])\n",
      "Epoch 84,Loss 23.223436\n",
      "\tParams: tensor([ 2.6832, -2.1084])\n",
      "\tGrad:tensor([-0.4568,  2.5859])\n",
      "Epoch 85,Loss 23.154541\n",
      "\tParams: tensor([ 2.6878, -2.1342])\n",
      "\tGrad:tensor([-0.4560,  2.5815])\n",
      "Epoch 86,Loss 23.085882\n",
      "\tParams: tensor([ 2.6923, -2.1600])\n",
      "\tGrad:tensor([-0.4553,  2.5771])\n",
      "Epoch 87,Loss 23.017447\n",
      "\tParams: tensor([ 2.6969, -2.1857])\n",
      "\tGrad:tensor([-0.4545,  2.5727])\n",
      "Epoch 88,Loss 22.949251\n",
      "\tParams: tensor([ 2.7014, -2.2114])\n",
      "\tGrad:tensor([-0.4537,  2.5684])\n",
      "Epoch 89,Loss 22.881283\n",
      "\tParams: tensor([ 2.7060, -2.2370])\n",
      "\tGrad:tensor([-0.4529,  2.5640])\n",
      "Epoch 90,Loss 22.813549\n",
      "\tParams: tensor([ 2.7105, -2.2626])\n",
      "\tGrad:tensor([-0.4522,  2.5597])\n",
      "Epoch 91,Loss 22.746044\n",
      "\tParams: tensor([ 2.7150, -2.2882])\n",
      "\tGrad:tensor([-0.4514,  2.5553])\n",
      "Epoch 92,Loss 22.678766\n",
      "\tParams: tensor([ 2.7195, -2.3137])\n",
      "\tGrad:tensor([-0.4506,  2.5510])\n",
      "Epoch 93,Loss 22.611717\n",
      "\tParams: tensor([ 2.7240, -2.3392])\n",
      "\tGrad:tensor([-0.4499,  2.5466])\n",
      "Epoch 94,Loss 22.544899\n",
      "\tParams: tensor([ 2.7285, -2.3646])\n",
      "\tGrad:tensor([-0.4491,  2.5423])\n",
      "Epoch 95,Loss 22.478306\n",
      "\tParams: tensor([ 2.7330, -2.3900])\n",
      "\tGrad:tensor([-0.4483,  2.5380])\n",
      "Epoch 96,Loss 22.411934\n",
      "\tParams: tensor([ 2.7374, -2.4153])\n",
      "\tGrad:tensor([-0.4476,  2.5337])\n",
      "Epoch 97,Loss 22.345793\n",
      "\tParams: tensor([ 2.7419, -2.4406])\n",
      "\tGrad:tensor([-0.4468,  2.5294])\n",
      "Epoch 98,Loss 22.279875\n",
      "\tParams: tensor([ 2.7464, -2.4658])\n",
      "\tGrad:tensor([-0.4461,  2.5251])\n",
      "Epoch 99,Loss 22.214186\n",
      "\tParams: tensor([ 2.7508, -2.4910])\n",
      "\tGrad:tensor([-0.4453,  2.5208])\n",
      "Epoch 100,Loss 22.148710\n",
      "\tParams: tensor([ 2.7553, -2.5162])\n",
      "\tGrad:tensor([-0.4446,  2.5165])\n",
      "Epoch 101,Loss 22.083464\n",
      "\tParams: tensor([ 2.7597, -2.5413])\n",
      "\tGrad:tensor([-0.4438,  2.5122])\n",
      "Epoch 102,Loss 22.018436\n",
      "\tParams: tensor([ 2.7641, -2.5664])\n",
      "\tGrad:tensor([-0.4430,  2.5080])\n",
      "Epoch 103,Loss 21.953632\n",
      "\tParams: tensor([ 2.7686, -2.5914])\n",
      "\tGrad:tensor([-0.4423,  2.5037])\n",
      "Epoch 104,Loss 21.889046\n",
      "\tParams: tensor([ 2.7730, -2.6164])\n",
      "\tGrad:tensor([-0.4415,  2.4994])\n",
      "Epoch 105,Loss 21.824677\n",
      "\tParams: tensor([ 2.7774, -2.6414])\n",
      "\tGrad:tensor([-0.4408,  2.4952])\n",
      "Epoch 106,Loss 21.760529\n",
      "\tParams: tensor([ 2.7818, -2.6663])\n",
      "\tGrad:tensor([-0.4400,  2.4910])\n",
      "Epoch 107,Loss 21.696600\n",
      "\tParams: tensor([ 2.7862, -2.6912])\n",
      "\tGrad:tensor([-0.4393,  2.4867])\n",
      "Epoch 108,Loss 21.632883\n",
      "\tParams: tensor([ 2.7906, -2.7160])\n",
      "\tGrad:tensor([-0.4385,  2.4825])\n",
      "Epoch 109,Loss 21.569389\n",
      "\tParams: tensor([ 2.7949, -2.7408])\n",
      "\tGrad:tensor([-0.4378,  2.4783])\n",
      "Epoch 110,Loss 21.506102\n",
      "\tParams: tensor([ 2.7993, -2.7655])\n",
      "\tGrad:tensor([-0.4370,  2.4741])\n",
      "Epoch 111,Loss 21.443037\n",
      "\tParams: tensor([ 2.8037, -2.7902])\n",
      "\tGrad:tensor([-0.4363,  2.4699])\n",
      "Epoch 112,Loss 21.380186\n",
      "\tParams: tensor([ 2.8080, -2.8149])\n",
      "\tGrad:tensor([-0.4356,  2.4657])\n",
      "Epoch 113,Loss 21.317549\n",
      "\tParams: tensor([ 2.8124, -2.8395])\n",
      "\tGrad:tensor([-0.4348,  2.4615])\n",
      "Epoch 114,Loss 21.255117\n",
      "\tParams: tensor([ 2.8167, -2.8641])\n",
      "\tGrad:tensor([-0.4341,  2.4573])\n",
      "Epoch 115,Loss 21.192907\n",
      "\tParams: tensor([ 2.8211, -2.8886])\n",
      "\tGrad:tensor([-0.4334,  2.4531])\n",
      "Epoch 116,Loss 21.130898\n",
      "\tParams: tensor([ 2.8254, -2.9131])\n",
      "\tGrad:tensor([-0.4326,  2.4490])\n",
      "Epoch 117,Loss 21.069105\n",
      "\tParams: tensor([ 2.8297, -2.9375])\n",
      "\tGrad:tensor([-0.4319,  2.4448])\n",
      "Epoch 118,Loss 21.007526\n",
      "\tParams: tensor([ 2.8340, -2.9619])\n",
      "\tGrad:tensor([-0.4311,  2.4407])\n",
      "Epoch 119,Loss 20.946150\n",
      "\tParams: tensor([ 2.8383, -2.9863])\n",
      "\tGrad:tensor([-0.4304,  2.4365])\n",
      "Epoch 120,Loss 20.884981\n",
      "\tParams: tensor([ 2.8426, -3.0106])\n",
      "\tGrad:tensor([-0.4297,  2.4324])\n",
      "Epoch 121,Loss 20.824024\n",
      "\tParams: tensor([ 2.8469, -3.0349])\n",
      "\tGrad:tensor([-0.4290,  2.4282])\n",
      "Epoch 122,Loss 20.763273\n",
      "\tParams: tensor([ 2.8512, -3.0592])\n",
      "\tGrad:tensor([-0.4282,  2.4241])\n",
      "Epoch 123,Loss 20.702728\n",
      "\tParams: tensor([ 2.8555, -3.0834])\n",
      "\tGrad:tensor([-0.4275,  2.4200])\n",
      "Epoch 124,Loss 20.642384\n",
      "\tParams: tensor([ 2.8597, -3.1075])\n",
      "\tGrad:tensor([-0.4268,  2.4159])\n",
      "Epoch 125,Loss 20.582249\n",
      "\tParams: tensor([ 2.8640, -3.1316])\n",
      "\tGrad:tensor([-0.4260,  2.4118])\n",
      "Epoch 126,Loss 20.522322\n",
      "\tParams: tensor([ 2.8682, -3.1557])\n",
      "\tGrad:tensor([-0.4253,  2.4077])\n",
      "Epoch 127,Loss 20.462593\n",
      "\tParams: tensor([ 2.8725, -3.1797])\n",
      "\tGrad:tensor([-0.4246,  2.4036])\n",
      "Epoch 128,Loss 20.403069\n",
      "\tParams: tensor([ 2.8767, -3.2037])\n",
      "\tGrad:tensor([-0.4239,  2.3995])\n",
      "Epoch 129,Loss 20.343742\n",
      "\tParams: tensor([ 2.8810, -3.2277])\n",
      "\tGrad:tensor([-0.4232,  2.3954])\n",
      "Epoch 130,Loss 20.284624\n",
      "\tParams: tensor([ 2.8852, -3.2516])\n",
      "\tGrad:tensor([-0.4224,  2.3914])\n",
      "Epoch 131,Loss 20.225702\n",
      "\tParams: tensor([ 2.8894, -3.2755])\n",
      "\tGrad:tensor([-0.4217,  2.3873])\n",
      "Epoch 132,Loss 20.166981\n",
      "\tParams: tensor([ 2.8936, -3.2993])\n",
      "\tGrad:tensor([-0.4210,  2.3832])\n",
      "Epoch 133,Loss 20.108461\n",
      "\tParams: tensor([ 2.8978, -3.3231])\n",
      "\tGrad:tensor([-0.4203,  2.3792])\n",
      "Epoch 134,Loss 20.050137\n",
      "\tParams: tensor([ 2.9020, -3.3469])\n",
      "\tGrad:tensor([-0.4196,  2.3752])\n",
      "Epoch 135,Loss 19.992016\n",
      "\tParams: tensor([ 2.9062, -3.3706])\n",
      "\tGrad:tensor([-0.4189,  2.3711])\n",
      "Epoch 136,Loss 19.934086\n",
      "\tParams: tensor([ 2.9104, -3.3942])\n",
      "\tGrad:tensor([-0.4182,  2.3671])\n",
      "Epoch 137,Loss 19.876352\n",
      "\tParams: tensor([ 2.9146, -3.4179])\n",
      "\tGrad:tensor([-0.4174,  2.3631])\n",
      "Epoch 138,Loss 19.818823\n",
      "\tParams: tensor([ 2.9187, -3.4415])\n",
      "\tGrad:tensor([-0.4167,  2.3591])\n",
      "Epoch 139,Loss 19.761480\n",
      "\tParams: tensor([ 2.9229, -3.4650])\n",
      "\tGrad:tensor([-0.4160,  2.3550])\n",
      "Epoch 140,Loss 19.704336\n",
      "\tParams: tensor([ 2.9270, -3.4885])\n",
      "\tGrad:tensor([-0.4153,  2.3510])\n",
      "Epoch 141,Loss 19.647385\n",
      "\tParams: tensor([ 2.9312, -3.5120])\n",
      "\tGrad:tensor([-0.4146,  2.3471])\n",
      "Epoch 142,Loss 19.590626\n",
      "\tParams: tensor([ 2.9353, -3.5354])\n",
      "\tGrad:tensor([-0.4139,  2.3431])\n",
      "Epoch 143,Loss 19.534061\n",
      "\tParams: tensor([ 2.9395, -3.5588])\n",
      "\tGrad:tensor([-0.4132,  2.3391])\n",
      "Epoch 144,Loss 19.477690\n",
      "\tParams: tensor([ 2.9436, -3.5822])\n",
      "\tGrad:tensor([-0.4125,  2.3351])\n",
      "Epoch 145,Loss 19.421507\n",
      "\tParams: tensor([ 2.9477, -3.6055])\n",
      "\tGrad:tensor([-0.4118,  2.3311])\n",
      "Epoch 146,Loss 19.365515\n",
      "\tParams: tensor([ 2.9518, -3.6287])\n",
      "\tGrad:tensor([-0.4111,  2.3272])\n",
      "Epoch 147,Loss 19.309715\n",
      "\tParams: tensor([ 2.9559, -3.6520])\n",
      "\tGrad:tensor([-0.4104,  2.3232])\n",
      "Epoch 148,Loss 19.254107\n",
      "\tParams: tensor([ 2.9600, -3.6752])\n",
      "\tGrad:tensor([-0.4097,  2.3193])\n",
      "Epoch 149,Loss 19.198685\n",
      "\tParams: tensor([ 2.9641, -3.6983])\n",
      "\tGrad:tensor([-0.4090,  2.3153])\n",
      "Epoch 150,Loss 19.143446\n",
      "\tParams: tensor([ 2.9682, -3.7214])\n",
      "\tGrad:tensor([-0.4083,  2.3114])\n",
      "Epoch 151,Loss 19.088402\n",
      "\tParams: tensor([ 2.9723, -3.7445])\n",
      "\tGrad:tensor([-0.4076,  2.3075])\n",
      "Epoch 152,Loss 19.033543\n",
      "\tParams: tensor([ 2.9763, -3.7675])\n",
      "\tGrad:tensor([-0.4069,  2.3036])\n",
      "Epoch 153,Loss 18.978868\n",
      "\tParams: tensor([ 2.9804, -3.7905])\n",
      "\tGrad:tensor([-0.4062,  2.2997])\n",
      "Epoch 154,Loss 18.924377\n",
      "\tParams: tensor([ 2.9844, -3.8135])\n",
      "\tGrad:tensor([-0.4056,  2.2957])\n",
      "Epoch 155,Loss 18.870081\n",
      "\tParams: tensor([ 2.9885, -3.8364])\n",
      "\tGrad:tensor([-0.4049,  2.2918])\n",
      "Epoch 156,Loss 18.815960\n",
      "\tParams: tensor([ 2.9925, -3.8593])\n",
      "\tGrad:tensor([-0.4042,  2.2880])\n",
      "Epoch 157,Loss 18.762022\n",
      "\tParams: tensor([ 2.9966, -3.8821])\n",
      "\tGrad:tensor([-0.4035,  2.2841])\n",
      "Epoch 158,Loss 18.708271\n",
      "\tParams: tensor([ 3.0006, -3.9049])\n",
      "\tGrad:tensor([-0.4028,  2.2802])\n",
      "Epoch 159,Loss 18.654699\n",
      "\tParams: tensor([ 3.0046, -3.9277])\n",
      "\tGrad:tensor([-0.4021,  2.2763])\n",
      "Epoch 160,Loss 18.601313\n",
      "\tParams: tensor([ 3.0086, -3.9504])\n",
      "\tGrad:tensor([-0.4014,  2.2724])\n",
      "Epoch 161,Loss 18.548109\n",
      "\tParams: tensor([ 3.0126, -3.9731])\n",
      "\tGrad:tensor([-0.4007,  2.2686])\n",
      "Epoch 162,Loss 18.495085\n",
      "\tParams: tensor([ 3.0166, -3.9958])\n",
      "\tGrad:tensor([-0.4001,  2.2647])\n",
      "Epoch 163,Loss 18.442236\n",
      "\tParams: tensor([ 3.0206, -4.0184])\n",
      "\tGrad:tensor([-0.3994,  2.2609])\n",
      "Epoch 164,Loss 18.389570\n",
      "\tParams: tensor([ 3.0246, -4.0409])\n",
      "\tGrad:tensor([-0.3987,  2.2570])\n",
      "Epoch 165,Loss 18.337080\n",
      "\tParams: tensor([ 3.0286, -4.0635])\n",
      "\tGrad:tensor([-0.3980,  2.2532])\n",
      "Epoch 166,Loss 18.284777\n",
      "\tParams: tensor([ 3.0326, -4.0860])\n",
      "\tGrad:tensor([-0.3974,  2.2494])\n",
      "Epoch 167,Loss 18.232641\n",
      "\tParams: tensor([ 3.0365, -4.1084])\n",
      "\tGrad:tensor([-0.3967,  2.2456])\n",
      "Epoch 168,Loss 18.180685\n",
      "\tParams: tensor([ 3.0405, -4.1308])\n",
      "\tGrad:tensor([-0.3960,  2.2417])\n",
      "Epoch 169,Loss 18.128906\n",
      "\tParams: tensor([ 3.0445, -4.1532])\n",
      "\tGrad:tensor([-0.3953,  2.2379])\n",
      "Epoch 170,Loss 18.077301\n",
      "\tParams: tensor([ 3.0484, -4.1756])\n",
      "\tGrad:tensor([-0.3947,  2.2341])\n",
      "Epoch 171,Loss 18.025877\n",
      "\tParams: tensor([ 3.0523, -4.1979])\n",
      "\tGrad:tensor([-0.3940,  2.2303])\n",
      "Epoch 172,Loss 17.974623\n",
      "\tParams: tensor([ 3.0563, -4.2201])\n",
      "\tGrad:tensor([-0.3933,  2.2266])\n",
      "Epoch 173,Loss 17.923546\n",
      "\tParams: tensor([ 3.0602, -4.2424])\n",
      "\tGrad:tensor([-0.3927,  2.2228])\n",
      "Epoch 174,Loss 17.872643\n",
      "\tParams: tensor([ 3.0641, -4.2646])\n",
      "\tGrad:tensor([-0.3920,  2.2190])\n",
      "Epoch 175,Loss 17.821909\n",
      "\tParams: tensor([ 3.0680, -4.2867])\n",
      "\tGrad:tensor([-0.3913,  2.2152])\n",
      "Epoch 176,Loss 17.771345\n",
      "\tParams: tensor([ 3.0719, -4.3088])\n",
      "\tGrad:tensor([-0.3907,  2.2115])\n",
      "Epoch 177,Loss 17.720955\n",
      "\tParams: tensor([ 3.0758, -4.3309])\n",
      "\tGrad:tensor([-0.3900,  2.2077])\n",
      "Epoch 178,Loss 17.670738\n",
      "\tParams: tensor([ 3.0797, -4.3529])\n",
      "\tGrad:tensor([-0.3893,  2.2040])\n",
      "Epoch 179,Loss 17.620689\n",
      "\tParams: tensor([ 3.0836, -4.3749])\n",
      "\tGrad:tensor([-0.3887,  2.2002])\n",
      "Epoch 180,Loss 17.570814\n",
      "\tParams: tensor([ 3.0875, -4.3969])\n",
      "\tGrad:tensor([-0.3880,  2.1965])\n",
      "Epoch 181,Loss 17.521103\n",
      "\tParams: tensor([ 3.0914, -4.4188])\n",
      "\tGrad:tensor([-0.3873,  2.1927])\n",
      "Epoch 182,Loss 17.471565\n",
      "\tParams: tensor([ 3.0952, -4.4407])\n",
      "\tGrad:tensor([-0.3867,  2.1890])\n",
      "Epoch 183,Loss 17.422192\n",
      "\tParams: tensor([ 3.0991, -4.4626])\n",
      "\tGrad:tensor([-0.3860,  2.1853])\n",
      "Epoch 184,Loss 17.372993\n",
      "\tParams: tensor([ 3.1030, -4.4844])\n",
      "\tGrad:tensor([-0.3854,  2.1816])\n",
      "Epoch 185,Loss 17.323954\n",
      "\tParams: tensor([ 3.1068, -4.5062])\n",
      "\tGrad:tensor([-0.3847,  2.1779])\n",
      "Epoch 186,Loss 17.275084\n",
      "\tParams: tensor([ 3.1106, -4.5279])\n",
      "\tGrad:tensor([-0.3841,  2.1742])\n",
      "Epoch 187,Loss 17.226379\n",
      "\tParams: tensor([ 3.1145, -4.5496])\n",
      "\tGrad:tensor([-0.3834,  2.1705])\n",
      "Epoch 188,Loss 17.177839\n",
      "\tParams: tensor([ 3.1183, -4.5713])\n",
      "\tGrad:tensor([-0.3828,  2.1668])\n",
      "Epoch 189,Loss 17.129463\n",
      "\tParams: tensor([ 3.1221, -4.5929])\n",
      "\tGrad:tensor([-0.3821,  2.1631])\n",
      "Epoch 190,Loss 17.081255\n",
      "\tParams: tensor([ 3.1259, -4.6145])\n",
      "\tGrad:tensor([-0.3815,  2.1594])\n",
      "Epoch 191,Loss 17.033209\n",
      "\tParams: tensor([ 3.1298, -4.6361])\n",
      "\tGrad:tensor([-0.3808,  2.1558])\n",
      "Epoch 192,Loss 16.985327\n",
      "\tParams: tensor([ 3.1336, -4.6576])\n",
      "\tGrad:tensor([-0.3802,  2.1521])\n",
      "Epoch 193,Loss 16.937605\n",
      "\tParams: tensor([ 3.1374, -4.6791])\n",
      "\tGrad:tensor([-0.3795,  2.1485])\n",
      "Epoch 194,Loss 16.890047\n",
      "\tParams: tensor([ 3.1411, -4.7005])\n",
      "\tGrad:tensor([-0.3789,  2.1448])\n",
      "Epoch 195,Loss 16.842649\n",
      "\tParams: tensor([ 3.1449, -4.7219])\n",
      "\tGrad:tensor([-0.3782,  2.1412])\n",
      "Epoch 196,Loss 16.795412\n",
      "\tParams: tensor([ 3.1487, -4.7433])\n",
      "\tGrad:tensor([-0.3776,  2.1375])\n",
      "Epoch 197,Loss 16.748339\n",
      "\tParams: tensor([ 3.1525, -4.7646])\n",
      "\tGrad:tensor([-0.3770,  2.1339])\n",
      "Epoch 198,Loss 16.701422\n",
      "\tParams: tensor([ 3.1562, -4.7859])\n",
      "\tGrad:tensor([-0.3763,  2.1303])\n",
      "Epoch 199,Loss 16.654661\n",
      "\tParams: tensor([ 3.1600, -4.8072])\n",
      "\tGrad:tensor([-0.3757,  2.1267])\n",
      "Epoch 200,Loss 16.608067\n",
      "\tParams: tensor([ 3.1637, -4.8284])\n",
      "\tGrad:tensor([-0.3750,  2.1230])\n",
      "Epoch 201,Loss 16.561623\n",
      "\tParams: tensor([ 3.1675, -4.8496])\n",
      "\tGrad:tensor([-0.3744,  2.1194])\n",
      "Epoch 202,Loss 16.515343\n",
      "\tParams: tensor([ 3.1712, -4.8708])\n",
      "\tGrad:tensor([-0.3738,  2.1158])\n",
      "Epoch 203,Loss 16.469219\n",
      "\tParams: tensor([ 3.1750, -4.8919])\n",
      "\tGrad:tensor([-0.3731,  2.1122])\n",
      "Epoch 204,Loss 16.423248\n",
      "\tParams: tensor([ 3.1787, -4.9130])\n",
      "\tGrad:tensor([-0.3725,  2.1087])\n",
      "Epoch 205,Loss 16.377434\n",
      "\tParams: tensor([ 3.1824, -4.9341])\n",
      "\tGrad:tensor([-0.3719,  2.1051])\n",
      "Epoch 206,Loss 16.331776\n",
      "\tParams: tensor([ 3.1861, -4.9551])\n",
      "\tGrad:tensor([-0.3712,  2.1015])\n",
      "Epoch 207,Loss 16.286276\n",
      "\tParams: tensor([ 3.1898, -4.9760])\n",
      "\tGrad:tensor([-0.3706,  2.0979])\n",
      "Epoch 208,Loss 16.240929\n",
      "\tParams: tensor([ 3.1935, -4.9970])\n",
      "\tGrad:tensor([-0.3700,  2.0944])\n",
      "Epoch 209,Loss 16.195732\n",
      "\tParams: tensor([ 3.1972, -5.0179])\n",
      "\tGrad:tensor([-0.3694,  2.0908])\n",
      "Epoch 210,Loss 16.150694\n",
      "\tParams: tensor([ 3.2009, -5.0388])\n",
      "\tGrad:tensor([-0.3687,  2.0873])\n",
      "Epoch 211,Loss 16.105806\n",
      "\tParams: tensor([ 3.2046, -5.0596])\n",
      "\tGrad:tensor([-0.3681,  2.0837])\n",
      "Epoch 212,Loss 16.061073\n",
      "\tParams: tensor([ 3.2082, -5.0804])\n",
      "\tGrad:tensor([-0.3675,  2.0802])\n",
      "Epoch 213,Loss 16.016487\n",
      "\tParams: tensor([ 3.2119, -5.1012])\n",
      "\tGrad:tensor([-0.3668,  2.0766])\n",
      "Epoch 214,Loss 15.972058\n",
      "\tParams: tensor([ 3.2156, -5.1219])\n",
      "\tGrad:tensor([-0.3662,  2.0731])\n",
      "Epoch 215,Loss 15.927776\n",
      "\tParams: tensor([ 3.2192, -5.1426])\n",
      "\tGrad:tensor([-0.3656,  2.0696])\n",
      "Epoch 216,Loss 15.883645\n",
      "\tParams: tensor([ 3.2229, -5.1633])\n",
      "\tGrad:tensor([-0.3650,  2.0661])\n",
      "Epoch 217,Loss 15.839664\n",
      "\tParams: tensor([ 3.2265, -5.1839])\n",
      "\tGrad:tensor([-0.3644,  2.0626])\n",
      "Epoch 218,Loss 15.795832\n",
      "\tParams: tensor([ 3.2302, -5.2045])\n",
      "\tGrad:tensor([-0.3637,  2.0591])\n",
      "Epoch 219,Loss 15.752152\n",
      "\tParams: tensor([ 3.2338, -5.2250])\n",
      "\tGrad:tensor([-0.3631,  2.0556])\n",
      "Epoch 220,Loss 15.708612\n",
      "\tParams: tensor([ 3.2374, -5.2456])\n",
      "\tGrad:tensor([-0.3625,  2.0521])\n",
      "Epoch 221,Loss 15.665226\n",
      "\tParams: tensor([ 3.2410, -5.2660])\n",
      "\tGrad:tensor([-0.3619,  2.0486])\n",
      "Epoch 222,Loss 15.621990\n",
      "\tParams: tensor([ 3.2447, -5.2865])\n",
      "\tGrad:tensor([-0.3613,  2.0451])\n",
      "Epoch 223,Loss 15.578897\n",
      "\tParams: tensor([ 3.2483, -5.3069])\n",
      "\tGrad:tensor([-0.3607,  2.0416])\n",
      "Epoch 224,Loss 15.535950\n",
      "\tParams: tensor([ 3.2519, -5.3273])\n",
      "\tGrad:tensor([-0.3601,  2.0382])\n",
      "Epoch 225,Loss 15.493150\n",
      "\tParams: tensor([ 3.2555, -5.3476])\n",
      "\tGrad:tensor([-0.3594,  2.0347])\n",
      "Epoch 226,Loss 15.450495\n",
      "\tParams: tensor([ 3.2590, -5.3680])\n",
      "\tGrad:tensor([-0.3588,  2.0312])\n",
      "Epoch 227,Loss 15.407981\n",
      "\tParams: tensor([ 3.2626, -5.3882])\n",
      "\tGrad:tensor([-0.3582,  2.0278])\n",
      "Epoch 228,Loss 15.365616\n",
      "\tParams: tensor([ 3.2662, -5.4085])\n",
      "\tGrad:tensor([-0.3576,  2.0243])\n",
      "Epoch 229,Loss 15.323396\n",
      "\tParams: tensor([ 3.2698, -5.4287])\n",
      "\tGrad:tensor([-0.3570,  2.0209])\n",
      "Epoch 230,Loss 15.281317\n",
      "\tParams: tensor([ 3.2733, -5.4489])\n",
      "\tGrad:tensor([-0.3564,  2.0175])\n",
      "Epoch 231,Loss 15.239380\n",
      "\tParams: tensor([ 3.2769, -5.4690])\n",
      "\tGrad:tensor([-0.3558,  2.0140])\n",
      "Epoch 232,Loss 15.197585\n",
      "\tParams: tensor([ 3.2804, -5.4891])\n",
      "\tGrad:tensor([-0.3552,  2.0106])\n",
      "Epoch 233,Loss 15.155932\n",
      "\tParams: tensor([ 3.2840, -5.5092])\n",
      "\tGrad:tensor([-0.3546,  2.0072])\n",
      "Epoch 234,Loss 15.114425\n",
      "\tParams: tensor([ 3.2875, -5.5292])\n",
      "\tGrad:tensor([-0.3540,  2.0038])\n",
      "Epoch 235,Loss 15.073055\n",
      "\tParams: tensor([ 3.2911, -5.5492])\n",
      "\tGrad:tensor([-0.3534,  2.0004])\n",
      "Epoch 236,Loss 15.031823\n",
      "\tParams: tensor([ 3.2946, -5.5692])\n",
      "\tGrad:tensor([-0.3528,  1.9970])\n",
      "Epoch 237,Loss 14.990734\n",
      "\tParams: tensor([ 3.2981, -5.5891])\n",
      "\tGrad:tensor([-0.3522,  1.9936])\n",
      "Epoch 238,Loss 14.949784\n",
      "\tParams: tensor([ 3.3016, -5.6090])\n",
      "\tGrad:tensor([-0.3516,  1.9902])\n",
      "Epoch 239,Loss 14.908973\n",
      "\tParams: tensor([ 3.3051, -5.6289])\n",
      "\tGrad:tensor([-0.3510,  1.9868])\n",
      "Epoch 240,Loss 14.868304\n",
      "\tParams: tensor([ 3.3086, -5.6487])\n",
      "\tGrad:tensor([-0.3504,  1.9835])\n",
      "Epoch 241,Loss 14.827767\n",
      "\tParams: tensor([ 3.3121, -5.6685])\n",
      "\tGrad:tensor([-0.3498,  1.9801])\n",
      "Epoch 242,Loss 14.787370\n",
      "\tParams: tensor([ 3.3156, -5.6883])\n",
      "\tGrad:tensor([-0.3492,  1.9767])\n",
      "Epoch 243,Loss 14.747109\n",
      "\tParams: tensor([ 3.3191, -5.7080])\n",
      "\tGrad:tensor([-0.3486,  1.9734])\n",
      "Epoch 244,Loss 14.706989\n",
      "\tParams: tensor([ 3.3226, -5.7277])\n",
      "\tGrad:tensor([-0.3480,  1.9700])\n",
      "Epoch 245,Loss 14.667002\n",
      "\tParams: tensor([ 3.3261, -5.7474])\n",
      "\tGrad:tensor([-0.3474,  1.9667])\n",
      "Epoch 246,Loss 14.627151\n",
      "\tParams: tensor([ 3.3295, -5.7670])\n",
      "\tGrad:tensor([-0.3468,  1.9633])\n",
      "Epoch 247,Loss 14.587436\n",
      "\tParams: tensor([ 3.3330, -5.7866])\n",
      "\tGrad:tensor([-0.3462,  1.9600])\n",
      "Epoch 248,Loss 14.547855\n",
      "\tParams: tensor([ 3.3365, -5.8062])\n",
      "\tGrad:tensor([-0.3456,  1.9567])\n",
      "Epoch 249,Loss 14.508409\n",
      "\tParams: tensor([ 3.3399, -5.8257])\n",
      "\tGrad:tensor([-0.3451,  1.9533])\n",
      "Epoch 250,Loss 14.469097\n",
      "\tParams: tensor([ 3.3434, -5.8452])\n",
      "\tGrad:tensor([-0.3445,  1.9500])\n",
      "Epoch 251,Loss 14.429920\n",
      "\tParams: tensor([ 3.3468, -5.8647])\n",
      "\tGrad:tensor([-0.3439,  1.9467])\n",
      "Epoch 252,Loss 14.390870\n",
      "\tParams: tensor([ 3.3502, -5.8841])\n",
      "\tGrad:tensor([-0.3433,  1.9434])\n",
      "Epoch 253,Loss 14.351956\n",
      "\tParams: tensor([ 3.3537, -5.9035])\n",
      "\tGrad:tensor([-0.3427,  1.9401])\n",
      "Epoch 254,Loss 14.313177\n",
      "\tParams: tensor([ 3.3571, -5.9229])\n",
      "\tGrad:tensor([-0.3421,  1.9368])\n",
      "Epoch 255,Loss 14.274529\n",
      "\tParams: tensor([ 3.3605, -5.9422])\n",
      "\tGrad:tensor([-0.3416,  1.9335])\n",
      "Epoch 256,Loss 14.236009\n",
      "\tParams: tensor([ 3.3639, -5.9615])\n",
      "\tGrad:tensor([-0.3410,  1.9302])\n",
      "Epoch 257,Loss 14.197620\n",
      "\tParams: tensor([ 3.3673, -5.9808])\n",
      "\tGrad:tensor([-0.3404,  1.9269])\n",
      "Epoch 258,Loss 14.159363\n",
      "\tParams: tensor([ 3.3707, -6.0000])\n",
      "\tGrad:tensor([-0.3398,  1.9237])\n",
      "Epoch 259,Loss 14.121234\n",
      "\tParams: tensor([ 3.3741, -6.0192])\n",
      "\tGrad:tensor([-0.3392,  1.9204])\n",
      "Epoch 260,Loss 14.083236\n",
      "\tParams: tensor([ 3.3775, -6.0384])\n",
      "\tGrad:tensor([-0.3387,  1.9171])\n",
      "Epoch 261,Loss 14.045367\n",
      "\tParams: tensor([ 3.3809, -6.0576])\n",
      "\tGrad:tensor([-0.3381,  1.9139])\n",
      "Epoch 262,Loss 14.007627\n",
      "\tParams: tensor([ 3.3842, -6.0767])\n",
      "\tGrad:tensor([-0.3375,  1.9106])\n",
      "Epoch 263,Loss 13.970016\n",
      "\tParams: tensor([ 3.3876, -6.0957])\n",
      "\tGrad:tensor([-0.3369,  1.9074])\n",
      "Epoch 264,Loss 13.932531\n",
      "\tParams: tensor([ 3.3910, -6.1148])\n",
      "\tGrad:tensor([-0.3364,  1.9041])\n",
      "Epoch 265,Loss 13.895172\n",
      "\tParams: tensor([ 3.3943, -6.1338])\n",
      "\tGrad:tensor([-0.3358,  1.9009])\n",
      "Epoch 266,Loss 13.857944\n",
      "\tParams: tensor([ 3.3977, -6.1528])\n",
      "\tGrad:tensor([-0.3352,  1.8977])\n",
      "Epoch 267,Loss 13.820837\n",
      "\tParams: tensor([ 3.4010, -6.1717])\n",
      "\tGrad:tensor([-0.3347,  1.8945])\n",
      "Epoch 268,Loss 13.783858\n",
      "\tParams: tensor([ 3.4044, -6.1906])\n",
      "\tGrad:tensor([-0.3341,  1.8912])\n",
      "Epoch 269,Loss 13.747006\n",
      "\tParams: tensor([ 3.4077, -6.2095])\n",
      "\tGrad:tensor([-0.3335,  1.8880])\n",
      "Epoch 270,Loss 13.710278\n",
      "\tParams: tensor([ 3.4110, -6.2284])\n",
      "\tGrad:tensor([-0.3330,  1.8848])\n",
      "Epoch 271,Loss 13.673676\n",
      "\tParams: tensor([ 3.4144, -6.2472])\n",
      "\tGrad:tensor([-0.3324,  1.8816])\n",
      "Epoch 272,Loss 13.637196\n",
      "\tParams: tensor([ 3.4177, -6.2660])\n",
      "\tGrad:tensor([-0.3318,  1.8784])\n",
      "Epoch 273,Loss 13.600842\n",
      "\tParams: tensor([ 3.4210, -6.2847])\n",
      "\tGrad:tensor([-0.3313,  1.8752])\n",
      "Epoch 274,Loss 13.564609\n",
      "\tParams: tensor([ 3.4243, -6.3034])\n",
      "\tGrad:tensor([-0.3307,  1.8720])\n",
      "Epoch 275,Loss 13.528501\n",
      "\tParams: tensor([ 3.4276, -6.3221])\n",
      "\tGrad:tensor([-0.3301,  1.8689])\n",
      "Epoch 276,Loss 13.492514\n",
      "\tParams: tensor([ 3.4309, -6.3408])\n",
      "\tGrad:tensor([-0.3296,  1.8657])\n",
      "Epoch 277,Loss 13.456651\n",
      "\tParams: tensor([ 3.4342, -6.3594])\n",
      "\tGrad:tensor([-0.3290,  1.8625])\n",
      "Epoch 278,Loss 13.420910\n",
      "\tParams: tensor([ 3.4375, -6.3780])\n",
      "\tGrad:tensor([-0.3285,  1.8594])\n",
      "Epoch 279,Loss 13.385287\n",
      "\tParams: tensor([ 3.4407, -6.3966])\n",
      "\tGrad:tensor([-0.3279,  1.8562])\n",
      "Epoch 280,Loss 13.349789\n",
      "\tParams: tensor([ 3.4440, -6.4151])\n",
      "\tGrad:tensor([-0.3274,  1.8530])\n",
      "Epoch 281,Loss 13.314407\n",
      "\tParams: tensor([ 3.4473, -6.4336])\n",
      "\tGrad:tensor([-0.3268,  1.8499])\n",
      "Epoch 282,Loss 13.279150\n",
      "\tParams: tensor([ 3.4506, -6.4520])\n",
      "\tGrad:tensor([-0.3262,  1.8468])\n",
      "Epoch 283,Loss 13.244009\n",
      "\tParams: tensor([ 3.4538, -6.4705])\n",
      "\tGrad:tensor([-0.3257,  1.8436])\n",
      "Epoch 284,Loss 13.208991\n",
      "\tParams: tensor([ 3.4571, -6.4889])\n",
      "\tGrad:tensor([-0.3251,  1.8405])\n",
      "Epoch 285,Loss 13.174088\n",
      "\tParams: tensor([ 3.4603, -6.5073])\n",
      "\tGrad:tensor([-0.3246,  1.8374])\n",
      "Epoch 286,Loss 13.139307\n",
      "\tParams: tensor([ 3.4635, -6.5256])\n",
      "\tGrad:tensor([-0.3240,  1.8342])\n",
      "Epoch 287,Loss 13.104639\n",
      "\tParams: tensor([ 3.4668, -6.5439])\n",
      "\tGrad:tensor([-0.3235,  1.8311])\n",
      "Epoch 288,Loss 13.070092\n",
      "\tParams: tensor([ 3.4700, -6.5622])\n",
      "\tGrad:tensor([-0.3229,  1.8280])\n",
      "Epoch 289,Loss 13.035664\n",
      "\tParams: tensor([ 3.4732, -6.5804])\n",
      "\tGrad:tensor([-0.3224,  1.8249])\n",
      "Epoch 290,Loss 13.001349\n",
      "\tParams: tensor([ 3.4765, -6.5987])\n",
      "\tGrad:tensor([-0.3218,  1.8218])\n",
      "Epoch 291,Loss 12.967152\n",
      "\tParams: tensor([ 3.4797, -6.6169])\n",
      "\tGrad:tensor([-0.3213,  1.8187])\n",
      "Epoch 292,Loss 12.933075\n",
      "\tParams: tensor([ 3.4829, -6.6350])\n",
      "\tGrad:tensor([-0.3207,  1.8156])\n",
      "Epoch 293,Loss 12.899109\n",
      "\tParams: tensor([ 3.4861, -6.6531])\n",
      "\tGrad:tensor([-0.3202,  1.8125])\n",
      "Epoch 294,Loss 12.865259\n",
      "\tParams: tensor([ 3.4893, -6.6712])\n",
      "\tGrad:tensor([-0.3196,  1.8095])\n",
      "Epoch 295,Loss 12.831525\n",
      "\tParams: tensor([ 3.4925, -6.6893])\n",
      "\tGrad:tensor([-0.3191,  1.8064])\n",
      "Epoch 296,Loss 12.797904\n",
      "\tParams: tensor([ 3.4956, -6.7073])\n",
      "\tGrad:tensor([-0.3186,  1.8033])\n",
      "Epoch 297,Loss 12.764399\n",
      "\tParams: tensor([ 3.4988, -6.7253])\n",
      "\tGrad:tensor([-0.3180,  1.8003])\n",
      "Epoch 298,Loss 12.731007\n",
      "\tParams: tensor([ 3.5020, -6.7433])\n",
      "\tGrad:tensor([-0.3175,  1.7972])\n",
      "Epoch 299,Loss 12.697727\n",
      "\tParams: tensor([ 3.5052, -6.7612])\n",
      "\tGrad:tensor([-0.3169,  1.7941])\n",
      "Epoch 300,Loss 12.664559\n",
      "\tParams: tensor([ 3.5083, -6.7792])\n",
      "\tGrad:tensor([-0.3164,  1.7911])\n",
      "Epoch 301,Loss 12.631507\n",
      "\tParams: tensor([ 3.5115, -6.7970])\n",
      "\tGrad:tensor([-0.3159,  1.7881])\n",
      "Epoch 302,Loss 12.598568\n",
      "\tParams: tensor([ 3.5146, -6.8149])\n",
      "\tGrad:tensor([-0.3153,  1.7850])\n",
      "Epoch 303,Loss 12.565738\n",
      "\tParams: tensor([ 3.5178, -6.8327])\n",
      "\tGrad:tensor([-0.3148,  1.7820])\n",
      "Epoch 304,Loss 12.533021\n",
      "\tParams: tensor([ 3.5209, -6.8505])\n",
      "\tGrad:tensor([-0.3143,  1.7790])\n",
      "Epoch 305,Loss 12.500413\n",
      "\tParams: tensor([ 3.5241, -6.8683])\n",
      "\tGrad:tensor([-0.3137,  1.7759])\n",
      "Epoch 306,Loss 12.467919\n",
      "\tParams: tensor([ 3.5272, -6.8860])\n",
      "\tGrad:tensor([-0.3132,  1.7729])\n",
      "Epoch 307,Loss 12.435532\n",
      "\tParams: tensor([ 3.5303, -6.9037])\n",
      "\tGrad:tensor([-0.3127,  1.7699])\n",
      "Epoch 308,Loss 12.403256\n",
      "\tParams: tensor([ 3.5335, -6.9213])\n",
      "\tGrad:tensor([-0.3121,  1.7669])\n",
      "Epoch 309,Loss 12.371090\n",
      "\tParams: tensor([ 3.5366, -6.9390])\n",
      "\tGrad:tensor([-0.3116,  1.7639])\n",
      "Epoch 310,Loss 12.339031\n",
      "\tParams: tensor([ 3.5397, -6.9566])\n",
      "\tGrad:tensor([-0.3111,  1.7609])\n",
      "Epoch 311,Loss 12.307082\n",
      "\tParams: tensor([ 3.5428, -6.9742])\n",
      "\tGrad:tensor([-0.3105,  1.7579])\n",
      "Epoch 312,Loss 12.275247\n",
      "\tParams: tensor([ 3.5459, -6.9917])\n",
      "\tGrad:tensor([-0.3100,  1.7549])\n",
      "Epoch 313,Loss 12.243509\n",
      "\tParams: tensor([ 3.5490, -7.0092])\n",
      "\tGrad:tensor([-0.3095,  1.7519])\n",
      "Epoch 314,Loss 12.211887\n",
      "\tParams: tensor([ 3.5521, -7.0267])\n",
      "\tGrad:tensor([-0.3090,  1.7490])\n",
      "Epoch 315,Loss 12.180370\n",
      "\tParams: tensor([ 3.5552, -7.0442])\n",
      "\tGrad:tensor([-0.3084,  1.7460])\n",
      "Epoch 316,Loss 12.148962\n",
      "\tParams: tensor([ 3.5582, -7.0616])\n",
      "\tGrad:tensor([-0.3079,  1.7430])\n",
      "Epoch 317,Loss 12.117657\n",
      "\tParams: tensor([ 3.5613, -7.0790])\n",
      "\tGrad:tensor([-0.3074,  1.7401])\n",
      "Epoch 318,Loss 12.086462\n",
      "\tParams: tensor([ 3.5644, -7.0964])\n",
      "\tGrad:tensor([-0.3069,  1.7371])\n",
      "Epoch 319,Loss 12.055373\n",
      "\tParams: tensor([ 3.5674, -7.1137])\n",
      "\tGrad:tensor([-0.3063,  1.7342])\n",
      "Epoch 320,Loss 12.024384\n",
      "\tParams: tensor([ 3.5705, -7.1310])\n",
      "\tGrad:tensor([-0.3058,  1.7312])\n",
      "Epoch 321,Loss 11.993508\n",
      "\tParams: tensor([ 3.5736, -7.1483])\n",
      "\tGrad:tensor([-0.3053,  1.7283])\n",
      "Epoch 322,Loss 11.962731\n",
      "\tParams: tensor([ 3.5766, -7.1656])\n",
      "\tGrad:tensor([-0.3048,  1.7253])\n",
      "Epoch 323,Loss 11.932056\n",
      "\tParams: tensor([ 3.5796, -7.1828])\n",
      "\tGrad:tensor([-0.3043,  1.7224])\n",
      "Epoch 324,Loss 11.901492\n",
      "\tParams: tensor([ 3.5827, -7.2000])\n",
      "\tGrad:tensor([-0.3037,  1.7195])\n",
      "Epoch 325,Loss 11.871029\n",
      "\tParams: tensor([ 3.5857, -7.2172])\n",
      "\tGrad:tensor([-0.3032,  1.7166])\n",
      "Epoch 326,Loss 11.840671\n",
      "\tParams: tensor([ 3.5887, -7.2343])\n",
      "\tGrad:tensor([-0.3027,  1.7136])\n",
      "Epoch 327,Loss 11.810413\n",
      "\tParams: tensor([ 3.5918, -7.2514])\n",
      "\tGrad:tensor([-0.3022,  1.7107])\n",
      "Epoch 328,Loss 11.780257\n",
      "\tParams: tensor([ 3.5948, -7.2685])\n",
      "\tGrad:tensor([-0.3017,  1.7078])\n",
      "Epoch 329,Loss 11.750208\n",
      "\tParams: tensor([ 3.5978, -7.2855])\n",
      "\tGrad:tensor([-0.3012,  1.7049])\n",
      "Epoch 330,Loss 11.720258\n",
      "\tParams: tensor([ 3.6008, -7.3026])\n",
      "\tGrad:tensor([-0.3007,  1.7020])\n",
      "Epoch 331,Loss 11.690412\n",
      "\tParams: tensor([ 3.6038, -7.3196])\n",
      "\tGrad:tensor([-0.3002,  1.6991])\n",
      "Epoch 332,Loss 11.660664\n",
      "\tParams: tensor([ 3.6068, -7.3365])\n",
      "\tGrad:tensor([-0.2996,  1.6963])\n",
      "Epoch 333,Loss 11.631015\n",
      "\tParams: tensor([ 3.6098, -7.3535])\n",
      "\tGrad:tensor([-0.2991,  1.6934])\n",
      "Epoch 334,Loss 11.601473\n",
      "\tParams: tensor([ 3.6128, -7.3704])\n",
      "\tGrad:tensor([-0.2986,  1.6905])\n",
      "Epoch 335,Loss 11.572030\n",
      "\tParams: tensor([ 3.6158, -7.3872])\n",
      "\tGrad:tensor([-0.2981,  1.6876])\n",
      "Epoch 336,Loss 11.542686\n",
      "\tParams: tensor([ 3.6187, -7.4041])\n",
      "\tGrad:tensor([-0.2976,  1.6848])\n",
      "Epoch 337,Loss 11.513440\n",
      "\tParams: tensor([ 3.6217, -7.4209])\n",
      "\tGrad:tensor([-0.2971,  1.6819])\n",
      "Epoch 338,Loss 11.484293\n",
      "\tParams: tensor([ 3.6247, -7.4377])\n",
      "\tGrad:tensor([-0.2966,  1.6790])\n",
      "Epoch 339,Loss 11.455246\n",
      "\tParams: tensor([ 3.6276, -7.4545])\n",
      "\tGrad:tensor([-0.2961,  1.6762])\n",
      "Epoch 340,Loss 11.426300\n",
      "\tParams: tensor([ 3.6306, -7.4712])\n",
      "\tGrad:tensor([-0.2956,  1.6733])\n",
      "Epoch 341,Loss 11.397448\n",
      "\tParams: tensor([ 3.6335, -7.4879])\n",
      "\tGrad:tensor([-0.2951,  1.6705])\n",
      "Epoch 342,Loss 11.368696\n",
      "\tParams: tensor([ 3.6365, -7.5046])\n",
      "\tGrad:tensor([-0.2946,  1.6677])\n",
      "Epoch 343,Loss 11.340043\n",
      "\tParams: tensor([ 3.6394, -7.5212])\n",
      "\tGrad:tensor([-0.2941,  1.6648])\n",
      "Epoch 344,Loss 11.311487\n",
      "\tParams: tensor([ 3.6424, -7.5378])\n",
      "\tGrad:tensor([-0.2936,  1.6620])\n",
      "Epoch 345,Loss 11.283028\n",
      "\tParams: tensor([ 3.6453, -7.5544])\n",
      "\tGrad:tensor([-0.2931,  1.6592])\n",
      "Epoch 346,Loss 11.254662\n",
      "\tParams: tensor([ 3.6482, -7.5710])\n",
      "\tGrad:tensor([-0.2926,  1.6564])\n",
      "Epoch 347,Loss 11.226396\n",
      "\tParams: tensor([ 3.6511, -7.5875])\n",
      "\tGrad:tensor([-0.2921,  1.6535])\n",
      "Epoch 348,Loss 11.198220\n",
      "\tParams: tensor([ 3.6541, -7.6040])\n",
      "\tGrad:tensor([-0.2916,  1.6507])\n",
      "Epoch 349,Loss 11.170150\n",
      "\tParams: tensor([ 3.6570, -7.6205])\n",
      "\tGrad:tensor([-0.2911,  1.6479])\n",
      "Epoch 350,Loss 11.142170\n",
      "\tParams: tensor([ 3.6599, -7.6370])\n",
      "\tGrad:tensor([-0.2906,  1.6451])\n",
      "Epoch 351,Loss 11.114282\n",
      "\tParams: tensor([ 3.6628, -7.6534])\n",
      "\tGrad:tensor([-0.2901,  1.6423])\n",
      "Epoch 352,Loss 11.086491\n",
      "\tParams: tensor([ 3.6657, -7.6698])\n",
      "\tGrad:tensor([-0.2896,  1.6395])\n",
      "Epoch 353,Loss 11.058797\n",
      "\tParams: tensor([ 3.6686, -7.6861])\n",
      "\tGrad:tensor([-0.2892,  1.6368])\n",
      "Epoch 354,Loss 11.031193\n",
      "\tParams: tensor([ 3.6714, -7.7025])\n",
      "\tGrad:tensor([-0.2886,  1.6340])\n",
      "Epoch 355,Loss 11.003686\n",
      "\tParams: tensor([ 3.6743, -7.7188])\n",
      "\tGrad:tensor([-0.2882,  1.6312])\n",
      "Epoch 356,Loss 10.976270\n",
      "\tParams: tensor([ 3.6772, -7.7351])\n",
      "\tGrad:tensor([-0.2877,  1.6284])\n",
      "Epoch 357,Loss 10.948948\n",
      "\tParams: tensor([ 3.6801, -7.7513])\n",
      "\tGrad:tensor([-0.2872,  1.6257])\n",
      "Epoch 358,Loss 10.921719\n",
      "\tParams: tensor([ 3.6829, -7.7676])\n",
      "\tGrad:tensor([-0.2867,  1.6229])\n",
      "Epoch 359,Loss 10.894581\n",
      "\tParams: tensor([ 3.6858, -7.7838])\n",
      "\tGrad:tensor([-0.2862,  1.6201])\n",
      "Epoch 360,Loss 10.867537\n",
      "\tParams: tensor([ 3.6887, -7.7999])\n",
      "\tGrad:tensor([-0.2857,  1.6174])\n",
      "Epoch 361,Loss 10.840583\n",
      "\tParams: tensor([ 3.6915, -7.8161])\n",
      "\tGrad:tensor([-0.2852,  1.6146])\n",
      "Epoch 362,Loss 10.813721\n",
      "\tParams: tensor([ 3.6944, -7.8322])\n",
      "\tGrad:tensor([-0.2847,  1.6119])\n",
      "Epoch 363,Loss 10.786950\n",
      "\tParams: tensor([ 3.6972, -7.8483])\n",
      "\tGrad:tensor([-0.2843,  1.6092])\n",
      "Epoch 364,Loss 10.760270\n",
      "\tParams: tensor([ 3.7000, -7.8644])\n",
      "\tGrad:tensor([-0.2838,  1.6064])\n",
      "Epoch 365,Loss 10.733681\n",
      "\tParams: tensor([ 3.7029, -7.8804])\n",
      "\tGrad:tensor([-0.2833,  1.6037])\n",
      "Epoch 366,Loss 10.707184\n",
      "\tParams: tensor([ 3.7057, -7.8964])\n",
      "\tGrad:tensor([-0.2828,  1.6010])\n",
      "Epoch 367,Loss 10.680775\n",
      "\tParams: tensor([ 3.7085, -7.9124])\n",
      "\tGrad:tensor([-0.2823,  1.5983])\n",
      "Epoch 368,Loss 10.654454\n",
      "\tParams: tensor([ 3.7113, -7.9284])\n",
      "\tGrad:tensor([-0.2819,  1.5955])\n",
      "Epoch 369,Loss 10.628225\n",
      "\tParams: tensor([ 3.7142, -7.9443])\n",
      "\tGrad:tensor([-0.2814,  1.5928])\n",
      "Epoch 370,Loss 10.602086\n",
      "\tParams: tensor([ 3.7170, -7.9602])\n",
      "\tGrad:tensor([-0.2809,  1.5901])\n",
      "Epoch 371,Loss 10.576034\n",
      "\tParams: tensor([ 3.7198, -7.9761])\n",
      "\tGrad:tensor([-0.2804,  1.5874])\n",
      "Epoch 372,Loss 10.550071\n",
      "\tParams: tensor([ 3.7226, -7.9919])\n",
      "\tGrad:tensor([-0.2799,  1.5847])\n",
      "Epoch 373,Loss 10.524194\n",
      "\tParams: tensor([ 3.7254, -8.0077])\n",
      "\tGrad:tensor([-0.2795,  1.5820])\n",
      "Epoch 374,Loss 10.498409\n",
      "\tParams: tensor([ 3.7282, -8.0235])\n",
      "\tGrad:tensor([-0.2790,  1.5794])\n",
      "Epoch 375,Loss 10.472707\n",
      "\tParams: tensor([ 3.7309, -8.0393])\n",
      "\tGrad:tensor([-0.2785,  1.5767])\n",
      "Epoch 376,Loss 10.447093\n",
      "\tParams: tensor([ 3.7337, -8.0550])\n",
      "\tGrad:tensor([-0.2780,  1.5740])\n",
      "Epoch 377,Loss 10.421569\n",
      "\tParams: tensor([ 3.7365, -8.0707])\n",
      "\tGrad:tensor([-0.2776,  1.5713])\n",
      "Epoch 378,Loss 10.396132\n",
      "\tParams: tensor([ 3.7393, -8.0864])\n",
      "\tGrad:tensor([-0.2771,  1.5686])\n",
      "Epoch 379,Loss 10.370779\n",
      "\tParams: tensor([ 3.7420, -8.1021])\n",
      "\tGrad:tensor([-0.2766,  1.5660])\n",
      "Epoch 380,Loss 10.345510\n",
      "\tParams: tensor([ 3.7448, -8.1177])\n",
      "\tGrad:tensor([-0.2762,  1.5633])\n",
      "Epoch 381,Loss 10.320328\n",
      "\tParams: tensor([ 3.7476, -8.1333])\n",
      "\tGrad:tensor([-0.2757,  1.5607])\n",
      "Epoch 382,Loss 10.295234\n",
      "\tParams: tensor([ 3.7503, -8.1489])\n",
      "\tGrad:tensor([-0.2752,  1.5580])\n",
      "Epoch 383,Loss 10.270224\n",
      "\tParams: tensor([ 3.7531, -8.1645])\n",
      "\tGrad:tensor([-0.2748,  1.5554])\n",
      "Epoch 384,Loss 10.245296\n",
      "\tParams: tensor([ 3.7558, -8.1800])\n",
      "\tGrad:tensor([-0.2743,  1.5527])\n",
      "Epoch 385,Loss 10.220457\n",
      "\tParams: tensor([ 3.7585, -8.1955])\n",
      "\tGrad:tensor([-0.2738,  1.5501])\n",
      "Epoch 386,Loss 10.195701\n",
      "\tParams: tensor([ 3.7613, -8.2110])\n",
      "\tGrad:tensor([-0.2734,  1.5475])\n",
      "Epoch 387,Loss 10.171029\n",
      "\tParams: tensor([ 3.7640, -8.2264])\n",
      "\tGrad:tensor([-0.2729,  1.5448])\n",
      "Epoch 388,Loss 10.146438\n",
      "\tParams: tensor([ 3.7667, -8.2418])\n",
      "\tGrad:tensor([-0.2724,  1.5422])\n",
      "Epoch 389,Loss 10.121935\n",
      "\tParams: tensor([ 3.7694, -8.2572])\n",
      "\tGrad:tensor([-0.2720,  1.5396])\n",
      "Epoch 390,Loss 10.097512\n",
      "\tParams: tensor([ 3.7722, -8.2726])\n",
      "\tGrad:tensor([-0.2715,  1.5370])\n",
      "Epoch 391,Loss 10.073173\n",
      "\tParams: tensor([ 3.7749, -8.2879])\n",
      "\tGrad:tensor([-0.2711,  1.5344])\n",
      "Epoch 392,Loss 10.048919\n",
      "\tParams: tensor([ 3.7776, -8.3033])\n",
      "\tGrad:tensor([-0.2706,  1.5317])\n",
      "Epoch 393,Loss 10.024743\n",
      "\tParams: tensor([ 3.7803, -8.3185])\n",
      "\tGrad:tensor([-0.2701,  1.5291])\n",
      "Epoch 394,Loss 10.000652\n",
      "\tParams: tensor([ 3.7830, -8.3338])\n",
      "\tGrad:tensor([-0.2697,  1.5265])\n",
      "Epoch 395,Loss 9.976640\n",
      "\tParams: tensor([ 3.7857, -8.3491])\n",
      "\tGrad:tensor([-0.2692,  1.5240])\n",
      "Epoch 396,Loss 9.952712\n",
      "\tParams: tensor([ 3.7884, -8.3643])\n",
      "\tGrad:tensor([-0.2688,  1.5214])\n",
      "Epoch 397,Loss 9.928862\n",
      "\tParams: tensor([ 3.7910, -8.3795])\n",
      "\tGrad:tensor([-0.2683,  1.5188])\n",
      "Epoch 398,Loss 9.905093\n",
      "\tParams: tensor([ 3.7937, -8.3946])\n",
      "\tGrad:tensor([-0.2678,  1.5162])\n",
      "Epoch 399,Loss 9.881409\n",
      "\tParams: tensor([ 3.7964, -8.4098])\n",
      "\tGrad:tensor([-0.2674,  1.5136])\n",
      "Epoch 400,Loss 9.857804\n",
      "\tParams: tensor([ 3.7991, -8.4249])\n",
      "\tGrad:tensor([-0.2669,  1.5111])\n",
      "Epoch 401,Loss 9.834277\n",
      "\tParams: tensor([ 3.8017, -8.4399])\n",
      "\tGrad:tensor([-0.2665,  1.5085])\n",
      "Epoch 402,Loss 9.810831\n",
      "\tParams: tensor([ 3.8044, -8.4550])\n",
      "\tGrad:tensor([-0.2660,  1.5059])\n",
      "Epoch 403,Loss 9.787466\n",
      "\tParams: tensor([ 3.8070, -8.4700])\n",
      "\tGrad:tensor([-0.2656,  1.5034])\n",
      "Epoch 404,Loss 9.764176\n",
      "\tParams: tensor([ 3.8097, -8.4851])\n",
      "\tGrad:tensor([-0.2651,  1.5008])\n",
      "Epoch 405,Loss 9.740973\n",
      "\tParams: tensor([ 3.8123, -8.5000])\n",
      "\tGrad:tensor([-0.2647,  1.4983])\n",
      "Epoch 406,Loss 9.717843\n",
      "\tParams: tensor([ 3.8150, -8.5150])\n",
      "\tGrad:tensor([-0.2642,  1.4957])\n",
      "Epoch 407,Loss 9.694793\n",
      "\tParams: tensor([ 3.8176, -8.5299])\n",
      "\tGrad:tensor([-0.2638,  1.4932])\n",
      "Epoch 408,Loss 9.671824\n",
      "\tParams: tensor([ 3.8202, -8.5448])\n",
      "\tGrad:tensor([-0.2633,  1.4906])\n",
      "Epoch 409,Loss 9.648926\n",
      "\tParams: tensor([ 3.8229, -8.5597])\n",
      "\tGrad:tensor([-0.2629,  1.4881])\n",
      "Epoch 410,Loss 9.626110\n",
      "\tParams: tensor([ 3.8255, -8.5746])\n",
      "\tGrad:tensor([-0.2624,  1.4856])\n",
      "Epoch 411,Loss 9.603373\n",
      "\tParams: tensor([ 3.8281, -8.5894])\n",
      "\tGrad:tensor([-0.2620,  1.4831])\n",
      "Epoch 412,Loss 9.580709\n",
      "\tParams: tensor([ 3.8307, -8.6042])\n",
      "\tGrad:tensor([-0.2615,  1.4805])\n",
      "Epoch 413,Loss 9.558125\n",
      "\tParams: tensor([ 3.8333, -8.6190])\n",
      "\tGrad:tensor([-0.2611,  1.4780])\n",
      "Epoch 414,Loss 9.535617\n",
      "\tParams: tensor([ 3.8360, -8.6337])\n",
      "\tGrad:tensor([-0.2606,  1.4755])\n",
      "Epoch 415,Loss 9.513184\n",
      "\tParams: tensor([ 3.8386, -8.6485])\n",
      "\tGrad:tensor([-0.2602,  1.4730])\n",
      "Epoch 416,Loss 9.490829\n",
      "\tParams: tensor([ 3.8412, -8.6632])\n",
      "\tGrad:tensor([-0.2598,  1.4705])\n",
      "Epoch 417,Loss 9.468551\n",
      "\tParams: tensor([ 3.8437, -8.6779])\n",
      "\tGrad:tensor([-0.2593,  1.4680])\n",
      "Epoch 418,Loss 9.446347\n",
      "\tParams: tensor([ 3.8463, -8.6925])\n",
      "\tGrad:tensor([-0.2589,  1.4655])\n",
      "Epoch 419,Loss 9.424216\n",
      "\tParams: tensor([ 3.8489, -8.7071])\n",
      "\tGrad:tensor([-0.2584,  1.4630])\n",
      "Epoch 420,Loss 9.402164\n",
      "\tParams: tensor([ 3.8515, -8.7217])\n",
      "\tGrad:tensor([-0.2580,  1.4605])\n",
      "Epoch 421,Loss 9.380184\n",
      "\tParams: tensor([ 3.8541, -8.7363])\n",
      "\tGrad:tensor([-0.2576,  1.4581])\n",
      "Epoch 422,Loss 9.358282\n",
      "\tParams: tensor([ 3.8566, -8.7509])\n",
      "\tGrad:tensor([-0.2571,  1.4556])\n",
      "Epoch 423,Loss 9.336448\n",
      "\tParams: tensor([ 3.8592, -8.7654])\n",
      "\tGrad:tensor([-0.2567,  1.4531])\n",
      "Epoch 424,Loss 9.314695\n",
      "\tParams: tensor([ 3.8618, -8.7799])\n",
      "\tGrad:tensor([-0.2563,  1.4506])\n",
      "Epoch 425,Loss 9.293012\n",
      "\tParams: tensor([ 3.8643, -8.7944])\n",
      "\tGrad:tensor([-0.2558,  1.4482])\n",
      "Epoch 426,Loss 9.271403\n",
      "\tParams: tensor([ 3.8669, -8.8089])\n",
      "\tGrad:tensor([-0.2554,  1.4457])\n",
      "Epoch 427,Loss 9.249871\n",
      "\tParams: tensor([ 3.8694, -8.8233])\n",
      "\tGrad:tensor([-0.2550,  1.4433])\n",
      "Epoch 428,Loss 9.228410\n",
      "\tParams: tensor([ 3.8720, -8.8377])\n",
      "\tGrad:tensor([-0.2545,  1.4408])\n",
      "Epoch 429,Loss 9.207022\n",
      "\tParams: tensor([ 3.8745, -8.8521])\n",
      "\tGrad:tensor([-0.2541,  1.4384])\n",
      "Epoch 430,Loss 9.185704\n",
      "\tParams: tensor([ 3.8771, -8.8664])\n",
      "\tGrad:tensor([-0.2537,  1.4359])\n",
      "Epoch 431,Loss 9.164462\n",
      "\tParams: tensor([ 3.8796, -8.8808])\n",
      "\tGrad:tensor([-0.2532,  1.4335])\n",
      "Epoch 432,Loss 9.143289\n",
      "\tParams: tensor([ 3.8821, -8.8951])\n",
      "\tGrad:tensor([-0.2528,  1.4310])\n",
      "Epoch 433,Loss 9.122189\n",
      "\tParams: tensor([ 3.8846, -8.9094])\n",
      "\tGrad:tensor([-0.2524,  1.4286])\n",
      "Epoch 434,Loss 9.101160\n",
      "\tParams: tensor([ 3.8872, -8.9236])\n",
      "\tGrad:tensor([-0.2519,  1.4262])\n",
      "Epoch 435,Loss 9.080204\n",
      "\tParams: tensor([ 3.8897, -8.9379])\n",
      "\tGrad:tensor([-0.2515,  1.4238])\n",
      "Epoch 436,Loss 9.059318\n",
      "\tParams: tensor([ 3.8922, -8.9521])\n",
      "\tGrad:tensor([-0.2511,  1.4213])\n",
      "Epoch 437,Loss 9.038502\n",
      "\tParams: tensor([ 3.8947, -8.9663])\n",
      "\tGrad:tensor([-0.2507,  1.4189])\n",
      "Epoch 438,Loss 9.017757\n",
      "\tParams: tensor([ 3.8972, -8.9804])\n",
      "\tGrad:tensor([-0.2502,  1.4165])\n",
      "Epoch 439,Loss 8.997084\n",
      "\tParams: tensor([ 3.8997, -8.9946])\n",
      "\tGrad:tensor([-0.2498,  1.4141])\n",
      "Epoch 440,Loss 8.976479\n",
      "\tParams: tensor([ 3.9022, -9.0087])\n",
      "\tGrad:tensor([-0.2494,  1.4117])\n",
      "Epoch 441,Loss 8.955944\n",
      "\tParams: tensor([ 3.9047, -9.0228])\n",
      "\tGrad:tensor([-0.2489,  1.4093])\n",
      "Epoch 442,Loss 8.935480\n",
      "\tParams: tensor([ 3.9072, -9.0369])\n",
      "\tGrad:tensor([-0.2485,  1.4069])\n",
      "Epoch 443,Loss 8.915089\n",
      "\tParams: tensor([ 3.9096, -9.0509])\n",
      "\tGrad:tensor([-0.2481,  1.4045])\n",
      "Epoch 444,Loss 8.894762\n",
      "\tParams: tensor([ 3.9121, -9.0649])\n",
      "\tGrad:tensor([-0.2477,  1.4021])\n",
      "Epoch 445,Loss 8.874508\n",
      "\tParams: tensor([ 3.9146, -9.0789])\n",
      "\tGrad:tensor([-0.2473,  1.3998])\n",
      "Epoch 446,Loss 8.854318\n",
      "\tParams: tensor([ 3.9171, -9.0929])\n",
      "\tGrad:tensor([-0.2468,  1.3974])\n",
      "Epoch 447,Loss 8.834197\n",
      "\tParams: tensor([ 3.9195, -9.1068])\n",
      "\tGrad:tensor([-0.2464,  1.3950])\n",
      "Epoch 448,Loss 8.814149\n",
      "\tParams: tensor([ 3.9220, -9.1208])\n",
      "\tGrad:tensor([-0.2460,  1.3926])\n",
      "Epoch 449,Loss 8.794162\n",
      "\tParams: tensor([ 3.9244, -9.1347])\n",
      "\tGrad:tensor([-0.2456,  1.3903])\n",
      "Epoch 450,Loss 8.774253\n",
      "\tParams: tensor([ 3.9269, -9.1486])\n",
      "\tGrad:tensor([-0.2452,  1.3879])\n",
      "Epoch 451,Loss 8.754405\n",
      "\tParams: tensor([ 3.9293, -9.1624])\n",
      "\tGrad:tensor([-0.2448,  1.3856])\n",
      "Epoch 452,Loss 8.734623\n",
      "\tParams: tensor([ 3.9318, -9.1762])\n",
      "\tGrad:tensor([-0.2443,  1.3832])\n",
      "Epoch 453,Loss 8.714911\n",
      "\tParams: tensor([ 3.9342, -9.1901])\n",
      "\tGrad:tensor([-0.2439,  1.3808])\n",
      "Epoch 454,Loss 8.695266\n",
      "\tParams: tensor([ 3.9367, -9.2038])\n",
      "\tGrad:tensor([-0.2435,  1.3785])\n",
      "Epoch 455,Loss 8.675688\n",
      "\tParams: tensor([ 3.9391, -9.2176])\n",
      "\tGrad:tensor([-0.2431,  1.3762])\n",
      "Epoch 456,Loss 8.656173\n",
      "\tParams: tensor([ 3.9415, -9.2313])\n",
      "\tGrad:tensor([-0.2427,  1.3738])\n",
      "Epoch 457,Loss 8.636729\n",
      "\tParams: tensor([ 3.9439, -9.2451])\n",
      "\tGrad:tensor([-0.2423,  1.3715])\n",
      "Epoch 458,Loss 8.617347\n",
      "\tParams: tensor([ 3.9464, -9.2587])\n",
      "\tGrad:tensor([-0.2419,  1.3692])\n",
      "Epoch 459,Loss 8.598029\n",
      "\tParams: tensor([ 3.9488, -9.2724])\n",
      "\tGrad:tensor([-0.2414,  1.3668])\n",
      "Epoch 460,Loss 8.578781\n",
      "\tParams: tensor([ 3.9512, -9.2861])\n",
      "\tGrad:tensor([-0.2410,  1.3645])\n",
      "Epoch 461,Loss 8.559597\n",
      "\tParams: tensor([ 3.9536, -9.2997])\n",
      "\tGrad:tensor([-0.2406,  1.3622])\n",
      "Epoch 462,Loss 8.540479\n",
      "\tParams: tensor([ 3.9560, -9.3133])\n",
      "\tGrad:tensor([-0.2402,  1.3599])\n",
      "Epoch 463,Loss 8.521426\n",
      "\tParams: tensor([ 3.9584, -9.3269])\n",
      "\tGrad:tensor([-0.2398,  1.3576])\n",
      "Epoch 464,Loss 8.502437\n",
      "\tParams: tensor([ 3.9608, -9.3404])\n",
      "\tGrad:tensor([-0.2394,  1.3553])\n",
      "Epoch 465,Loss 8.483517\n",
      "\tParams: tensor([ 3.9632, -9.3539])\n",
      "\tGrad:tensor([-0.2390,  1.3530])\n",
      "Epoch 466,Loss 8.464652\n",
      "\tParams: tensor([ 3.9656, -9.3674])\n",
      "\tGrad:tensor([-0.2386,  1.3507])\n",
      "Epoch 467,Loss 8.445858\n",
      "\tParams: tensor([ 3.9679, -9.3809])\n",
      "\tGrad:tensor([-0.2382,  1.3484])\n",
      "Epoch 468,Loss 8.427128\n",
      "\tParams: tensor([ 3.9703, -9.3944])\n",
      "\tGrad:tensor([-0.2378,  1.3461])\n",
      "Epoch 469,Loss 8.408454\n",
      "\tParams: tensor([ 3.9727, -9.4078])\n",
      "\tGrad:tensor([-0.2374,  1.3438])\n",
      "Epoch 470,Loss 8.389848\n",
      "\tParams: tensor([ 3.9751, -9.4212])\n",
      "\tGrad:tensor([-0.2370,  1.3415])\n",
      "Epoch 471,Loss 8.371305\n",
      "\tParams: tensor([ 3.9774, -9.4346])\n",
      "\tGrad:tensor([-0.2366,  1.3392])\n",
      "Epoch 472,Loss 8.352828\n",
      "\tParams: tensor([ 3.9798, -9.4480])\n",
      "\tGrad:tensor([-0.2362,  1.3370])\n",
      "Epoch 473,Loss 8.334409\n",
      "\tParams: tensor([ 3.9822, -9.4614])\n",
      "\tGrad:tensor([-0.2358,  1.3347])\n",
      "Epoch 474,Loss 8.316054\n",
      "\tParams: tensor([ 3.9845, -9.4747])\n",
      "\tGrad:tensor([-0.2354,  1.3324])\n",
      "Epoch 475,Loss 8.297764\n",
      "\tParams: tensor([ 3.9869, -9.4880])\n",
      "\tGrad:tensor([-0.2350,  1.3301])\n",
      "Epoch 476,Loss 8.279534\n",
      "\tParams: tensor([ 3.9892, -9.5013])\n",
      "\tGrad:tensor([-0.2346,  1.3279])\n",
      "Epoch 477,Loss 8.261369\n",
      "\tParams: tensor([ 3.9915, -9.5145])\n",
      "\tGrad:tensor([-0.2342,  1.3256])\n",
      "Epoch 478,Loss 8.243259\n",
      "\tParams: tensor([ 3.9939, -9.5277])\n",
      "\tGrad:tensor([-0.2338,  1.3234])\n",
      "Epoch 479,Loss 8.225213\n",
      "\tParams: tensor([ 3.9962, -9.5410])\n",
      "\tGrad:tensor([-0.2334,  1.3211])\n",
      "Epoch 480,Loss 8.207231\n",
      "\tParams: tensor([ 3.9985, -9.5541])\n",
      "\tGrad:tensor([-0.2330,  1.3189])\n",
      "Epoch 481,Loss 8.189310\n",
      "\tParams: tensor([ 4.0009, -9.5673])\n",
      "\tGrad:tensor([-0.2326,  1.3166])\n",
      "Epoch 482,Loss 8.171452\n",
      "\tParams: tensor([ 4.0032, -9.5805])\n",
      "\tGrad:tensor([-0.2322,  1.3144])\n",
      "Epoch 483,Loss 8.153647\n",
      "\tParams: tensor([ 4.0055, -9.5936])\n",
      "\tGrad:tensor([-0.2318,  1.3122])\n",
      "Epoch 484,Loss 8.135906\n",
      "\tParams: tensor([ 4.0078, -9.6067])\n",
      "\tGrad:tensor([-0.2314,  1.3100])\n",
      "Epoch 485,Loss 8.118226\n",
      "\tParams: tensor([ 4.0101, -9.6198])\n",
      "\tGrad:tensor([-0.2310,  1.3077])\n",
      "Epoch 486,Loss 8.100607\n",
      "\tParams: tensor([ 4.0124, -9.6328])\n",
      "\tGrad:tensor([-0.2306,  1.3055])\n",
      "Epoch 487,Loss 8.083045\n",
      "\tParams: tensor([ 4.0147, -9.6458])\n",
      "\tGrad:tensor([-0.2302,  1.3033])\n",
      "Epoch 488,Loss 8.065548\n",
      "\tParams: tensor([ 4.0170, -9.6589])\n",
      "\tGrad:tensor([-0.2298,  1.3011])\n",
      "Epoch 489,Loss 8.048104\n",
      "\tParams: tensor([ 4.0193, -9.6718])\n",
      "\tGrad:tensor([-0.2295,  1.2989])\n",
      "Epoch 490,Loss 8.030724\n",
      "\tParams: tensor([ 4.0216, -9.6848])\n",
      "\tGrad:tensor([-0.2291,  1.2967])\n",
      "Epoch 491,Loss 8.013401\n",
      "\tParams: tensor([ 4.0239, -9.6978])\n",
      "\tGrad:tensor([-0.2287,  1.2945])\n",
      "Epoch 492,Loss 7.996137\n",
      "\tParams: tensor([ 4.0262, -9.7107])\n",
      "\tGrad:tensor([-0.2283,  1.2923])\n",
      "Epoch 493,Loss 7.978930\n",
      "\tParams: tensor([ 4.0285, -9.7236])\n",
      "\tGrad:tensor([-0.2279,  1.2901])\n",
      "Epoch 494,Loss 7.961783\n",
      "\tParams: tensor([ 4.0308, -9.7365])\n",
      "\tGrad:tensor([-0.2275,  1.2879])\n",
      "Epoch 495,Loss 7.944690\n",
      "\tParams: tensor([ 4.0330, -9.7493])\n",
      "\tGrad:tensor([-0.2271,  1.2857])\n",
      "Epoch 496,Loss 7.927663\n",
      "\tParams: tensor([ 4.0353, -9.7621])\n",
      "\tGrad:tensor([-0.2267,  1.2835])\n",
      "Epoch 497,Loss 7.910690\n",
      "\tParams: tensor([ 4.0376, -9.7750])\n",
      "\tGrad:tensor([-0.2263,  1.2813])\n",
      "Epoch 498,Loss 7.893775\n",
      "\tParams: tensor([ 4.0398, -9.7878])\n",
      "\tGrad:tensor([-0.2260,  1.2791])\n",
      "Epoch 499,Loss 7.876915\n",
      "\tParams: tensor([ 4.0421, -9.8005])\n",
      "\tGrad:tensor([-0.2256,  1.2770])\n",
      "Epoch 500,Loss 7.860115\n",
      "\tParams: tensor([ 4.0443, -9.8133])\n",
      "\tGrad:tensor([-0.2252,  1.2748])\n",
      "Epoch 501,Loss 7.843369\n",
      "\tParams: tensor([ 4.0466, -9.8260])\n",
      "\tGrad:tensor([-0.2248,  1.2726])\n",
      "Epoch 502,Loss 7.826683\n",
      "\tParams: tensor([ 4.0488, -9.8387])\n",
      "\tGrad:tensor([-0.2244,  1.2705])\n",
      "Epoch 503,Loss 7.810053\n",
      "\tParams: tensor([ 4.0511, -9.8514])\n",
      "\tGrad:tensor([-0.2241,  1.2683])\n",
      "Epoch 504,Loss 7.793481\n",
      "\tParams: tensor([ 4.0533, -9.8640])\n",
      "\tGrad:tensor([-0.2237,  1.2662])\n",
      "Epoch 505,Loss 7.776962\n",
      "\tParams: tensor([ 4.0555, -9.8767])\n",
      "\tGrad:tensor([-0.2233,  1.2640])\n",
      "Epoch 506,Loss 7.760498\n",
      "\tParams: tensor([ 4.0578, -9.8893])\n",
      "\tGrad:tensor([-0.2229,  1.2619])\n",
      "Epoch 507,Loss 7.744092\n",
      "\tParams: tensor([ 4.0600, -9.9019])\n",
      "\tGrad:tensor([-0.2225,  1.2597])\n",
      "Epoch 508,Loss 7.727745\n",
      "\tParams: tensor([ 4.0622, -9.9145])\n",
      "\tGrad:tensor([-0.2222,  1.2576])\n",
      "Epoch 509,Loss 7.711449\n",
      "\tParams: tensor([ 4.0644, -9.9270])\n",
      "\tGrad:tensor([-0.2218,  1.2554])\n",
      "Epoch 510,Loss 7.695211\n",
      "\tParams: tensor([ 4.0666, -9.9396])\n",
      "\tGrad:tensor([-0.2214,  1.2533])\n",
      "Epoch 511,Loss 7.679024\n",
      "\tParams: tensor([ 4.0688, -9.9521])\n",
      "\tGrad:tensor([-0.2210,  1.2512])\n",
      "Epoch 512,Loss 7.662896\n",
      "\tParams: tensor([ 4.0710, -9.9646])\n",
      "\tGrad:tensor([-0.2207,  1.2490])\n",
      "Epoch 513,Loss 7.646820\n",
      "\tParams: tensor([ 4.0733, -9.9770])\n",
      "\tGrad:tensor([-0.2203,  1.2469])\n",
      "Epoch 514,Loss 7.630803\n",
      "\tParams: tensor([ 4.0754, -9.9895])\n",
      "\tGrad:tensor([-0.2199,  1.2448])\n",
      "Epoch 515,Loss 7.614836\n",
      "\tParams: tensor([  4.0776, -10.0019])\n",
      "\tGrad:tensor([-0.2195,  1.2427])\n",
      "Epoch 516,Loss 7.598925\n",
      "\tParams: tensor([  4.0798, -10.0143])\n",
      "\tGrad:tensor([-0.2192,  1.2406])\n",
      "Epoch 517,Loss 7.583069\n",
      "\tParams: tensor([  4.0820, -10.0267])\n",
      "\tGrad:tensor([-0.2188,  1.2385])\n",
      "Epoch 518,Loss 7.567265\n",
      "\tParams: tensor([  4.0842, -10.0391])\n",
      "\tGrad:tensor([-0.2184,  1.2364])\n",
      "Epoch 519,Loss 7.551515\n",
      "\tParams: tensor([  4.0864, -10.0514])\n",
      "\tGrad:tensor([-0.2180,  1.2343])\n",
      "Epoch 520,Loss 7.535818\n",
      "\tParams: tensor([  4.0886, -10.0637])\n",
      "\tGrad:tensor([-0.2177,  1.2322])\n",
      "Epoch 521,Loss 7.520176\n",
      "\tParams: tensor([  4.0907, -10.0760])\n",
      "\tGrad:tensor([-0.2173,  1.2301])\n",
      "Epoch 522,Loss 7.504587\n",
      "\tParams: tensor([  4.0929, -10.0883])\n",
      "\tGrad:tensor([-0.2169,  1.2280])\n",
      "Epoch 523,Loss 7.489048\n",
      "\tParams: tensor([  4.0951, -10.1006])\n",
      "\tGrad:tensor([-0.2165,  1.2259])\n",
      "Epoch 524,Loss 7.473566\n",
      "\tParams: tensor([  4.0972, -10.1128])\n",
      "\tGrad:tensor([-0.2162,  1.2238])\n",
      "Epoch 525,Loss 7.458135\n",
      "\tParams: tensor([  4.0994, -10.1250])\n",
      "\tGrad:tensor([-0.2158,  1.2217])\n",
      "Epoch 526,Loss 7.442750\n",
      "\tParams: tensor([  4.1015, -10.1372])\n",
      "\tGrad:tensor([-0.2155,  1.2197])\n",
      "Epoch 527,Loss 7.427427\n",
      "\tParams: tensor([  4.1037, -10.1494])\n",
      "\tGrad:tensor([-0.2151,  1.2176])\n",
      "Epoch 528,Loss 7.412152\n",
      "\tParams: tensor([  4.1058, -10.1616])\n",
      "\tGrad:tensor([-0.2147,  1.2155])\n",
      "Epoch 529,Loss 7.396928\n",
      "\tParams: tensor([  4.1080, -10.1737])\n",
      "\tGrad:tensor([-0.2144,  1.2135])\n",
      "Epoch 530,Loss 7.381757\n",
      "\tParams: tensor([  4.1101, -10.1858])\n",
      "\tGrad:tensor([-0.2140,  1.2114])\n",
      "Epoch 531,Loss 7.366637\n",
      "\tParams: tensor([  4.1123, -10.1979])\n",
      "\tGrad:tensor([-0.2136,  1.2093])\n",
      "Epoch 532,Loss 7.351567\n",
      "\tParams: tensor([  4.1144, -10.2100])\n",
      "\tGrad:tensor([-0.2133,  1.2073])\n",
      "Epoch 533,Loss 7.336549\n",
      "\tParams: tensor([  4.1165, -10.2220])\n",
      "\tGrad:tensor([-0.2129,  1.2052])\n",
      "Epoch 534,Loss 7.321584\n",
      "\tParams: tensor([  4.1187, -10.2340])\n",
      "\tGrad:tensor([-0.2125,  1.2032])\n",
      "Epoch 535,Loss 7.306671\n",
      "\tParams: tensor([  4.1208, -10.2461])\n",
      "\tGrad:tensor([-0.2122,  1.2012])\n",
      "Epoch 536,Loss 7.291804\n",
      "\tParams: tensor([  4.1229, -10.2581])\n",
      "\tGrad:tensor([-0.2118,  1.1991])\n",
      "Epoch 537,Loss 7.276989\n",
      "\tParams: tensor([  4.1250, -10.2700])\n",
      "\tGrad:tensor([-0.2115,  1.1971])\n",
      "Epoch 538,Loss 7.262227\n",
      "\tParams: tensor([  4.1271, -10.2820])\n",
      "\tGrad:tensor([-0.2111,  1.1950])\n",
      "Epoch 539,Loss 7.247512\n",
      "\tParams: tensor([  4.1292, -10.2939])\n",
      "\tGrad:tensor([-0.2108,  1.1930])\n",
      "Epoch 540,Loss 7.232845\n",
      "\tParams: tensor([  4.1313, -10.3058])\n",
      "\tGrad:tensor([-0.2104,  1.1910])\n",
      "Epoch 541,Loss 7.218231\n",
      "\tParams: tensor([  4.1334, -10.3177])\n",
      "\tGrad:tensor([-0.2100,  1.1890])\n",
      "Epoch 542,Loss 7.203665\n",
      "\tParams: tensor([  4.1355, -10.3296])\n",
      "\tGrad:tensor([-0.2097,  1.1869])\n",
      "Epoch 543,Loss 7.189151\n",
      "\tParams: tensor([  4.1376, -10.3414])\n",
      "\tGrad:tensor([-0.2093,  1.1849])\n",
      "Epoch 544,Loss 7.174683\n",
      "\tParams: tensor([  4.1397, -10.3533])\n",
      "\tGrad:tensor([-0.2090,  1.1829])\n",
      "Epoch 545,Loss 7.160266\n",
      "\tParams: tensor([  4.1418, -10.3651])\n",
      "\tGrad:tensor([-0.2086,  1.1809])\n",
      "Epoch 546,Loss 7.145897\n",
      "\tParams: tensor([  4.1439, -10.3769])\n",
      "\tGrad:tensor([-0.2083,  1.1789])\n",
      "Epoch 547,Loss 7.131581\n",
      "\tParams: tensor([  4.1460, -10.3886])\n",
      "\tGrad:tensor([-0.2079,  1.1769])\n",
      "Epoch 548,Loss 7.117305\n",
      "\tParams: tensor([  4.1480, -10.4004])\n",
      "\tGrad:tensor([-0.2075,  1.1749])\n",
      "Epoch 549,Loss 7.103083\n",
      "\tParams: tensor([  4.1501, -10.4121])\n",
      "\tGrad:tensor([-0.2072,  1.1729])\n",
      "Epoch 550,Loss 7.088911\n",
      "\tParams: tensor([  4.1522, -10.4238])\n",
      "\tGrad:tensor([-0.2068,  1.1709])\n",
      "Epoch 551,Loss 7.074785\n",
      "\tParams: tensor([  4.1542, -10.4355])\n",
      "\tGrad:tensor([-0.2065,  1.1689])\n",
      "Epoch 552,Loss 7.060707\n",
      "\tParams: tensor([  4.1563, -10.4472])\n",
      "\tGrad:tensor([-0.2062,  1.1669])\n",
      "Epoch 553,Loss 7.046676\n",
      "\tParams: tensor([  4.1584, -10.4588])\n",
      "\tGrad:tensor([-0.2058,  1.1649])\n",
      "Epoch 554,Loss 7.032695\n",
      "\tParams: tensor([  4.1604, -10.4704])\n",
      "\tGrad:tensor([-0.2054,  1.1630])\n",
      "Epoch 555,Loss 7.018755\n",
      "\tParams: tensor([  4.1625, -10.4821])\n",
      "\tGrad:tensor([-0.2051,  1.1610])\n",
      "Epoch 556,Loss 7.004870\n",
      "\tParams: tensor([  4.1645, -10.4936])\n",
      "\tGrad:tensor([-0.2047,  1.1590])\n",
      "Epoch 557,Loss 6.991028\n",
      "\tParams: tensor([  4.1666, -10.5052])\n",
      "\tGrad:tensor([-0.2044,  1.1571])\n",
      "Epoch 558,Loss 6.977232\n",
      "\tParams: tensor([  4.1686, -10.5168])\n",
      "\tGrad:tensor([-0.2041,  1.1551])\n",
      "Epoch 559,Loss 6.963488\n",
      "\tParams: tensor([  4.1706, -10.5283])\n",
      "\tGrad:tensor([-0.2037,  1.1531])\n",
      "Epoch 560,Loss 6.949787\n",
      "\tParams: tensor([  4.1727, -10.5398])\n",
      "\tGrad:tensor([-0.2034,  1.1512])\n",
      "Epoch 561,Loss 6.936135\n",
      "\tParams: tensor([  4.1747, -10.5513])\n",
      "\tGrad:tensor([-0.2030,  1.1492])\n",
      "Epoch 562,Loss 6.922528\n",
      "\tParams: tensor([  4.1767, -10.5628])\n",
      "\tGrad:tensor([-0.2027,  1.1473])\n",
      "Epoch 563,Loss 6.908967\n",
      "\tParams: tensor([  4.1787, -10.5742])\n",
      "\tGrad:tensor([-0.2023,  1.1453])\n",
      "Epoch 564,Loss 6.895452\n",
      "\tParams: tensor([  4.1808, -10.5857])\n",
      "\tGrad:tensor([-0.2020,  1.1434])\n",
      "Epoch 565,Loss 6.881980\n",
      "\tParams: tensor([  4.1828, -10.5971])\n",
      "\tGrad:tensor([-0.2016,  1.1414])\n",
      "Epoch 566,Loss 6.868559\n",
      "\tParams: tensor([  4.1848, -10.6085])\n",
      "\tGrad:tensor([-0.2013,  1.1395])\n",
      "Epoch 567,Loss 6.855180\n",
      "\tParams: tensor([  4.1868, -10.6198])\n",
      "\tGrad:tensor([-0.2010,  1.1375])\n",
      "Epoch 568,Loss 6.841848\n",
      "\tParams: tensor([  4.1888, -10.6312])\n",
      "\tGrad:tensor([-0.2006,  1.1356])\n",
      "Epoch 569,Loss 6.828561\n",
      "\tParams: tensor([  4.1908, -10.6425])\n",
      "\tGrad:tensor([-0.2003,  1.1337])\n",
      "Epoch 570,Loss 6.815319\n",
      "\tParams: tensor([  4.1928, -10.6539])\n",
      "\tGrad:tensor([-0.1999,  1.1318])\n",
      "Epoch 571,Loss 6.802118\n",
      "\tParams: tensor([  4.1948, -10.6652])\n",
      "\tGrad:tensor([-0.1996,  1.1298])\n",
      "Epoch 572,Loss 6.788968\n",
      "\tParams: tensor([  4.1968, -10.6764])\n",
      "\tGrad:tensor([-0.1993,  1.1279])\n",
      "Epoch 573,Loss 6.775864\n",
      "\tParams: tensor([  4.1988, -10.6877])\n",
      "\tGrad:tensor([-0.1989,  1.1260])\n",
      "Epoch 574,Loss 6.762797\n",
      "\tParams: tensor([  4.2008, -10.6989])\n",
      "\tGrad:tensor([-0.1986,  1.1241])\n",
      "Epoch 575,Loss 6.749779\n",
      "\tParams: tensor([  4.2028, -10.7102])\n",
      "\tGrad:tensor([-0.1982,  1.1222])\n",
      "Epoch 576,Loss 6.736804\n",
      "\tParams: tensor([  4.2047, -10.7214])\n",
      "\tGrad:tensor([-0.1979,  1.1203])\n",
      "Epoch 577,Loss 6.723876\n",
      "\tParams: tensor([  4.2067, -10.7325])\n",
      "\tGrad:tensor([-0.1976,  1.1184])\n",
      "Epoch 578,Loss 6.710987\n",
      "\tParams: tensor([  4.2087, -10.7437])\n",
      "\tGrad:tensor([-0.1972,  1.1165])\n",
      "Epoch 579,Loss 6.698142\n",
      "\tParams: tensor([  4.2107, -10.7549])\n",
      "\tGrad:tensor([-0.1969,  1.1146])\n",
      "Epoch 580,Loss 6.685345\n",
      "\tParams: tensor([  4.2126, -10.7660])\n",
      "\tGrad:tensor([-0.1966,  1.1127])\n",
      "Epoch 581,Loss 6.672589\n",
      "\tParams: tensor([  4.2146, -10.7771])\n",
      "\tGrad:tensor([-0.1962,  1.1108])\n",
      "Epoch 582,Loss 6.659873\n",
      "\tParams: tensor([  4.2165, -10.7882])\n",
      "\tGrad:tensor([-0.1959,  1.1089])\n",
      "Epoch 583,Loss 6.647207\n",
      "\tParams: tensor([  4.2185, -10.7992])\n",
      "\tGrad:tensor([-0.1956,  1.1070])\n",
      "Epoch 584,Loss 6.634578\n",
      "\tParams: tensor([  4.2204, -10.8103])\n",
      "\tGrad:tensor([-0.1952,  1.1051])\n",
      "Epoch 585,Loss 6.621994\n",
      "\tParams: tensor([  4.2224, -10.8213])\n",
      "\tGrad:tensor([-0.1949,  1.1033])\n",
      "Epoch 586,Loss 6.609454\n",
      "\tParams: tensor([  4.2243, -10.8323])\n",
      "\tGrad:tensor([-0.1946,  1.1014])\n",
      "Epoch 587,Loss 6.596953\n",
      "\tParams: tensor([  4.2263, -10.8433])\n",
      "\tGrad:tensor([-0.1942,  1.0995])\n",
      "Epoch 588,Loss 6.584499\n",
      "\tParams: tensor([  4.2282, -10.8543])\n",
      "\tGrad:tensor([-0.1939,  1.0976])\n",
      "Epoch 589,Loss 6.572087\n",
      "\tParams: tensor([  4.2302, -10.8653])\n",
      "\tGrad:tensor([-0.1936,  1.0958])\n",
      "Epoch 590,Loss 6.559712\n",
      "\tParams: tensor([  4.2321, -10.8762])\n",
      "\tGrad:tensor([-0.1932,  1.0939])\n",
      "Epoch 591,Loss 6.547384\n",
      "\tParams: tensor([  4.2340, -10.8871])\n",
      "\tGrad:tensor([-0.1929,  1.0921])\n",
      "Epoch 592,Loss 6.535097\n",
      "\tParams: tensor([  4.2359, -10.8980])\n",
      "\tGrad:tensor([-0.1926,  1.0902])\n",
      "Epoch 593,Loss 6.522851\n",
      "\tParams: tensor([  4.2379, -10.9089])\n",
      "\tGrad:tensor([-0.1923,  1.0884])\n",
      "Epoch 594,Loss 6.510646\n",
      "\tParams: tensor([  4.2398, -10.9198])\n",
      "\tGrad:tensor([-0.1919,  1.0865])\n",
      "Epoch 595,Loss 6.498482\n",
      "\tParams: tensor([  4.2417, -10.9306])\n",
      "\tGrad:tensor([-0.1916,  1.0847])\n",
      "Epoch 596,Loss 6.486361\n",
      "\tParams: tensor([  4.2436, -10.9415])\n",
      "\tGrad:tensor([-0.1913,  1.0828])\n",
      "Epoch 597,Loss 6.474282\n",
      "\tParams: tensor([  4.2455, -10.9523])\n",
      "\tGrad:tensor([-0.1910,  1.0810])\n",
      "Epoch 598,Loss 6.462241\n",
      "\tParams: tensor([  4.2474, -10.9631])\n",
      "\tGrad:tensor([-0.1906,  1.0791])\n",
      "Epoch 599,Loss 6.450243\n",
      "\tParams: tensor([  4.2493, -10.9738])\n",
      "\tGrad:tensor([-0.1903,  1.0773])\n",
      "Epoch 600,Loss 6.438284\n",
      "\tParams: tensor([  4.2512, -10.9846])\n",
      "\tGrad:tensor([-0.1900,  1.0755])\n",
      "Epoch 601,Loss 6.426368\n",
      "\tParams: tensor([  4.2531, -10.9953])\n",
      "\tGrad:tensor([-0.1897,  1.0737])\n",
      "Epoch 602,Loss 6.414490\n",
      "\tParams: tensor([  4.2550, -11.0060])\n",
      "\tGrad:tensor([-0.1893,  1.0718])\n",
      "Epoch 603,Loss 6.402653\n",
      "\tParams: tensor([  4.2569, -11.0167])\n",
      "\tGrad:tensor([-0.1890,  1.0700])\n",
      "Epoch 604,Loss 6.390859\n",
      "\tParams: tensor([  4.2588, -11.0274])\n",
      "\tGrad:tensor([-0.1887,  1.0682])\n",
      "Epoch 605,Loss 6.379103\n",
      "\tParams: tensor([  4.2607, -11.0381])\n",
      "\tGrad:tensor([-0.1884,  1.0664])\n",
      "Epoch 606,Loss 6.367385\n",
      "\tParams: tensor([  4.2626, -11.0487])\n",
      "\tGrad:tensor([-0.1880,  1.0646])\n",
      "Epoch 607,Loss 6.355706\n",
      "\tParams: tensor([  4.2644, -11.0594])\n",
      "\tGrad:tensor([-0.1877,  1.0628])\n",
      "Epoch 608,Loss 6.344070\n",
      "\tParams: tensor([  4.2663, -11.0700])\n",
      "\tGrad:tensor([-0.1874,  1.0609])\n",
      "Epoch 609,Loss 6.332472\n",
      "\tParams: tensor([  4.2682, -11.0806])\n",
      "\tGrad:tensor([-0.1871,  1.0591])\n",
      "Epoch 610,Loss 6.320912\n",
      "\tParams: tensor([  4.2701, -11.0911])\n",
      "\tGrad:tensor([-0.1868,  1.0573])\n",
      "Epoch 611,Loss 6.309395\n",
      "\tParams: tensor([  4.2719, -11.1017])\n",
      "\tGrad:tensor([-0.1865,  1.0555])\n",
      "Epoch 612,Loss 6.297915\n",
      "\tParams: tensor([  4.2738, -11.1122])\n",
      "\tGrad:tensor([-0.1861,  1.0538])\n",
      "Epoch 613,Loss 6.286473\n",
      "\tParams: tensor([  4.2756, -11.1227])\n",
      "\tGrad:tensor([-0.1858,  1.0520])\n",
      "Epoch 614,Loss 6.275074\n",
      "\tParams: tensor([  4.2775, -11.1333])\n",
      "\tGrad:tensor([-0.1855,  1.0502])\n",
      "Epoch 615,Loss 6.263708\n",
      "\tParams: tensor([  4.2794, -11.1437])\n",
      "\tGrad:tensor([-0.1852,  1.0484])\n",
      "Epoch 616,Loss 6.252382\n",
      "\tParams: tensor([  4.2812, -11.1542])\n",
      "\tGrad:tensor([-0.1849,  1.0466])\n",
      "Epoch 617,Loss 6.241098\n",
      "\tParams: tensor([  4.2830, -11.1646])\n",
      "\tGrad:tensor([-0.1846,  1.0448])\n",
      "Epoch 618,Loss 6.229849\n",
      "\tParams: tensor([  4.2849, -11.1751])\n",
      "\tGrad:tensor([-0.1843,  1.0431])\n",
      "Epoch 619,Loss 6.218639\n",
      "\tParams: tensor([  4.2867, -11.1855])\n",
      "\tGrad:tensor([-0.1840,  1.0413])\n",
      "Epoch 620,Loss 6.207470\n",
      "\tParams: tensor([  4.2886, -11.1959])\n",
      "\tGrad:tensor([-0.1836,  1.0395])\n",
      "Epoch 621,Loss 6.196334\n",
      "\tParams: tensor([  4.2904, -11.2063])\n",
      "\tGrad:tensor([-0.1833,  1.0378])\n",
      "Epoch 622,Loss 6.185240\n",
      "\tParams: tensor([  4.2922, -11.2166])\n",
      "\tGrad:tensor([-0.1830,  1.0360])\n",
      "Epoch 623,Loss 6.174181\n",
      "\tParams: tensor([  4.2941, -11.2270])\n",
      "\tGrad:tensor([-0.1827,  1.0342])\n",
      "Epoch 624,Loss 6.163159\n",
      "\tParams: tensor([  4.2959, -11.2373])\n",
      "\tGrad:tensor([-0.1824,  1.0325])\n",
      "Epoch 625,Loss 6.152177\n",
      "\tParams: tensor([  4.2977, -11.2476])\n",
      "\tGrad:tensor([-0.1821,  1.0307])\n",
      "Epoch 626,Loss 6.141230\n",
      "\tParams: tensor([  4.2995, -11.2579])\n",
      "\tGrad:tensor([-0.1818,  1.0290])\n",
      "Epoch 627,Loss 6.130322\n",
      "\tParams: tensor([  4.3013, -11.2682])\n",
      "\tGrad:tensor([-0.1815,  1.0272])\n",
      "Epoch 628,Loss 6.119448\n",
      "\tParams: tensor([  4.3031, -11.2784])\n",
      "\tGrad:tensor([-0.1811,  1.0255])\n",
      "Epoch 629,Loss 6.108614\n",
      "\tParams: tensor([  4.3050, -11.2887])\n",
      "\tGrad:tensor([-0.1808,  1.0237])\n",
      "Epoch 630,Loss 6.097815\n",
      "\tParams: tensor([  4.3068, -11.2989])\n",
      "\tGrad:tensor([-0.1805,  1.0220])\n",
      "Epoch 631,Loss 6.087054\n",
      "\tParams: tensor([  4.3086, -11.3091])\n",
      "\tGrad:tensor([-0.1802,  1.0203])\n",
      "Epoch 632,Loss 6.076329\n",
      "\tParams: tensor([  4.3104, -11.3193])\n",
      "\tGrad:tensor([-0.1799,  1.0185])\n",
      "Epoch 633,Loss 6.065644\n",
      "\tParams: tensor([  4.3122, -11.3294])\n",
      "\tGrad:tensor([-0.1796,  1.0168])\n",
      "Epoch 634,Loss 6.054988\n",
      "\tParams: tensor([  4.3139, -11.3396])\n",
      "\tGrad:tensor([-0.1793,  1.0151])\n",
      "Epoch 635,Loss 6.044372\n",
      "\tParams: tensor([  4.3157, -11.3497])\n",
      "\tGrad:tensor([-0.1790,  1.0133])\n",
      "Epoch 636,Loss 6.033794\n",
      "\tParams: tensor([  4.3175, -11.3598])\n",
      "\tGrad:tensor([-0.1787,  1.0116])\n",
      "Epoch 637,Loss 6.023247\n",
      "\tParams: tensor([  4.3193, -11.3699])\n",
      "\tGrad:tensor([-0.1784,  1.0099])\n",
      "Epoch 638,Loss 6.012738\n",
      "\tParams: tensor([  4.3211, -11.3800])\n",
      "\tGrad:tensor([-0.1781,  1.0082])\n",
      "Epoch 639,Loss 6.002264\n",
      "\tParams: tensor([  4.3229, -11.3901])\n",
      "\tGrad:tensor([-0.1778,  1.0065])\n",
      "Epoch 640,Loss 5.991828\n",
      "\tParams: tensor([  4.3246, -11.4001])\n",
      "\tGrad:tensor([-0.1775,  1.0048])\n",
      "Epoch 641,Loss 5.981425\n",
      "\tParams: tensor([  4.3264, -11.4102])\n",
      "\tGrad:tensor([-0.1772,  1.0031])\n",
      "Epoch 642,Loss 5.971058\n",
      "\tParams: tensor([  4.3282, -11.4202])\n",
      "\tGrad:tensor([-0.1769,  1.0014])\n",
      "Epoch 643,Loss 5.960727\n",
      "\tParams: tensor([  4.3300, -11.4302])\n",
      "\tGrad:tensor([-0.1766,  0.9997])\n",
      "Epoch 644,Loss 5.950432\n",
      "\tParams: tensor([  4.3317, -11.4401])\n",
      "\tGrad:tensor([-0.1763,  0.9980])\n",
      "Epoch 645,Loss 5.940171\n",
      "\tParams: tensor([  4.3335, -11.4501])\n",
      "\tGrad:tensor([-0.1760,  0.9963])\n",
      "Epoch 646,Loss 5.929944\n",
      "\tParams: tensor([  4.3352, -11.4601])\n",
      "\tGrad:tensor([-0.1757,  0.9946])\n",
      "Epoch 647,Loss 5.919752\n",
      "\tParams: tensor([  4.3370, -11.4700])\n",
      "\tGrad:tensor([-0.1754,  0.9929])\n",
      "Epoch 648,Loss 5.909596\n",
      "\tParams: tensor([  4.3387, -11.4799])\n",
      "\tGrad:tensor([-0.1751,  0.9912])\n",
      "Epoch 649,Loss 5.899472\n",
      "\tParams: tensor([  4.3405, -11.4898])\n",
      "\tGrad:tensor([-0.1748,  0.9895])\n",
      "Epoch 650,Loss 5.889383\n",
      "\tParams: tensor([  4.3422, -11.4997])\n",
      "\tGrad:tensor([-0.1745,  0.9878])\n",
      "Epoch 651,Loss 5.879326\n",
      "\tParams: tensor([  4.3440, -11.5095])\n",
      "\tGrad:tensor([-0.1742,  0.9862])\n",
      "Epoch 652,Loss 5.869310\n",
      "\tParams: tensor([  4.3457, -11.5194])\n",
      "\tGrad:tensor([-0.1739,  0.9845])\n",
      "Epoch 653,Loss 5.859322\n",
      "\tParams: tensor([  4.3474, -11.5292])\n",
      "\tGrad:tensor([-0.1736,  0.9828])\n",
      "Epoch 654,Loss 5.849374\n",
      "\tParams: tensor([  4.3492, -11.5390])\n",
      "\tGrad:tensor([-0.1733,  0.9811])\n",
      "Epoch 655,Loss 5.839453\n",
      "\tParams: tensor([  4.3509, -11.5488])\n",
      "\tGrad:tensor([-0.1730,  0.9795])\n",
      "Epoch 656,Loss 5.829570\n",
      "\tParams: tensor([  4.3526, -11.5586])\n",
      "\tGrad:tensor([-0.1727,  0.9778])\n",
      "Epoch 657,Loss 5.819718\n",
      "\tParams: tensor([  4.3544, -11.5683])\n",
      "\tGrad:tensor([-0.1724,  0.9761])\n",
      "Epoch 658,Loss 5.809901\n",
      "\tParams: tensor([  4.3561, -11.5781])\n",
      "\tGrad:tensor([-0.1722,  0.9745])\n",
      "Epoch 659,Loss 5.800116\n",
      "\tParams: tensor([  4.3578, -11.5878])\n",
      "\tGrad:tensor([-0.1719,  0.9728])\n",
      "Epoch 660,Loss 5.790367\n",
      "\tParams: tensor([  4.3595, -11.5975])\n",
      "\tGrad:tensor([-0.1716,  0.9712])\n",
      "Epoch 661,Loss 5.780646\n",
      "\tParams: tensor([  4.3612, -11.6072])\n",
      "\tGrad:tensor([-0.1713,  0.9695])\n",
      "Epoch 662,Loss 5.770962\n",
      "\tParams: tensor([  4.3629, -11.6169])\n",
      "\tGrad:tensor([-0.1710,  0.9679])\n",
      "Epoch 663,Loss 5.761312\n",
      "\tParams: tensor([  4.3646, -11.6266])\n",
      "\tGrad:tensor([-0.1707,  0.9662])\n",
      "Epoch 664,Loss 5.751694\n",
      "\tParams: tensor([  4.3664, -11.6362])\n",
      "\tGrad:tensor([-0.1704,  0.9646])\n",
      "Epoch 665,Loss 5.742105\n",
      "\tParams: tensor([  4.3681, -11.6458])\n",
      "\tGrad:tensor([-0.1701,  0.9630])\n",
      "Epoch 666,Loss 5.732550\n",
      "\tParams: tensor([  4.3697, -11.6555])\n",
      "\tGrad:tensor([-0.1698,  0.9613])\n",
      "Epoch 667,Loss 5.723031\n",
      "\tParams: tensor([  4.3714, -11.6651])\n",
      "\tGrad:tensor([-0.1695,  0.9597])\n",
      "Epoch 668,Loss 5.713540\n",
      "\tParams: tensor([  4.3731, -11.6746])\n",
      "\tGrad:tensor([-0.1692,  0.9581])\n",
      "Epoch 669,Loss 5.704083\n",
      "\tParams: tensor([  4.3748, -11.6842])\n",
      "\tGrad:tensor([-0.1690,  0.9564])\n",
      "Epoch 670,Loss 5.694659\n",
      "\tParams: tensor([  4.3765, -11.6937])\n",
      "\tGrad:tensor([-0.1687,  0.9548])\n",
      "Epoch 671,Loss 5.685266\n",
      "\tParams: tensor([  4.3782, -11.7033])\n",
      "\tGrad:tensor([-0.1684,  0.9532])\n",
      "Epoch 672,Loss 5.675904\n",
      "\tParams: tensor([  4.3799, -11.7128])\n",
      "\tGrad:tensor([-0.1681,  0.9516])\n",
      "Epoch 673,Loss 5.666573\n",
      "\tParams: tensor([  4.3816, -11.7223])\n",
      "\tGrad:tensor([-0.1678,  0.9499])\n",
      "Epoch 674,Loss 5.657277\n",
      "\tParams: tensor([  4.3832, -11.7318])\n",
      "\tGrad:tensor([-0.1675,  0.9483])\n",
      "Epoch 675,Loss 5.648010\n",
      "\tParams: tensor([  4.3849, -11.7412])\n",
      "\tGrad:tensor([-0.1673,  0.9467])\n",
      "Epoch 676,Loss 5.638776\n",
      "\tParams: tensor([  4.3866, -11.7507])\n",
      "\tGrad:tensor([-0.1670,  0.9451])\n",
      "Epoch 677,Loss 5.629574\n",
      "\tParams: tensor([  4.3882, -11.7601])\n",
      "\tGrad:tensor([-0.1667,  0.9435])\n",
      "Epoch 678,Loss 5.620402\n",
      "\tParams: tensor([  4.3899, -11.7696])\n",
      "\tGrad:tensor([-0.1664,  0.9419])\n",
      "Epoch 679,Loss 5.611260\n",
      "\tParams: tensor([  4.3916, -11.7790])\n",
      "\tGrad:tensor([-0.1661,  0.9403])\n",
      "Epoch 680,Loss 5.602149\n",
      "\tParams: tensor([  4.3932, -11.7883])\n",
      "\tGrad:tensor([-0.1658,  0.9387])\n",
      "Epoch 681,Loss 5.593071\n",
      "\tParams: tensor([  4.3949, -11.7977])\n",
      "\tGrad:tensor([-0.1656,  0.9371])\n",
      "Epoch 682,Loss 5.584022\n",
      "\tParams: tensor([  4.3965, -11.8071])\n",
      "\tGrad:tensor([-0.1653,  0.9355])\n",
      "Epoch 683,Loss 5.575005\n",
      "\tParams: tensor([  4.3982, -11.8164])\n",
      "\tGrad:tensor([-0.1650,  0.9339])\n",
      "Epoch 684,Loss 5.566019\n",
      "\tParams: tensor([  4.3998, -11.8257])\n",
      "\tGrad:tensor([-0.1647,  0.9323])\n",
      "Epoch 685,Loss 5.557063\n",
      "\tParams: tensor([  4.4015, -11.8350])\n",
      "\tGrad:tensor([-0.1644,  0.9308])\n",
      "Epoch 686,Loss 5.548136\n",
      "\tParams: tensor([  4.4031, -11.8443])\n",
      "\tGrad:tensor([-0.1641,  0.9292])\n",
      "Epoch 687,Loss 5.539241\n",
      "\tParams: tensor([  4.4048, -11.8536])\n",
      "\tGrad:tensor([-0.1639,  0.9276])\n",
      "Epoch 688,Loss 5.530376\n",
      "\tParams: tensor([  4.4064, -11.8629])\n",
      "\tGrad:tensor([-0.1636,  0.9260])\n",
      "Epoch 689,Loss 5.521540\n",
      "\tParams: tensor([  4.4080, -11.8721])\n",
      "\tGrad:tensor([-0.1633,  0.9245])\n",
      "Epoch 690,Loss 5.512734\n",
      "\tParams: tensor([  4.4097, -11.8813])\n",
      "\tGrad:tensor([-0.1630,  0.9229])\n",
      "Epoch 691,Loss 5.503958\n",
      "\tParams: tensor([  4.4113, -11.8906])\n",
      "\tGrad:tensor([-0.1628,  0.9213])\n",
      "Epoch 692,Loss 5.495212\n",
      "\tParams: tensor([  4.4129, -11.8998])\n",
      "\tGrad:tensor([-0.1625,  0.9197])\n",
      "Epoch 693,Loss 5.486496\n",
      "\tParams: tensor([  4.4145, -11.9089])\n",
      "\tGrad:tensor([-0.1622,  0.9182])\n",
      "Epoch 694,Loss 5.477808\n",
      "\tParams: tensor([  4.4161, -11.9181])\n",
      "\tGrad:tensor([-0.1619,  0.9166])\n",
      "Epoch 695,Loss 5.469152\n",
      "\tParams: tensor([  4.4178, -11.9272])\n",
      "\tGrad:tensor([-0.1617,  0.9151])\n",
      "Epoch 696,Loss 5.460525\n",
      "\tParams: tensor([  4.4194, -11.9364])\n",
      "\tGrad:tensor([-0.1614,  0.9135])\n",
      "Epoch 697,Loss 5.451928\n",
      "\tParams: tensor([  4.4210, -11.9455])\n",
      "\tGrad:tensor([-0.1611,  0.9120])\n",
      "Epoch 698,Loss 5.443358\n",
      "\tParams: tensor([  4.4226, -11.9546])\n",
      "\tGrad:tensor([-0.1608,  0.9104])\n",
      "Epoch 699,Loss 5.434819\n",
      "\tParams: tensor([  4.4242, -11.9637])\n",
      "\tGrad:tensor([-0.1605,  0.9089])\n",
      "Epoch 700,Loss 5.426309\n",
      "\tParams: tensor([  4.4258, -11.9728])\n",
      "\tGrad:tensor([-0.1603,  0.9073])\n",
      "Epoch 701,Loss 5.417827\n",
      "\tParams: tensor([  4.4274, -11.9818])\n",
      "\tGrad:tensor([-0.1600,  0.9058])\n",
      "Epoch 702,Loss 5.409372\n",
      "\tParams: tensor([  4.4290, -11.9909])\n",
      "\tGrad:tensor([-0.1597,  0.9042])\n",
      "Epoch 703,Loss 5.400949\n",
      "\tParams: tensor([  4.4306, -11.9999])\n",
      "\tGrad:tensor([-0.1595,  0.9027])\n",
      "Epoch 704,Loss 5.392550\n",
      "\tParams: tensor([  4.4322, -12.0089])\n",
      "\tGrad:tensor([-0.1592,  0.9012])\n",
      "Epoch 705,Loss 5.384184\n",
      "\tParams: tensor([  4.4338, -12.0179])\n",
      "\tGrad:tensor([-0.1589,  0.8996])\n",
      "Epoch 706,Loss 5.375846\n",
      "\tParams: tensor([  4.4354, -12.0269])\n",
      "\tGrad:tensor([-0.1586,  0.8981])\n",
      "Epoch 707,Loss 5.367537\n",
      "\tParams: tensor([  4.4369, -12.0359])\n",
      "\tGrad:tensor([-0.1584,  0.8966])\n",
      "Epoch 708,Loss 5.359253\n",
      "\tParams: tensor([  4.4385, -12.0448])\n",
      "\tGrad:tensor([-0.1581,  0.8951])\n",
      "Epoch 709,Loss 5.350999\n",
      "\tParams: tensor([  4.4401, -12.0537])\n",
      "\tGrad:tensor([-0.1578,  0.8935])\n",
      "Epoch 710,Loss 5.342772\n",
      "\tParams: tensor([  4.4417, -12.0627])\n",
      "\tGrad:tensor([-0.1576,  0.8920])\n",
      "Epoch 711,Loss 5.334575\n",
      "\tParams: tensor([  4.4433, -12.0716])\n",
      "\tGrad:tensor([-0.1573,  0.8905])\n",
      "Epoch 712,Loss 5.326402\n",
      "\tParams: tensor([  4.4448, -12.0805])\n",
      "\tGrad:tensor([-0.1570,  0.8890])\n",
      "Epoch 713,Loss 5.318260\n",
      "\tParams: tensor([  4.4464, -12.0893])\n",
      "\tGrad:tensor([-0.1568,  0.8875])\n",
      "Epoch 714,Loss 5.310144\n",
      "\tParams: tensor([  4.4480, -12.0982])\n",
      "\tGrad:tensor([-0.1565,  0.8860])\n",
      "Epoch 715,Loss 5.302055\n",
      "\tParams: tensor([  4.4495, -12.1070])\n",
      "\tGrad:tensor([-0.1562,  0.8845])\n",
      "Epoch 716,Loss 5.293994\n",
      "\tParams: tensor([  4.4511, -12.1159])\n",
      "\tGrad:tensor([-0.1560,  0.8830])\n",
      "Epoch 717,Loss 5.285964\n",
      "\tParams: tensor([  4.4526, -12.1247])\n",
      "\tGrad:tensor([-0.1557,  0.8815])\n",
      "Epoch 718,Loss 5.277958\n",
      "\tParams: tensor([  4.4542, -12.1335])\n",
      "\tGrad:tensor([-0.1555,  0.8800])\n",
      "Epoch 719,Loss 5.269979\n",
      "\tParams: tensor([  4.4557, -12.1423])\n",
      "\tGrad:tensor([-0.1552,  0.8785])\n",
      "Epoch 720,Loss 5.262027\n",
      "\tParams: tensor([  4.4573, -12.1510])\n",
      "\tGrad:tensor([-0.1549,  0.8770])\n",
      "Epoch 721,Loss 5.254103\n",
      "\tParams: tensor([  4.4588, -12.1598])\n",
      "\tGrad:tensor([-0.1547,  0.8755])\n",
      "Epoch 722,Loss 5.246205\n",
      "\tParams: tensor([  4.4604, -12.1685])\n",
      "\tGrad:tensor([-0.1544,  0.8740])\n",
      "Epoch 723,Loss 5.238335\n",
      "\tParams: tensor([  4.4619, -12.1773])\n",
      "\tGrad:tensor([-0.1541,  0.8725])\n",
      "Epoch 724,Loss 5.230492\n",
      "\tParams: tensor([  4.4635, -12.1860])\n",
      "\tGrad:tensor([-0.1539,  0.8710])\n",
      "Epoch 725,Loss 5.222674\n",
      "\tParams: tensor([  4.4650, -12.1947])\n",
      "\tGrad:tensor([-0.1536,  0.8696])\n",
      "Epoch 726,Loss 5.214881\n",
      "\tParams: tensor([  4.4665, -12.2033])\n",
      "\tGrad:tensor([-0.1533,  0.8681])\n",
      "Epoch 727,Loss 5.207120\n",
      "\tParams: tensor([  4.4681, -12.2120])\n",
      "\tGrad:tensor([-0.1531,  0.8666])\n",
      "Epoch 728,Loss 5.199381\n",
      "\tParams: tensor([  4.4696, -12.2207])\n",
      "\tGrad:tensor([-0.1528,  0.8651])\n",
      "Epoch 729,Loss 5.191670\n",
      "\tParams: tensor([  4.4711, -12.2293])\n",
      "\tGrad:tensor([-0.1526,  0.8637])\n",
      "Epoch 730,Loss 5.183985\n",
      "\tParams: tensor([  4.4726, -12.2379])\n",
      "\tGrad:tensor([-0.1523,  0.8622])\n",
      "Epoch 731,Loss 5.176324\n",
      "\tParams: tensor([  4.4742, -12.2465])\n",
      "\tGrad:tensor([-0.1520,  0.8607])\n",
      "Epoch 732,Loss 5.168688\n",
      "\tParams: tensor([  4.4757, -12.2551])\n",
      "\tGrad:tensor([-0.1518,  0.8593])\n",
      "Epoch 733,Loss 5.161084\n",
      "\tParams: tensor([  4.4772, -12.2637])\n",
      "\tGrad:tensor([-0.1515,  0.8578])\n",
      "Epoch 734,Loss 5.153500\n",
      "\tParams: tensor([  4.4787, -12.2723])\n",
      "\tGrad:tensor([-0.1513,  0.8564])\n",
      "Epoch 735,Loss 5.145944\n",
      "\tParams: tensor([  4.4802, -12.2808])\n",
      "\tGrad:tensor([-0.1510,  0.8549])\n",
      "Epoch 736,Loss 5.138413\n",
      "\tParams: tensor([  4.4817, -12.2893])\n",
      "\tGrad:tensor([-0.1508,  0.8535])\n",
      "Epoch 737,Loss 5.130910\n",
      "\tParams: tensor([  4.4832, -12.2979])\n",
      "\tGrad:tensor([-0.1505,  0.8520])\n",
      "Epoch 738,Loss 5.123428\n",
      "\tParams: tensor([  4.4847, -12.3064])\n",
      "\tGrad:tensor([-0.1502,  0.8506])\n",
      "Epoch 739,Loss 5.115978\n",
      "\tParams: tensor([  4.4862, -12.3149])\n",
      "\tGrad:tensor([-0.1500,  0.8491])\n",
      "Epoch 740,Loss 5.108547\n",
      "\tParams: tensor([  4.4877, -12.3233])\n",
      "\tGrad:tensor([-0.1497,  0.8477])\n",
      "Epoch 741,Loss 5.101143\n",
      "\tParams: tensor([  4.4892, -12.3318])\n",
      "\tGrad:tensor([-0.1495,  0.8462])\n",
      "Epoch 742,Loss 5.093765\n",
      "\tParams: tensor([  4.4907, -12.3402])\n",
      "\tGrad:tensor([-0.1492,  0.8448])\n",
      "Epoch 743,Loss 5.086414\n",
      "\tParams: tensor([  4.4922, -12.3487])\n",
      "\tGrad:tensor([-0.1490,  0.8434])\n",
      "Epoch 744,Loss 5.079086\n",
      "\tParams: tensor([  4.4937, -12.3571])\n",
      "\tGrad:tensor([-0.1487,  0.8419])\n",
      "Epoch 745,Loss 5.071781\n",
      "\tParams: tensor([  4.4952, -12.3655])\n",
      "\tGrad:tensor([-0.1485,  0.8405])\n",
      "Epoch 746,Loss 5.064505\n",
      "\tParams: tensor([  4.4967, -12.3739])\n",
      "\tGrad:tensor([-0.1482,  0.8391])\n",
      "Epoch 747,Loss 5.057247\n",
      "\tParams: tensor([  4.4981, -12.3823])\n",
      "\tGrad:tensor([-0.1480,  0.8376])\n",
      "Epoch 748,Loss 5.050021\n",
      "\tParams: tensor([  4.4996, -12.3906])\n",
      "\tGrad:tensor([-0.1477,  0.8362])\n",
      "Epoch 749,Loss 5.042817\n",
      "\tParams: tensor([  4.5011, -12.3990])\n",
      "\tGrad:tensor([-0.1475,  0.8348])\n",
      "Epoch 750,Loss 5.035636\n",
      "\tParams: tensor([  4.5026, -12.4073])\n",
      "\tGrad:tensor([-0.1472,  0.8334])\n",
      "Epoch 751,Loss 5.028476\n",
      "\tParams: tensor([  4.5040, -12.4156])\n",
      "\tGrad:tensor([-0.1470,  0.8320])\n",
      "Epoch 752,Loss 5.021346\n",
      "\tParams: tensor([  4.5055, -12.4239])\n",
      "\tGrad:tensor([-0.1467,  0.8305])\n",
      "Epoch 753,Loss 5.014240\n",
      "\tParams: tensor([  4.5070, -12.4322])\n",
      "\tGrad:tensor([-0.1465,  0.8291])\n",
      "Epoch 754,Loss 5.007157\n",
      "\tParams: tensor([  4.5084, -12.4405])\n",
      "\tGrad:tensor([-0.1462,  0.8277])\n",
      "Epoch 755,Loss 5.000099\n",
      "\tParams: tensor([  4.5099, -12.4488])\n",
      "\tGrad:tensor([-0.1460,  0.8263])\n",
      "Epoch 756,Loss 4.993064\n",
      "\tParams: tensor([  4.5113, -12.4570])\n",
      "\tGrad:tensor([-0.1457,  0.8249])\n",
      "Epoch 757,Loss 4.986051\n",
      "\tParams: tensor([  4.5128, -12.4653])\n",
      "\tGrad:tensor([-0.1455,  0.8235])\n",
      "Epoch 758,Loss 4.979064\n",
      "\tParams: tensor([  4.5143, -12.4735])\n",
      "\tGrad:tensor([-0.1452,  0.8221])\n",
      "Epoch 759,Loss 4.972100\n",
      "\tParams: tensor([  4.5157, -12.4817])\n",
      "\tGrad:tensor([-0.1450,  0.8207])\n",
      "Epoch 760,Loss 4.965159\n",
      "\tParams: tensor([  4.5172, -12.4899])\n",
      "\tGrad:tensor([-0.1447,  0.8193])\n",
      "Epoch 761,Loss 4.958245\n",
      "\tParams: tensor([  4.5186, -12.4981])\n",
      "\tGrad:tensor([-0.1445,  0.8179])\n",
      "Epoch 762,Loss 4.951351\n",
      "\tParams: tensor([  4.5200, -12.5062])\n",
      "\tGrad:tensor([-0.1443,  0.8165])\n",
      "Epoch 763,Loss 4.944479\n",
      "\tParams: tensor([  4.5215, -12.5144])\n",
      "\tGrad:tensor([-0.1440,  0.8152])\n",
      "Epoch 764,Loss 4.937633\n",
      "\tParams: tensor([  4.5229, -12.5225])\n",
      "\tGrad:tensor([-0.1438,  0.8138])\n",
      "Epoch 765,Loss 4.930812\n",
      "\tParams: tensor([  4.5244, -12.5306])\n",
      "\tGrad:tensor([-0.1435,  0.8124])\n",
      "Epoch 766,Loss 4.924009\n",
      "\tParams: tensor([  4.5258, -12.5387])\n",
      "\tGrad:tensor([-0.1433,  0.8110])\n",
      "Epoch 767,Loss 4.917234\n",
      "\tParams: tensor([  4.5272, -12.5468])\n",
      "\tGrad:tensor([-0.1430,  0.8096])\n",
      "Epoch 768,Loss 4.910480\n",
      "\tParams: tensor([  4.5286, -12.5549])\n",
      "\tGrad:tensor([-0.1428,  0.8083])\n",
      "Epoch 769,Loss 4.903749\n",
      "\tParams: tensor([  4.5301, -12.5630])\n",
      "\tGrad:tensor([-0.1426,  0.8069])\n",
      "Epoch 770,Loss 4.897040\n",
      "\tParams: tensor([  4.5315, -12.5711])\n",
      "\tGrad:tensor([-0.1423,  0.8055])\n",
      "Epoch 771,Loss 4.890356\n",
      "\tParams: tensor([  4.5329, -12.5791])\n",
      "\tGrad:tensor([-0.1420,  0.8042])\n",
      "Epoch 772,Loss 4.883692\n",
      "\tParams: tensor([  4.5343, -12.5871])\n",
      "\tGrad:tensor([-0.1418,  0.8028])\n",
      "Epoch 773,Loss 4.877052\n",
      "\tParams: tensor([  4.5357, -12.5951])\n",
      "\tGrad:tensor([-0.1416,  0.8014])\n",
      "Epoch 774,Loss 4.870436\n",
      "\tParams: tensor([  4.5372, -12.6031])\n",
      "\tGrad:tensor([-0.1413,  0.8001])\n",
      "Epoch 775,Loss 4.863839\n",
      "\tParams: tensor([  4.5386, -12.6111])\n",
      "\tGrad:tensor([-0.1411,  0.7987])\n",
      "Epoch 776,Loss 4.857268\n",
      "\tParams: tensor([  4.5400, -12.6191])\n",
      "\tGrad:tensor([-0.1408,  0.7973])\n",
      "Epoch 777,Loss 4.850718\n",
      "\tParams: tensor([  4.5414, -12.6271])\n",
      "\tGrad:tensor([-0.1406,  0.7960])\n",
      "Epoch 778,Loss 4.844189\n",
      "\tParams: tensor([  4.5428, -12.6350])\n",
      "\tGrad:tensor([-0.1404,  0.7946])\n",
      "Epoch 779,Loss 4.837683\n",
      "\tParams: tensor([  4.5442, -12.6429])\n",
      "\tGrad:tensor([-0.1401,  0.7933])\n",
      "Epoch 780,Loss 4.831196\n",
      "\tParams: tensor([  4.5456, -12.6509])\n",
      "\tGrad:tensor([-0.1399,  0.7919])\n",
      "Epoch 781,Loss 4.824737\n",
      "\tParams: tensor([  4.5470, -12.6588])\n",
      "\tGrad:tensor([-0.1397,  0.7906])\n",
      "Epoch 782,Loss 4.818298\n",
      "\tParams: tensor([  4.5484, -12.6667])\n",
      "\tGrad:tensor([-0.1394,  0.7893])\n",
      "Epoch 783,Loss 4.811879\n",
      "\tParams: tensor([  4.5498, -12.6745])\n",
      "\tGrad:tensor([-0.1392,  0.7879])\n",
      "Epoch 784,Loss 4.805481\n",
      "\tParams: tensor([  4.5512, -12.6824])\n",
      "\tGrad:tensor([-0.1389,  0.7866])\n",
      "Epoch 785,Loss 4.799106\n",
      "\tParams: tensor([  4.5525, -12.6902])\n",
      "\tGrad:tensor([-0.1387,  0.7852])\n",
      "Epoch 786,Loss 4.792755\n",
      "\tParams: tensor([  4.5539, -12.6981])\n",
      "\tGrad:tensor([-0.1385,  0.7839])\n",
      "Epoch 787,Loss 4.786422\n",
      "\tParams: tensor([  4.5553, -12.7059])\n",
      "\tGrad:tensor([-0.1383,  0.7826])\n",
      "Epoch 788,Loss 4.780112\n",
      "\tParams: tensor([  4.5567, -12.7137])\n",
      "\tGrad:tensor([-0.1380,  0.7812])\n",
      "Epoch 789,Loss 4.773824\n",
      "\tParams: tensor([  4.5581, -12.7215])\n",
      "\tGrad:tensor([-0.1378,  0.7799])\n",
      "Epoch 790,Loss 4.767558\n",
      "\tParams: tensor([  4.5594, -12.7293])\n",
      "\tGrad:tensor([-0.1375,  0.7786])\n",
      "Epoch 791,Loss 4.761312\n",
      "\tParams: tensor([  4.5608, -12.7371])\n",
      "\tGrad:tensor([-0.1373,  0.7773])\n",
      "Epoch 792,Loss 4.755087\n",
      "\tParams: tensor([  4.5622, -12.7448])\n",
      "\tGrad:tensor([-0.1371,  0.7759])\n",
      "Epoch 793,Loss 4.748885\n",
      "\tParams: tensor([  4.5636, -12.7526])\n",
      "\tGrad:tensor([-0.1368,  0.7746])\n",
      "Epoch 794,Loss 4.742700\n",
      "\tParams: tensor([  4.5649, -12.7603])\n",
      "\tGrad:tensor([-0.1366,  0.7733])\n",
      "Epoch 795,Loss 4.736537\n",
      "\tParams: tensor([  4.5663, -12.7680])\n",
      "\tGrad:tensor([-0.1364,  0.7720])\n",
      "Epoch 796,Loss 4.730397\n",
      "\tParams: tensor([  4.5677, -12.7758])\n",
      "\tGrad:tensor([-0.1361,  0.7707])\n",
      "Epoch 797,Loss 4.724279\n",
      "\tParams: tensor([  4.5690, -12.7834])\n",
      "\tGrad:tensor([-0.1359,  0.7694])\n",
      "Epoch 798,Loss 4.718181\n",
      "\tParams: tensor([  4.5704, -12.7911])\n",
      "\tGrad:tensor([-0.1357,  0.7681])\n",
      "Epoch 799,Loss 4.712101\n",
      "\tParams: tensor([  4.5717, -12.7988])\n",
      "\tGrad:tensor([-0.1354,  0.7668])\n",
      "Epoch 800,Loss 4.706046\n",
      "\tParams: tensor([  4.5731, -12.8064])\n",
      "\tGrad:tensor([-0.1352,  0.7655])\n",
      "Epoch 801,Loss 4.700009\n",
      "\tParams: tensor([  4.5744, -12.8141])\n",
      "\tGrad:tensor([-0.1350,  0.7642])\n",
      "Epoch 802,Loss 4.693990\n",
      "\tParams: tensor([  4.5758, -12.8217])\n",
      "\tGrad:tensor([-0.1347,  0.7629])\n",
      "Epoch 803,Loss 4.687995\n",
      "\tParams: tensor([  4.5771, -12.8293])\n",
      "\tGrad:tensor([-0.1345,  0.7616])\n",
      "Epoch 804,Loss 4.682020\n",
      "\tParams: tensor([  4.5785, -12.8369])\n",
      "\tGrad:tensor([-0.1343,  0.7603])\n",
      "Epoch 805,Loss 4.676063\n",
      "\tParams: tensor([  4.5798, -12.8445])\n",
      "\tGrad:tensor([-0.1341,  0.7590])\n",
      "Epoch 806,Loss 4.670130\n",
      "\tParams: tensor([  4.5811, -12.8521])\n",
      "\tGrad:tensor([-0.1338,  0.7577])\n",
      "Epoch 807,Loss 4.664214\n",
      "\tParams: tensor([  4.5825, -12.8597])\n",
      "\tGrad:tensor([-0.1336,  0.7564])\n",
      "Epoch 808,Loss 4.658319\n",
      "\tParams: tensor([  4.5838, -12.8672])\n",
      "\tGrad:tensor([-0.1334,  0.7551])\n",
      "Epoch 809,Loss 4.652445\n",
      "\tParams: tensor([  4.5851, -12.8748])\n",
      "\tGrad:tensor([-0.1332,  0.7538])\n",
      "Epoch 810,Loss 4.646592\n",
      "\tParams: tensor([  4.5865, -12.8823])\n",
      "\tGrad:tensor([-0.1330,  0.7526])\n",
      "Epoch 811,Loss 4.640754\n",
      "\tParams: tensor([  4.5878, -12.8898])\n",
      "\tGrad:tensor([-0.1327,  0.7513])\n",
      "Epoch 812,Loss 4.634938\n",
      "\tParams: tensor([  4.5891, -12.8973])\n",
      "\tGrad:tensor([-0.1325,  0.7500])\n",
      "Epoch 813,Loss 4.629142\n",
      "\tParams: tensor([  4.5904, -12.9048])\n",
      "\tGrad:tensor([-0.1323,  0.7487])\n",
      "Epoch 814,Loss 4.623367\n",
      "\tParams: tensor([  4.5918, -12.9123])\n",
      "\tGrad:tensor([-0.1320,  0.7475])\n",
      "Epoch 815,Loss 4.617611\n",
      "\tParams: tensor([  4.5931, -12.9197])\n",
      "\tGrad:tensor([-0.1318,  0.7462])\n",
      "Epoch 816,Loss 4.611872\n",
      "\tParams: tensor([  4.5944, -12.9272])\n",
      "\tGrad:tensor([-0.1316,  0.7449])\n",
      "Epoch 817,Loss 4.606156\n",
      "\tParams: tensor([  4.5957, -12.9346])\n",
      "\tGrad:tensor([-0.1314,  0.7437])\n",
      "Epoch 818,Loss 4.600458\n",
      "\tParams: tensor([  4.5970, -12.9420])\n",
      "\tGrad:tensor([-0.1311,  0.7424])\n",
      "Epoch 819,Loss 4.594780\n",
      "\tParams: tensor([  4.5983, -12.9494])\n",
      "\tGrad:tensor([-0.1309,  0.7411])\n",
      "Epoch 820,Loss 4.589119\n",
      "\tParams: tensor([  4.5996, -12.9568])\n",
      "\tGrad:tensor([-0.1307,  0.7399])\n",
      "Epoch 821,Loss 4.583479\n",
      "\tParams: tensor([  4.6009, -12.9642])\n",
      "\tGrad:tensor([-0.1305,  0.7386])\n",
      "Epoch 822,Loss 4.577857\n",
      "\tParams: tensor([  4.6022, -12.9716])\n",
      "\tGrad:tensor([-0.1303,  0.7374])\n",
      "Epoch 823,Loss 4.572256\n",
      "\tParams: tensor([  4.6035, -12.9790])\n",
      "\tGrad:tensor([-0.1300,  0.7361])\n",
      "Epoch 824,Loss 4.566675\n",
      "\tParams: tensor([  4.6048, -12.9863])\n",
      "\tGrad:tensor([-0.1298,  0.7349])\n",
      "Epoch 825,Loss 4.561108\n",
      "\tParams: tensor([  4.6061, -12.9936])\n",
      "\tGrad:tensor([-0.1296,  0.7336])\n",
      "Epoch 826,Loss 4.555565\n",
      "\tParams: tensor([  4.6074, -13.0010])\n",
      "\tGrad:tensor([-0.1294,  0.7324])\n",
      "Epoch 827,Loss 4.550039\n",
      "\tParams: tensor([  4.6087, -13.0083])\n",
      "\tGrad:tensor([-0.1292,  0.7311])\n",
      "Epoch 828,Loss 4.544534\n",
      "\tParams: tensor([  4.6100, -13.0156])\n",
      "\tGrad:tensor([-0.1289,  0.7299])\n",
      "Epoch 829,Loss 4.539044\n",
      "\tParams: tensor([  4.6113, -13.0229])\n",
      "\tGrad:tensor([-0.1287,  0.7286])\n",
      "Epoch 830,Loss 4.533575\n",
      "\tParams: tensor([  4.6126, -13.0301])\n",
      "\tGrad:tensor([-0.1285,  0.7274])\n",
      "Epoch 831,Loss 4.528122\n",
      "\tParams: tensor([  4.6139, -13.0374])\n",
      "\tGrad:tensor([-0.1283,  0.7262])\n",
      "Epoch 832,Loss 4.522691\n",
      "\tParams: tensor([  4.6152, -13.0446])\n",
      "\tGrad:tensor([-0.1280,  0.7249])\n",
      "Epoch 833,Loss 4.517276\n",
      "\tParams: tensor([  4.6164, -13.0519])\n",
      "\tGrad:tensor([-0.1278,  0.7237])\n",
      "Epoch 834,Loss 4.511879\n",
      "\tParams: tensor([  4.6177, -13.0591])\n",
      "\tGrad:tensor([-0.1276,  0.7225])\n",
      "Epoch 835,Loss 4.506505\n",
      "\tParams: tensor([  4.6190, -13.0663])\n",
      "\tGrad:tensor([-0.1274,  0.7212])\n",
      "Epoch 836,Loss 4.501141\n",
      "\tParams: tensor([  4.6203, -13.0735])\n",
      "\tGrad:tensor([-0.1272,  0.7200])\n",
      "Epoch 837,Loss 4.495801\n",
      "\tParams: tensor([  4.6215, -13.0807])\n",
      "\tGrad:tensor([-0.1270,  0.7188])\n",
      "Epoch 838,Loss 4.490475\n",
      "\tParams: tensor([  4.6228, -13.0879])\n",
      "\tGrad:tensor([-0.1268,  0.7176])\n",
      "Epoch 839,Loss 4.485169\n",
      "\tParams: tensor([  4.6241, -13.0950])\n",
      "\tGrad:tensor([-0.1266,  0.7163])\n",
      "Epoch 840,Loss 4.479884\n",
      "\tParams: tensor([  4.6253, -13.1022])\n",
      "\tGrad:tensor([-0.1263,  0.7151])\n",
      "Epoch 841,Loss 4.474613\n",
      "\tParams: tensor([  4.6266, -13.1093])\n",
      "\tGrad:tensor([-0.1261,  0.7139])\n",
      "Epoch 842,Loss 4.469364\n",
      "\tParams: tensor([  4.6278, -13.1165])\n",
      "\tGrad:tensor([-0.1259,  0.7127])\n",
      "Epoch 843,Loss 4.464130\n",
      "\tParams: tensor([  4.6291, -13.1236])\n",
      "\tGrad:tensor([-0.1257,  0.7115])\n",
      "Epoch 844,Loss 4.458913\n",
      "\tParams: tensor([  4.6304, -13.1307])\n",
      "\tGrad:tensor([-0.1255,  0.7103])\n",
      "Epoch 845,Loss 4.453716\n",
      "\tParams: tensor([  4.6316, -13.1378])\n",
      "\tGrad:tensor([-0.1253,  0.7091])\n",
      "Epoch 846,Loss 4.448535\n",
      "\tParams: tensor([  4.6329, -13.1449])\n",
      "\tGrad:tensor([-0.1250,  0.7079])\n",
      "Epoch 847,Loss 4.443372\n",
      "\tParams: tensor([  4.6341, -13.1519])\n",
      "\tGrad:tensor([-0.1249,  0.7067])\n",
      "Epoch 848,Loss 4.438226\n",
      "\tParams: tensor([  4.6353, -13.1590])\n",
      "\tGrad:tensor([-0.1246,  0.7055])\n",
      "Epoch 849,Loss 4.433099\n",
      "\tParams: tensor([  4.6366, -13.1660])\n",
      "\tGrad:tensor([-0.1244,  0.7043])\n",
      "Epoch 850,Loss 4.427990\n",
      "\tParams: tensor([  4.6378, -13.1730])\n",
      "\tGrad:tensor([-0.1242,  0.7031])\n",
      "Epoch 851,Loss 4.422897\n",
      "\tParams: tensor([  4.6391, -13.1801])\n",
      "\tGrad:tensor([-0.1240,  0.7019])\n",
      "Epoch 852,Loss 4.417819\n",
      "\tParams: tensor([  4.6403, -13.1871])\n",
      "\tGrad:tensor([-0.1238,  0.7007])\n",
      "Epoch 853,Loss 4.412762\n",
      "\tParams: tensor([  4.6415, -13.1941])\n",
      "\tGrad:tensor([-0.1236,  0.6995])\n",
      "Epoch 854,Loss 4.407721\n",
      "\tParams: tensor([  4.6428, -13.2010])\n",
      "\tGrad:tensor([-0.1234,  0.6983])\n",
      "Epoch 855,Loss 4.402698\n",
      "\tParams: tensor([  4.6440, -13.2080])\n",
      "\tGrad:tensor([-0.1232,  0.6971])\n",
      "Epoch 856,Loss 4.397688\n",
      "\tParams: tensor([  4.6452, -13.2150])\n",
      "\tGrad:tensor([-0.1229,  0.6959])\n",
      "Epoch 857,Loss 4.392697\n",
      "\tParams: tensor([  4.6465, -13.2219])\n",
      "\tGrad:tensor([-0.1227,  0.6948])\n",
      "Epoch 858,Loss 4.387725\n",
      "\tParams: tensor([  4.6477, -13.2289])\n",
      "\tGrad:tensor([-0.1225,  0.6936])\n",
      "Epoch 859,Loss 4.382770\n",
      "\tParams: tensor([  4.6489, -13.2358])\n",
      "\tGrad:tensor([-0.1223,  0.6924])\n",
      "Epoch 860,Loss 4.377828\n",
      "\tParams: tensor([  4.6501, -13.2427])\n",
      "\tGrad:tensor([-0.1221,  0.6912])\n",
      "Epoch 861,Loss 4.372905\n",
      "\tParams: tensor([  4.6514, -13.2496])\n",
      "\tGrad:tensor([-0.1219,  0.6901])\n",
      "Epoch 862,Loss 4.368000\n",
      "\tParams: tensor([  4.6526, -13.2565])\n",
      "\tGrad:tensor([-0.1217,  0.6889])\n",
      "Epoch 863,Loss 4.363111\n",
      "\tParams: tensor([  4.6538, -13.2634])\n",
      "\tGrad:tensor([-0.1215,  0.6877])\n",
      "Epoch 864,Loss 4.358238\n",
      "\tParams: tensor([  4.6550, -13.2702])\n",
      "\tGrad:tensor([-0.1213,  0.6865])\n",
      "Epoch 865,Loss 4.353383\n",
      "\tParams: tensor([  4.6562, -13.2771])\n",
      "\tGrad:tensor([-0.1211,  0.6854])\n",
      "Epoch 866,Loss 4.348542\n",
      "\tParams: tensor([  4.6574, -13.2839])\n",
      "\tGrad:tensor([-0.1209,  0.6842])\n",
      "Epoch 867,Loss 4.343716\n",
      "\tParams: tensor([  4.6586, -13.2908])\n",
      "\tGrad:tensor([-0.1207,  0.6830])\n",
      "Epoch 868,Loss 4.338911\n",
      "\tParams: tensor([  4.6598, -13.2976])\n",
      "\tGrad:tensor([-0.1205,  0.6819])\n",
      "Epoch 869,Loss 4.334120\n",
      "\tParams: tensor([  4.6610, -13.3044])\n",
      "\tGrad:tensor([-0.1203,  0.6807])\n",
      "Epoch 870,Loss 4.329345\n",
      "\tParams: tensor([  4.6622, -13.3112])\n",
      "\tGrad:tensor([-0.1201,  0.6796])\n",
      "Epoch 871,Loss 4.324588\n",
      "\tParams: tensor([  4.6634, -13.3180])\n",
      "\tGrad:tensor([-0.1198,  0.6784])\n",
      "Epoch 872,Loss 4.319846\n",
      "\tParams: tensor([  4.6646, -13.3247])\n",
      "\tGrad:tensor([-0.1196,  0.6773])\n",
      "Epoch 873,Loss 4.315117\n",
      "\tParams: tensor([  4.6658, -13.3315])\n",
      "\tGrad:tensor([-0.1195,  0.6761])\n",
      "Epoch 874,Loss 4.310409\n",
      "\tParams: tensor([  4.6670, -13.3382])\n",
      "\tGrad:tensor([-0.1192,  0.6750])\n",
      "Epoch 875,Loss 4.305714\n",
      "\tParams: tensor([  4.6682, -13.3450])\n",
      "\tGrad:tensor([-0.1190,  0.6738])\n",
      "Epoch 876,Loss 4.301036\n",
      "\tParams: tensor([  4.6694, -13.3517])\n",
      "\tGrad:tensor([-0.1188,  0.6727])\n",
      "Epoch 877,Loss 4.296376\n",
      "\tParams: tensor([  4.6706, -13.3584])\n",
      "\tGrad:tensor([-0.1186,  0.6715])\n",
      "Epoch 878,Loss 4.291727\n",
      "\tParams: tensor([  4.6718, -13.3651])\n",
      "\tGrad:tensor([-0.1184,  0.6704])\n",
      "Epoch 879,Loss 4.287098\n",
      "\tParams: tensor([  4.6730, -13.3718])\n",
      "\tGrad:tensor([-0.1182,  0.6693])\n",
      "Epoch 880,Loss 4.282482\n",
      "\tParams: tensor([  4.6741, -13.3785])\n",
      "\tGrad:tensor([-0.1180,  0.6681])\n",
      "Epoch 881,Loss 4.277882\n",
      "\tParams: tensor([  4.6753, -13.3852])\n",
      "\tGrad:tensor([-0.1178,  0.6670])\n",
      "Epoch 882,Loss 4.273299\n",
      "\tParams: tensor([  4.6765, -13.3918])\n",
      "\tGrad:tensor([-0.1176,  0.6658])\n",
      "Epoch 883,Loss 4.268732\n",
      "\tParams: tensor([  4.6777, -13.3985])\n",
      "\tGrad:tensor([-0.1174,  0.6647])\n",
      "Epoch 884,Loss 4.264178\n",
      "\tParams: tensor([  4.6788, -13.4051])\n",
      "\tGrad:tensor([-0.1172,  0.6636])\n",
      "Epoch 885,Loss 4.259643\n",
      "\tParams: tensor([  4.6800, -13.4117])\n",
      "\tGrad:tensor([-0.1170,  0.6625])\n",
      "Epoch 886,Loss 4.255120\n",
      "\tParams: tensor([  4.6812, -13.4184])\n",
      "\tGrad:tensor([-0.1168,  0.6613])\n",
      "Epoch 887,Loss 4.250614\n",
      "\tParams: tensor([  4.6823, -13.4250])\n",
      "\tGrad:tensor([-0.1166,  0.6602])\n",
      "Epoch 888,Loss 4.246124\n",
      "\tParams: tensor([  4.6835, -13.4316])\n",
      "\tGrad:tensor([-0.1164,  0.6591])\n",
      "Epoch 889,Loss 4.241648\n",
      "\tParams: tensor([  4.6847, -13.4381])\n",
      "\tGrad:tensor([-0.1162,  0.6580])\n",
      "Epoch 890,Loss 4.237185\n",
      "\tParams: tensor([  4.6858, -13.4447])\n",
      "\tGrad:tensor([-0.1160,  0.6569])\n",
      "Epoch 891,Loss 4.232740\n",
      "\tParams: tensor([  4.6870, -13.4513])\n",
      "\tGrad:tensor([-0.1158,  0.6557])\n",
      "Epoch 892,Loss 4.228308\n",
      "\tParams: tensor([  4.6881, -13.4578])\n",
      "\tGrad:tensor([-0.1157,  0.6546])\n",
      "Epoch 893,Loss 4.223895\n",
      "\tParams: tensor([  4.6893, -13.4643])\n",
      "\tGrad:tensor([-0.1154,  0.6535])\n",
      "Epoch 894,Loss 4.219494\n",
      "\tParams: tensor([  4.6904, -13.4709])\n",
      "\tGrad:tensor([-0.1153,  0.6524])\n",
      "Epoch 895,Loss 4.215109\n",
      "\tParams: tensor([  4.6916, -13.4774])\n",
      "\tGrad:tensor([-0.1151,  0.6513])\n",
      "Epoch 896,Loss 4.210737\n",
      "\tParams: tensor([  4.6927, -13.4839])\n",
      "\tGrad:tensor([-0.1148,  0.6502])\n",
      "Epoch 897,Loss 4.206383\n",
      "\tParams: tensor([  4.6939, -13.4904])\n",
      "\tGrad:tensor([-0.1147,  0.6491])\n",
      "Epoch 898,Loss 4.202043\n",
      "\tParams: tensor([  4.6950, -13.4968])\n",
      "\tGrad:tensor([-0.1145,  0.6480])\n",
      "Epoch 899,Loss 4.197715\n",
      "\tParams: tensor([  4.6962, -13.5033])\n",
      "\tGrad:tensor([-0.1143,  0.6469])\n",
      "Epoch 900,Loss 4.193405\n",
      "\tParams: tensor([  4.6973, -13.5098])\n",
      "\tGrad:tensor([-0.1141,  0.6458])\n",
      "Epoch 901,Loss 4.189108\n",
      "\tParams: tensor([  4.6985, -13.5162])\n",
      "\tGrad:tensor([-0.1139,  0.6447])\n",
      "Epoch 902,Loss 4.184825\n",
      "\tParams: tensor([  4.6996, -13.5227])\n",
      "\tGrad:tensor([-0.1137,  0.6436])\n",
      "Epoch 903,Loss 4.180559\n",
      "\tParams: tensor([  4.7007, -13.5291])\n",
      "\tGrad:tensor([-0.1135,  0.6425])\n",
      "Epoch 904,Loss 4.176305\n",
      "\tParams: tensor([  4.7019, -13.5355])\n",
      "\tGrad:tensor([-0.1133,  0.6414])\n",
      "Epoch 905,Loss 4.172065\n",
      "\tParams: tensor([  4.7030, -13.5419])\n",
      "\tGrad:tensor([-0.1131,  0.6403])\n",
      "Epoch 906,Loss 4.167842\n",
      "\tParams: tensor([  4.7041, -13.5483])\n",
      "\tGrad:tensor([-0.1129,  0.6392])\n",
      "Epoch 907,Loss 4.163630\n",
      "\tParams: tensor([  4.7053, -13.5547])\n",
      "\tGrad:tensor([-0.1127,  0.6381])\n",
      "Epoch 908,Loss 4.159436\n",
      "\tParams: tensor([  4.7064, -13.5610])\n",
      "\tGrad:tensor([-0.1125,  0.6371])\n",
      "Epoch 909,Loss 4.155253\n",
      "\tParams: tensor([  4.7075, -13.5674])\n",
      "\tGrad:tensor([-0.1124,  0.6360])\n",
      "Epoch 910,Loss 4.151086\n",
      "\tParams: tensor([  4.7086, -13.5738])\n",
      "\tGrad:tensor([-0.1122,  0.6349])\n",
      "Epoch 911,Loss 4.146934\n",
      "\tParams: tensor([  4.7097, -13.5801])\n",
      "\tGrad:tensor([-0.1120,  0.6338])\n",
      "Epoch 912,Loss 4.142794\n",
      "\tParams: tensor([  4.7109, -13.5864])\n",
      "\tGrad:tensor([-0.1118,  0.6327])\n",
      "Epoch 913,Loss 4.138669\n",
      "\tParams: tensor([  4.7120, -13.5927])\n",
      "\tGrad:tensor([-0.1116,  0.6317])\n",
      "Epoch 914,Loss 4.134559\n",
      "\tParams: tensor([  4.7131, -13.5990])\n",
      "\tGrad:tensor([-0.1114,  0.6306])\n",
      "Epoch 915,Loss 4.130465\n",
      "\tParams: tensor([  4.7142, -13.6053])\n",
      "\tGrad:tensor([-0.1112,  0.6295])\n",
      "Epoch 916,Loss 4.126378\n",
      "\tParams: tensor([  4.7153, -13.6116])\n",
      "\tGrad:tensor([-0.1110,  0.6284])\n",
      "Epoch 917,Loss 4.122310\n",
      "\tParams: tensor([  4.7164, -13.6179])\n",
      "\tGrad:tensor([-0.1108,  0.6274])\n",
      "Epoch 918,Loss 4.118253\n",
      "\tParams: tensor([  4.7175, -13.6242])\n",
      "\tGrad:tensor([-0.1107,  0.6263])\n",
      "Epoch 919,Loss 4.114213\n",
      "\tParams: tensor([  4.7186, -13.6304])\n",
      "\tGrad:tensor([-0.1104,  0.6253])\n",
      "Epoch 920,Loss 4.110184\n",
      "\tParams: tensor([  4.7197, -13.6367])\n",
      "\tGrad:tensor([-0.1103,  0.6242])\n",
      "Epoch 921,Loss 4.106170\n",
      "\tParams: tensor([  4.7208, -13.6429])\n",
      "\tGrad:tensor([-0.1101,  0.6231])\n",
      "Epoch 922,Loss 4.102171\n",
      "\tParams: tensor([  4.7219, -13.6491])\n",
      "\tGrad:tensor([-0.1099,  0.6221])\n",
      "Epoch 923,Loss 4.098181\n",
      "\tParams: tensor([  4.7230, -13.6553])\n",
      "\tGrad:tensor([-0.1097,  0.6210])\n",
      "Epoch 924,Loss 4.094209\n",
      "\tParams: tensor([  4.7241, -13.6615])\n",
      "\tGrad:tensor([-0.1095,  0.6200])\n",
      "Epoch 925,Loss 4.090250\n",
      "\tParams: tensor([  4.7252, -13.6677])\n",
      "\tGrad:tensor([-0.1093,  0.6189])\n",
      "Epoch 926,Loss 4.086300\n",
      "\tParams: tensor([  4.7263, -13.6739])\n",
      "\tGrad:tensor([-0.1091,  0.6179])\n",
      "Epoch 927,Loss 4.082366\n",
      "\tParams: tensor([  4.7274, -13.6800])\n",
      "\tGrad:tensor([-0.1090,  0.6168])\n",
      "Epoch 928,Loss 4.078448\n",
      "\tParams: tensor([  4.7285, -13.6862])\n",
      "\tGrad:tensor([-0.1088,  0.6158])\n",
      "Epoch 929,Loss 4.074540\n",
      "\tParams: tensor([  4.7296, -13.6924])\n",
      "\tGrad:tensor([-0.1086,  0.6147])\n",
      "Epoch 930,Loss 4.070650\n",
      "\tParams: tensor([  4.7307, -13.6985])\n",
      "\tGrad:tensor([-0.1084,  0.6137])\n",
      "Epoch 931,Loss 4.066769\n",
      "\tParams: tensor([  4.7317, -13.7046])\n",
      "\tGrad:tensor([-0.1082,  0.6126])\n",
      "Epoch 932,Loss 4.062900\n",
      "\tParams: tensor([  4.7328, -13.7107])\n",
      "\tGrad:tensor([-0.1080,  0.6116])\n",
      "Epoch 933,Loss 4.059047\n",
      "\tParams: tensor([  4.7339, -13.7168])\n",
      "\tGrad:tensor([-0.1079,  0.6105])\n",
      "Epoch 934,Loss 4.055204\n",
      "\tParams: tensor([  4.7350, -13.7229])\n",
      "\tGrad:tensor([-0.1077,  0.6095])\n",
      "Epoch 935,Loss 4.051378\n",
      "\tParams: tensor([  4.7360, -13.7290])\n",
      "\tGrad:tensor([-0.1075,  0.6085])\n",
      "Epoch 936,Loss 4.047564\n",
      "\tParams: tensor([  4.7371, -13.7351])\n",
      "\tGrad:tensor([-0.1073,  0.6074])\n",
      "Epoch 937,Loss 4.043762\n",
      "\tParams: tensor([  4.7382, -13.7412])\n",
      "\tGrad:tensor([-0.1071,  0.6064])\n",
      "Epoch 938,Loss 4.039972\n",
      "\tParams: tensor([  4.7393, -13.7472])\n",
      "\tGrad:tensor([-0.1069,  0.6054])\n",
      "Epoch 939,Loss 4.036197\n",
      "\tParams: tensor([  4.7403, -13.7533])\n",
      "\tGrad:tensor([-0.1068,  0.6043])\n",
      "Epoch 940,Loss 4.032433\n",
      "\tParams: tensor([  4.7414, -13.7593])\n",
      "\tGrad:tensor([-0.1066,  0.6033])\n",
      "Epoch 941,Loss 4.028685\n",
      "\tParams: tensor([  4.7425, -13.7653])\n",
      "\tGrad:tensor([-0.1064,  0.6023])\n",
      "Epoch 942,Loss 4.024947\n",
      "\tParams: tensor([  4.7435, -13.7713])\n",
      "\tGrad:tensor([-0.1062,  0.6013])\n",
      "Epoch 943,Loss 4.021221\n",
      "\tParams: tensor([  4.7446, -13.7773])\n",
      "\tGrad:tensor([-0.1060,  0.6003])\n",
      "Epoch 944,Loss 4.017508\n",
      "\tParams: tensor([  4.7456, -13.7833])\n",
      "\tGrad:tensor([-0.1058,  0.5992])\n",
      "Epoch 945,Loss 4.013809\n",
      "\tParams: tensor([  4.7467, -13.7893])\n",
      "\tGrad:tensor([-0.1057,  0.5982])\n",
      "Epoch 946,Loss 4.010123\n",
      "\tParams: tensor([  4.7478, -13.7953])\n",
      "\tGrad:tensor([-0.1055,  0.5972])\n",
      "Epoch 947,Loss 4.006446\n",
      "\tParams: tensor([  4.7488, -13.8012])\n",
      "\tGrad:tensor([-0.1053,  0.5962])\n",
      "Epoch 948,Loss 4.002786\n",
      "\tParams: tensor([  4.7499, -13.8072])\n",
      "\tGrad:tensor([-0.1051,  0.5952])\n",
      "Epoch 949,Loss 3.999135\n",
      "\tParams: tensor([  4.7509, -13.8131])\n",
      "\tGrad:tensor([-0.1050,  0.5942])\n",
      "Epoch 950,Loss 3.995498\n",
      "\tParams: tensor([  4.7520, -13.8191])\n",
      "\tGrad:tensor([-0.1048,  0.5931])\n",
      "Epoch 951,Loss 3.991874\n",
      "\tParams: tensor([  4.7530, -13.8250])\n",
      "\tGrad:tensor([-0.1046,  0.5921])\n",
      "Epoch 952,Loss 3.988261\n",
      "\tParams: tensor([  4.7540, -13.8309])\n",
      "\tGrad:tensor([-0.1044,  0.5911])\n",
      "Epoch 953,Loss 3.984660\n",
      "\tParams: tensor([  4.7551, -13.8368])\n",
      "\tGrad:tensor([-0.1042,  0.5901])\n",
      "Epoch 954,Loss 3.981073\n",
      "\tParams: tensor([  4.7561, -13.8427])\n",
      "\tGrad:tensor([-0.1041,  0.5891])\n",
      "Epoch 955,Loss 3.977496\n",
      "\tParams: tensor([  4.7572, -13.8486])\n",
      "\tGrad:tensor([-0.1039,  0.5881])\n",
      "Epoch 956,Loss 3.973931\n",
      "\tParams: tensor([  4.7582, -13.8544])\n",
      "\tGrad:tensor([-0.1037,  0.5871])\n",
      "Epoch 957,Loss 3.970381\n",
      "\tParams: tensor([  4.7592, -13.8603])\n",
      "\tGrad:tensor([-0.1035,  0.5861])\n",
      "Epoch 958,Loss 3.966841\n",
      "\tParams: tensor([  4.7603, -13.8661])\n",
      "\tGrad:tensor([-0.1034,  0.5851])\n",
      "Epoch 959,Loss 3.963313\n",
      "\tParams: tensor([  4.7613, -13.8720])\n",
      "\tGrad:tensor([-0.1032,  0.5841])\n",
      "Epoch 960,Loss 3.959796\n",
      "\tParams: tensor([  4.7623, -13.8778])\n",
      "\tGrad:tensor([-0.1030,  0.5831])\n",
      "Epoch 961,Loss 3.956295\n",
      "\tParams: tensor([  4.7634, -13.8836])\n",
      "\tGrad:tensor([-0.1028,  0.5822])\n",
      "Epoch 962,Loss 3.952801\n",
      "\tParams: tensor([  4.7644, -13.8895])\n",
      "\tGrad:tensor([-0.1026,  0.5812])\n",
      "Epoch 963,Loss 3.949323\n",
      "\tParams: tensor([  4.7654, -13.8953])\n",
      "\tGrad:tensor([-0.1025,  0.5802])\n",
      "Epoch 964,Loss 3.945855\n",
      "\tParams: tensor([  4.7664, -13.9010])\n",
      "\tGrad:tensor([-0.1023,  0.5792])\n",
      "Epoch 965,Loss 3.942398\n",
      "\tParams: tensor([  4.7675, -13.9068])\n",
      "\tGrad:tensor([-0.1021,  0.5782])\n",
      "Epoch 966,Loss 3.938954\n",
      "\tParams: tensor([  4.7685, -13.9126])\n",
      "\tGrad:tensor([-0.1020,  0.5772])\n",
      "Epoch 967,Loss 3.935520\n",
      "\tParams: tensor([  4.7695, -13.9184])\n",
      "\tGrad:tensor([-0.1018,  0.5762])\n",
      "Epoch 968,Loss 3.932096\n",
      "\tParams: tensor([  4.7705, -13.9241])\n",
      "\tGrad:tensor([-0.1016,  0.5753])\n",
      "Epoch 969,Loss 3.928688\n",
      "\tParams: tensor([  4.7715, -13.9299])\n",
      "\tGrad:tensor([-0.1015,  0.5743])\n",
      "Epoch 970,Loss 3.925292\n",
      "\tParams: tensor([  4.7725, -13.9356])\n",
      "\tGrad:tensor([-0.1013,  0.5733])\n",
      "Epoch 971,Loss 3.921906\n",
      "\tParams: tensor([  4.7736, -13.9413])\n",
      "\tGrad:tensor([-0.1011,  0.5723])\n",
      "Epoch 972,Loss 3.918527\n",
      "\tParams: tensor([  4.7746, -13.9470])\n",
      "\tGrad:tensor([-0.1009,  0.5714])\n",
      "Epoch 973,Loss 3.915166\n",
      "\tParams: tensor([  4.7756, -13.9527])\n",
      "\tGrad:tensor([-0.1008,  0.5704])\n",
      "Epoch 974,Loss 3.911815\n",
      "\tParams: tensor([  4.7766, -13.9584])\n",
      "\tGrad:tensor([-0.1006,  0.5694])\n",
      "Epoch 975,Loss 3.908474\n",
      "\tParams: tensor([  4.7776, -13.9641])\n",
      "\tGrad:tensor([-0.1004,  0.5685])\n",
      "Epoch 976,Loss 3.905143\n",
      "\tParams: tensor([  4.7786, -13.9698])\n",
      "\tGrad:tensor([-0.1003,  0.5675])\n",
      "Epoch 977,Loss 3.901825\n",
      "\tParams: tensor([  4.7796, -13.9755])\n",
      "\tGrad:tensor([-0.1001,  0.5665])\n",
      "Epoch 978,Loss 3.898517\n",
      "\tParams: tensor([  4.7806, -13.9811])\n",
      "\tGrad:tensor([-0.0999,  0.5656])\n",
      "Epoch 979,Loss 3.895222\n",
      "\tParams: tensor([  4.7816, -13.9868])\n",
      "\tGrad:tensor([-0.0997,  0.5646])\n",
      "Epoch 980,Loss 3.891935\n",
      "\tParams: tensor([  4.7826, -13.9924])\n",
      "\tGrad:tensor([-0.0996,  0.5637])\n",
      "Epoch 981,Loss 3.888664\n",
      "\tParams: tensor([  4.7836, -13.9980])\n",
      "\tGrad:tensor([-0.0994,  0.5627])\n",
      "Epoch 982,Loss 3.885401\n",
      "\tParams: tensor([  4.7846, -14.0036])\n",
      "\tGrad:tensor([-0.0992,  0.5617])\n",
      "Epoch 983,Loss 3.882150\n",
      "\tParams: tensor([  4.7856, -14.0092])\n",
      "\tGrad:tensor([-0.0991,  0.5608])\n",
      "Epoch 984,Loss 3.878910\n",
      "\tParams: tensor([  4.7865, -14.0148])\n",
      "\tGrad:tensor([-0.0989,  0.5598])\n",
      "Epoch 985,Loss 3.875680\n",
      "\tParams: tensor([  4.7875, -14.0204])\n",
      "\tGrad:tensor([-0.0987,  0.5589])\n",
      "Epoch 986,Loss 3.872463\n",
      "\tParams: tensor([  4.7885, -14.0260])\n",
      "\tGrad:tensor([-0.0986,  0.5579])\n",
      "Epoch 987,Loss 3.869256\n",
      "\tParams: tensor([  4.7895, -14.0316])\n",
      "\tGrad:tensor([-0.0984,  0.5570])\n",
      "Epoch 988,Loss 3.866060\n",
      "\tParams: tensor([  4.7905, -14.0371])\n",
      "\tGrad:tensor([-0.0982,  0.5560])\n",
      "Epoch 989,Loss 3.862872\n",
      "\tParams: tensor([  4.7915, -14.0427])\n",
      "\tGrad:tensor([-0.0981,  0.5551])\n",
      "Epoch 990,Loss 3.859699\n",
      "\tParams: tensor([  4.7924, -14.0482])\n",
      "\tGrad:tensor([-0.0979,  0.5541])\n",
      "Epoch 991,Loss 3.856535\n",
      "\tParams: tensor([  4.7934, -14.0538])\n",
      "\tGrad:tensor([-0.0978,  0.5532])\n",
      "Epoch 992,Loss 3.853381\n",
      "\tParams: tensor([  4.7944, -14.0593])\n",
      "\tGrad:tensor([-0.0976,  0.5523])\n",
      "Epoch 993,Loss 3.850237\n",
      "\tParams: tensor([  4.7954, -14.0648])\n",
      "\tGrad:tensor([-0.0974,  0.5513])\n",
      "Epoch 994,Loss 3.847109\n",
      "\tParams: tensor([  4.7963, -14.0703])\n",
      "\tGrad:tensor([-0.0973,  0.5504])\n",
      "Epoch 995,Loss 3.843984\n",
      "\tParams: tensor([  4.7973, -14.0758])\n",
      "\tGrad:tensor([-0.0971,  0.5495])\n",
      "Epoch 996,Loss 3.840876\n",
      "\tParams: tensor([  4.7983, -14.0813])\n",
      "\tGrad:tensor([-0.0969,  0.5485])\n",
      "Epoch 997,Loss 3.837775\n",
      "\tParams: tensor([  4.7992, -14.0868])\n",
      "\tGrad:tensor([-0.0967,  0.5476])\n",
      "Epoch 998,Loss 3.834686\n",
      "\tParams: tensor([  4.8002, -14.0922])\n",
      "\tGrad:tensor([-0.0966,  0.5467])\n",
      "Epoch 999,Loss 3.831606\n",
      "\tParams: tensor([  4.8012, -14.0977])\n",
      "\tGrad:tensor([-0.0964,  0.5457])\n",
      "Epoch 1000,Loss 3.828538\n",
      "\tParams: tensor([  4.8021, -14.1031])\n",
      "\tGrad:tensor([-0.0962,  0.5448])\n",
      "Epoch 1001,Loss 3.825483\n",
      "\tParams: tensor([  4.8031, -14.1086])\n",
      "\tGrad:tensor([-0.0961,  0.5439])\n",
      "Epoch 1002,Loss 3.822433\n",
      "\tParams: tensor([  4.8041, -14.1140])\n",
      "\tGrad:tensor([-0.0959,  0.5430])\n",
      "Epoch 1003,Loss 3.819398\n",
      "\tParams: tensor([  4.8050, -14.1194])\n",
      "\tGrad:tensor([-0.0957,  0.5420])\n",
      "Epoch 1004,Loss 3.816369\n",
      "\tParams: tensor([  4.8060, -14.1248])\n",
      "\tGrad:tensor([-0.0956,  0.5411])\n",
      "Epoch 1005,Loss 3.813350\n",
      "\tParams: tensor([  4.8069, -14.1302])\n",
      "\tGrad:tensor([-0.0954,  0.5402])\n",
      "Epoch 1006,Loss 3.810344\n",
      "\tParams: tensor([  4.8079, -14.1356])\n",
      "\tGrad:tensor([-0.0953,  0.5393])\n",
      "Epoch 1007,Loss 3.807348\n",
      "\tParams: tensor([  4.8088, -14.1410])\n",
      "\tGrad:tensor([-0.0951,  0.5384])\n",
      "Epoch 1008,Loss 3.804360\n",
      "\tParams: tensor([  4.8098, -14.1464])\n",
      "\tGrad:tensor([-0.0949,  0.5375])\n",
      "Epoch 1009,Loss 3.801384\n",
      "\tParams: tensor([  4.8107, -14.1518])\n",
      "\tGrad:tensor([-0.0948,  0.5365])\n",
      "Epoch 1010,Loss 3.798421\n",
      "\tParams: tensor([  4.8117, -14.1571])\n",
      "\tGrad:tensor([-0.0946,  0.5356])\n",
      "Epoch 1011,Loss 3.795465\n",
      "\tParams: tensor([  4.8126, -14.1625])\n",
      "\tGrad:tensor([-0.0945,  0.5347])\n",
      "Epoch 1012,Loss 3.792518\n",
      "\tParams: tensor([  4.8136, -14.1678])\n",
      "\tGrad:tensor([-0.0943,  0.5338])\n",
      "Epoch 1013,Loss 3.789584\n",
      "\tParams: tensor([  4.8145, -14.1731])\n",
      "\tGrad:tensor([-0.0942,  0.5329])\n",
      "Epoch 1014,Loss 3.786658\n",
      "\tParams: tensor([  4.8154, -14.1784])\n",
      "\tGrad:tensor([-0.0940,  0.5320])\n",
      "Epoch 1015,Loss 3.783740\n",
      "\tParams: tensor([  4.8164, -14.1838])\n",
      "\tGrad:tensor([-0.0938,  0.5311])\n",
      "Epoch 1016,Loss 3.780832\n",
      "\tParams: tensor([  4.8173, -14.1891])\n",
      "\tGrad:tensor([-0.0937,  0.5302])\n",
      "Epoch 1017,Loss 3.777939\n",
      "\tParams: tensor([  4.8183, -14.1943])\n",
      "\tGrad:tensor([-0.0935,  0.5293])\n",
      "Epoch 1018,Loss 3.775053\n",
      "\tParams: tensor([  4.8192, -14.1996])\n",
      "\tGrad:tensor([-0.0933,  0.5284])\n",
      "Epoch 1019,Loss 3.772173\n",
      "\tParams: tensor([  4.8201, -14.2049])\n",
      "\tGrad:tensor([-0.0932,  0.5275])\n",
      "Epoch 1020,Loss 3.769311\n",
      "\tParams: tensor([  4.8210, -14.2102])\n",
      "\tGrad:tensor([-0.0930,  0.5266])\n",
      "Epoch 1021,Loss 3.766450\n",
      "\tParams: tensor([  4.8220, -14.2154])\n",
      "\tGrad:tensor([-0.0929,  0.5257])\n",
      "Epoch 1022,Loss 3.763602\n",
      "\tParams: tensor([  4.8229, -14.2207])\n",
      "\tGrad:tensor([-0.0927,  0.5248])\n",
      "Epoch 1023,Loss 3.760766\n",
      "\tParams: tensor([  4.8238, -14.2259])\n",
      "\tGrad:tensor([-0.0926,  0.5239])\n",
      "Epoch 1024,Loss 3.757936\n",
      "\tParams: tensor([  4.8248, -14.2311])\n",
      "\tGrad:tensor([-0.0924,  0.5230])\n",
      "Epoch 1025,Loss 3.755118\n",
      "\tParams: tensor([  4.8257, -14.2364])\n",
      "\tGrad:tensor([-0.0922,  0.5221])\n",
      "Epoch 1026,Loss 3.752309\n",
      "\tParams: tensor([  4.8266, -14.2416])\n",
      "\tGrad:tensor([-0.0921,  0.5213])\n",
      "Epoch 1027,Loss 3.749511\n",
      "\tParams: tensor([  4.8275, -14.2468])\n",
      "\tGrad:tensor([-0.0919,  0.5204])\n",
      "Epoch 1028,Loss 3.746722\n",
      "\tParams: tensor([  4.8284, -14.2520])\n",
      "\tGrad:tensor([-0.0918,  0.5195])\n",
      "Epoch 1029,Loss 3.743940\n",
      "\tParams: tensor([  4.8293, -14.2572])\n",
      "\tGrad:tensor([-0.0916,  0.5186])\n",
      "Epoch 1030,Loss 3.741169\n",
      "\tParams: tensor([  4.8303, -14.2623])\n",
      "\tGrad:tensor([-0.0915,  0.5177])\n",
      "Epoch 1031,Loss 3.738407\n",
      "\tParams: tensor([  4.8312, -14.2675])\n",
      "\tGrad:tensor([-0.0913,  0.5168])\n",
      "Epoch 1032,Loss 3.735656\n",
      "\tParams: tensor([  4.8321, -14.2727])\n",
      "\tGrad:tensor([-0.0912,  0.5160])\n",
      "Epoch 1033,Loss 3.732914\n",
      "\tParams: tensor([  4.8330, -14.2778])\n",
      "\tGrad:tensor([-0.0910,  0.5151])\n",
      "Epoch 1034,Loss 3.730181\n",
      "\tParams: tensor([  4.8339, -14.2830])\n",
      "\tGrad:tensor([-0.0908,  0.5142])\n",
      "Epoch 1035,Loss 3.727456\n",
      "\tParams: tensor([  4.8348, -14.2881])\n",
      "\tGrad:tensor([-0.0907,  0.5133])\n",
      "Epoch 1036,Loss 3.724740\n",
      "\tParams: tensor([  4.8357, -14.2932])\n",
      "\tGrad:tensor([-0.0905,  0.5125])\n",
      "Epoch 1037,Loss 3.722034\n",
      "\tParams: tensor([  4.8366, -14.2983])\n",
      "\tGrad:tensor([-0.0904,  0.5116])\n",
      "Epoch 1038,Loss 3.719337\n",
      "\tParams: tensor([  4.8375, -14.3034])\n",
      "\tGrad:tensor([-0.0902,  0.5107])\n",
      "Epoch 1039,Loss 3.716651\n",
      "\tParams: tensor([  4.8384, -14.3085])\n",
      "\tGrad:tensor([-0.0901,  0.5099])\n",
      "Epoch 1040,Loss 3.713972\n",
      "\tParams: tensor([  4.8393, -14.3136])\n",
      "\tGrad:tensor([-0.0899,  0.5090])\n",
      "Epoch 1041,Loss 3.711302\n",
      "\tParams: tensor([  4.8402, -14.3187])\n",
      "\tGrad:tensor([-0.0898,  0.5081])\n",
      "Epoch 1042,Loss 3.708644\n",
      "\tParams: tensor([  4.8411, -14.3238])\n",
      "\tGrad:tensor([-0.0896,  0.5073])\n",
      "Epoch 1043,Loss 3.705991\n",
      "\tParams: tensor([  4.8420, -14.3288])\n",
      "\tGrad:tensor([-0.0895,  0.5064])\n",
      "Epoch 1044,Loss 3.703351\n",
      "\tParams: tensor([  4.8429, -14.3339])\n",
      "\tGrad:tensor([-0.0893,  0.5055])\n",
      "Epoch 1045,Loss 3.700716\n",
      "\tParams: tensor([  4.8438, -14.3390])\n",
      "\tGrad:tensor([-0.0892,  0.5047])\n",
      "Epoch 1046,Loss 3.698091\n",
      "\tParams: tensor([  4.8447, -14.3440])\n",
      "\tGrad:tensor([-0.0890,  0.5038])\n",
      "Epoch 1047,Loss 3.695476\n",
      "\tParams: tensor([  4.8456, -14.3490])\n",
      "\tGrad:tensor([-0.0888,  0.5030])\n",
      "Epoch 1048,Loss 3.692869\n",
      "\tParams: tensor([  4.8465, -14.3540])\n",
      "\tGrad:tensor([-0.0887,  0.5021])\n",
      "Epoch 1049,Loss 3.690273\n",
      "\tParams: tensor([  4.8473, -14.3591])\n",
      "\tGrad:tensor([-0.0886,  0.5013])\n",
      "Epoch 1050,Loss 3.687683\n",
      "\tParams: tensor([  4.8482, -14.3641])\n",
      "\tGrad:tensor([-0.0884,  0.5004])\n",
      "Epoch 1051,Loss 3.685104\n",
      "\tParams: tensor([  4.8491, -14.3691])\n",
      "\tGrad:tensor([-0.0882,  0.4996])\n",
      "Epoch 1052,Loss 3.682532\n",
      "\tParams: tensor([  4.8500, -14.3740])\n",
      "\tGrad:tensor([-0.0881,  0.4987])\n",
      "Epoch 1053,Loss 3.679969\n",
      "\tParams: tensor([  4.8509, -14.3790])\n",
      "\tGrad:tensor([-0.0879,  0.4979])\n",
      "Epoch 1054,Loss 3.677417\n",
      "\tParams: tensor([  4.8518, -14.3840])\n",
      "\tGrad:tensor([-0.0878,  0.4970])\n",
      "Epoch 1055,Loss 3.674871\n",
      "\tParams: tensor([  4.8526, -14.3889])\n",
      "\tGrad:tensor([-0.0877,  0.4962])\n",
      "Epoch 1056,Loss 3.672335\n",
      "\tParams: tensor([  4.8535, -14.3939])\n",
      "\tGrad:tensor([-0.0875,  0.4953])\n",
      "Epoch 1057,Loss 3.669804\n",
      "\tParams: tensor([  4.8544, -14.3988])\n",
      "\tGrad:tensor([-0.0873,  0.4945])\n",
      "Epoch 1058,Loss 3.667287\n",
      "\tParams: tensor([  4.8552, -14.4038])\n",
      "\tGrad:tensor([-0.0872,  0.4936])\n",
      "Epoch 1059,Loss 3.664775\n",
      "\tParams: tensor([  4.8561, -14.4087])\n",
      "\tGrad:tensor([-0.0870,  0.4928])\n",
      "Epoch 1060,Loss 3.662273\n",
      "\tParams: tensor([  4.8570, -14.4136])\n",
      "\tGrad:tensor([-0.0869,  0.4920])\n",
      "Epoch 1061,Loss 3.659778\n",
      "\tParams: tensor([  4.8579, -14.4185])\n",
      "\tGrad:tensor([-0.0868,  0.4911])\n",
      "Epoch 1062,Loss 3.657295\n",
      "\tParams: tensor([  4.8587, -14.4234])\n",
      "\tGrad:tensor([-0.0866,  0.4903])\n",
      "Epoch 1063,Loss 3.654816\n",
      "\tParams: tensor([  4.8596, -14.4283])\n",
      "\tGrad:tensor([-0.0865,  0.4895])\n",
      "Epoch 1064,Loss 3.652349\n",
      "\tParams: tensor([  4.8604, -14.4332])\n",
      "\tGrad:tensor([-0.0863,  0.4886])\n",
      "Epoch 1065,Loss 3.649889\n",
      "\tParams: tensor([  4.8613, -14.4381])\n",
      "\tGrad:tensor([-0.0862,  0.4878])\n",
      "Epoch 1066,Loss 3.647437\n",
      "\tParams: tensor([  4.8622, -14.4430])\n",
      "\tGrad:tensor([-0.0860,  0.4870])\n",
      "Epoch 1067,Loss 3.644991\n",
      "\tParams: tensor([  4.8630, -14.4478])\n",
      "\tGrad:tensor([-0.0859,  0.4862])\n",
      "Epoch 1068,Loss 3.642559\n",
      "\tParams: tensor([  4.8639, -14.4527])\n",
      "\tGrad:tensor([-0.0857,  0.4853])\n",
      "Epoch 1069,Loss 3.640132\n",
      "\tParams: tensor([  4.8647, -14.4575])\n",
      "\tGrad:tensor([-0.0856,  0.4845])\n",
      "Epoch 1070,Loss 3.637711\n",
      "\tParams: tensor([  4.8656, -14.4624])\n",
      "\tGrad:tensor([-0.0854,  0.4837])\n",
      "Epoch 1071,Loss 3.635302\n",
      "\tParams: tensor([  4.8665, -14.4672])\n",
      "\tGrad:tensor([-0.0853,  0.4829])\n",
      "Epoch 1072,Loss 3.632902\n",
      "\tParams: tensor([  4.8673, -14.4720])\n",
      "\tGrad:tensor([-0.0851,  0.4820])\n",
      "Epoch 1073,Loss 3.630508\n",
      "\tParams: tensor([  4.8682, -14.4768])\n",
      "\tGrad:tensor([-0.0850,  0.4812])\n",
      "Epoch 1074,Loss 3.628119\n",
      "\tParams: tensor([  4.8690, -14.4816])\n",
      "\tGrad:tensor([-0.0849,  0.4804])\n",
      "Epoch 1075,Loss 3.625741\n",
      "\tParams: tensor([  4.8698, -14.4864])\n",
      "\tGrad:tensor([-0.0847,  0.4796])\n",
      "Epoch 1076,Loss 3.623374\n",
      "\tParams: tensor([  4.8707, -14.4912])\n",
      "\tGrad:tensor([-0.0846,  0.4788])\n",
      "Epoch 1077,Loss 3.621010\n",
      "\tParams: tensor([  4.8715, -14.4960])\n",
      "\tGrad:tensor([-0.0844,  0.4780])\n",
      "Epoch 1078,Loss 3.618659\n",
      "\tParams: tensor([  4.8724, -14.5008])\n",
      "\tGrad:tensor([-0.0843,  0.4771])\n",
      "Epoch 1079,Loss 3.616311\n",
      "\tParams: tensor([  4.8732, -14.5055])\n",
      "\tGrad:tensor([-0.0841,  0.4763])\n",
      "Epoch 1080,Loss 3.613973\n",
      "\tParams: tensor([  4.8741, -14.5103])\n",
      "\tGrad:tensor([-0.0840,  0.4755])\n",
      "Epoch 1081,Loss 3.611643\n",
      "\tParams: tensor([  4.8749, -14.5150])\n",
      "\tGrad:tensor([-0.0839,  0.4747])\n",
      "Epoch 1082,Loss 3.609321\n",
      "\tParams: tensor([  4.8757, -14.5198])\n",
      "\tGrad:tensor([-0.0837,  0.4739])\n",
      "Epoch 1083,Loss 3.607008\n",
      "\tParams: tensor([  4.8766, -14.5245])\n",
      "\tGrad:tensor([-0.0836,  0.4731])\n",
      "Epoch 1084,Loss 3.604701\n",
      "\tParams: tensor([  4.8774, -14.5292])\n",
      "\tGrad:tensor([-0.0834,  0.4723])\n",
      "Epoch 1085,Loss 3.602403\n",
      "\tParams: tensor([  4.8782, -14.5339])\n",
      "\tGrad:tensor([-0.0833,  0.4715])\n",
      "Epoch 1086,Loss 3.600114\n",
      "\tParams: tensor([  4.8791, -14.5387])\n",
      "\tGrad:tensor([-0.0832,  0.4707])\n",
      "Epoch 1087,Loss 3.597831\n",
      "\tParams: tensor([  4.8799, -14.5434])\n",
      "\tGrad:tensor([-0.0830,  0.4699])\n",
      "Epoch 1088,Loss 3.595553\n",
      "\tParams: tensor([  4.8807, -14.5480])\n",
      "\tGrad:tensor([-0.0829,  0.4691])\n",
      "Epoch 1089,Loss 3.593287\n",
      "\tParams: tensor([  4.8816, -14.5527])\n",
      "\tGrad:tensor([-0.0827,  0.4683])\n",
      "Epoch 1090,Loss 3.591030\n",
      "\tParams: tensor([  4.8824, -14.5574])\n",
      "\tGrad:tensor([-0.0826,  0.4675])\n",
      "Epoch 1091,Loss 3.588776\n",
      "\tParams: tensor([  4.8832, -14.5621])\n",
      "\tGrad:tensor([-0.0824,  0.4667])\n",
      "Epoch 1092,Loss 3.586534\n",
      "\tParams: tensor([  4.8840, -14.5667])\n",
      "\tGrad:tensor([-0.0823,  0.4659])\n",
      "Epoch 1093,Loss 3.584294\n",
      "\tParams: tensor([  4.8849, -14.5714])\n",
      "\tGrad:tensor([-0.0822,  0.4651])\n",
      "Epoch 1094,Loss 3.582067\n",
      "\tParams: tensor([  4.8857, -14.5760])\n",
      "\tGrad:tensor([-0.0820,  0.4643])\n",
      "Epoch 1095,Loss 3.579845\n",
      "\tParams: tensor([  4.8865, -14.5807])\n",
      "\tGrad:tensor([-0.0819,  0.4636])\n",
      "Epoch 1096,Loss 3.577631\n",
      "\tParams: tensor([  4.8873, -14.5853])\n",
      "\tGrad:tensor([-0.0818,  0.4628])\n",
      "Epoch 1097,Loss 3.575424\n",
      "\tParams: tensor([  4.8881, -14.5899])\n",
      "\tGrad:tensor([-0.0816,  0.4620])\n",
      "Epoch 1098,Loss 3.573225\n",
      "\tParams: tensor([  4.8889, -14.5945])\n",
      "\tGrad:tensor([-0.0815,  0.4612])\n",
      "Epoch 1099,Loss 3.571035\n",
      "\tParams: tensor([  4.8898, -14.5991])\n",
      "\tGrad:tensor([-0.0813,  0.4604])\n",
      "Epoch 1100,Loss 3.568848\n",
      "\tParams: tensor([  4.8906, -14.6037])\n",
      "\tGrad:tensor([-0.0812,  0.4596])\n",
      "Epoch 1101,Loss 3.566673\n",
      "\tParams: tensor([  4.8914, -14.6083])\n",
      "\tGrad:tensor([-0.0810,  0.4588])\n",
      "Epoch 1102,Loss 3.564506\n",
      "\tParams: tensor([  4.8922, -14.6129])\n",
      "\tGrad:tensor([-0.0809,  0.4581])\n",
      "Epoch 1103,Loss 3.562341\n",
      "\tParams: tensor([  4.8930, -14.6175])\n",
      "\tGrad:tensor([-0.0808,  0.4573])\n",
      "Epoch 1104,Loss 3.560185\n",
      "\tParams: tensor([  4.8938, -14.6220])\n",
      "\tGrad:tensor([-0.0806,  0.4565])\n",
      "Epoch 1105,Loss 3.558040\n",
      "\tParams: tensor([  4.8946, -14.6266])\n",
      "\tGrad:tensor([-0.0805,  0.4557])\n",
      "Epoch 1106,Loss 3.555901\n",
      "\tParams: tensor([  4.8954, -14.6311])\n",
      "\tGrad:tensor([-0.0804,  0.4550])\n",
      "Epoch 1107,Loss 3.553767\n",
      "\tParams: tensor([  4.8962, -14.6357])\n",
      "\tGrad:tensor([-0.0802,  0.4542])\n",
      "Epoch 1108,Loss 3.551641\n",
      "\tParams: tensor([  4.8970, -14.6402])\n",
      "\tGrad:tensor([-0.0801,  0.4534])\n",
      "Epoch 1109,Loss 3.549524\n",
      "\tParams: tensor([  4.8978, -14.6447])\n",
      "\tGrad:tensor([-0.0799,  0.4527])\n",
      "Epoch 1110,Loss 3.547411\n",
      "\tParams: tensor([  4.8986, -14.6493])\n",
      "\tGrad:tensor([-0.0798,  0.4519])\n",
      "Epoch 1111,Loss 3.545309\n",
      "\tParams: tensor([  4.8994, -14.6538])\n",
      "\tGrad:tensor([-0.0797,  0.4511])\n",
      "Epoch 1112,Loss 3.543211\n",
      "\tParams: tensor([  4.9002, -14.6583])\n",
      "\tGrad:tensor([-0.0796,  0.4503])\n",
      "Epoch 1113,Loss 3.541124\n",
      "\tParams: tensor([  4.9010, -14.6628])\n",
      "\tGrad:tensor([-0.0794,  0.4496])\n",
      "Epoch 1114,Loss 3.539041\n",
      "\tParams: tensor([  4.9018, -14.6673])\n",
      "\tGrad:tensor([-0.0793,  0.4488])\n",
      "Epoch 1115,Loss 3.536967\n",
      "\tParams: tensor([  4.9026, -14.6717])\n",
      "\tGrad:tensor([-0.0791,  0.4481])\n",
      "Epoch 1116,Loss 3.534896\n",
      "\tParams: tensor([  4.9034, -14.6762])\n",
      "\tGrad:tensor([-0.0790,  0.4473])\n",
      "Epoch 1117,Loss 3.532835\n",
      "\tParams: tensor([  4.9042, -14.6807])\n",
      "\tGrad:tensor([-0.0789,  0.4465])\n",
      "Epoch 1118,Loss 3.530781\n",
      "\tParams: tensor([  4.9049, -14.6851])\n",
      "\tGrad:tensor([-0.0787,  0.4458])\n",
      "Epoch 1119,Loss 3.528734\n",
      "\tParams: tensor([  4.9057, -14.6896])\n",
      "\tGrad:tensor([-0.0786,  0.4450])\n",
      "Epoch 1120,Loss 3.526694\n",
      "\tParams: tensor([  4.9065, -14.6940])\n",
      "\tGrad:tensor([-0.0785,  0.4443])\n",
      "Epoch 1121,Loss 3.524662\n",
      "\tParams: tensor([  4.9073, -14.6985])\n",
      "\tGrad:tensor([-0.0784,  0.4435])\n",
      "Epoch 1122,Loss 3.522633\n",
      "\tParams: tensor([  4.9081, -14.7029])\n",
      "\tGrad:tensor([-0.0782,  0.4428])\n",
      "Epoch 1123,Loss 3.520614\n",
      "\tParams: tensor([  4.9089, -14.7073])\n",
      "\tGrad:tensor([-0.0781,  0.4420])\n",
      "Epoch 1124,Loss 3.518601\n",
      "\tParams: tensor([  4.9096, -14.7117])\n",
      "\tGrad:tensor([-0.0779,  0.4413])\n",
      "Epoch 1125,Loss 3.516594\n",
      "\tParams: tensor([  4.9104, -14.7161])\n",
      "\tGrad:tensor([-0.0778,  0.4405])\n",
      "Epoch 1126,Loss 3.514594\n",
      "\tParams: tensor([  4.9112, -14.7205])\n",
      "\tGrad:tensor([-0.0777,  0.4398])\n",
      "Epoch 1127,Loss 3.512602\n",
      "\tParams: tensor([  4.9120, -14.7249])\n",
      "\tGrad:tensor([-0.0775,  0.4390])\n",
      "Epoch 1128,Loss 3.510619\n",
      "\tParams: tensor([  4.9128, -14.7293])\n",
      "\tGrad:tensor([-0.0774,  0.4383])\n",
      "Epoch 1129,Loss 3.508637\n",
      "\tParams: tensor([  4.9135, -14.7337])\n",
      "\tGrad:tensor([-0.0773,  0.4375])\n",
      "Epoch 1130,Loss 3.506665\n",
      "\tParams: tensor([  4.9143, -14.7380])\n",
      "\tGrad:tensor([-0.0772,  0.4368])\n",
      "Epoch 1131,Loss 3.504699\n",
      "\tParams: tensor([  4.9151, -14.7424])\n",
      "\tGrad:tensor([-0.0770,  0.4360])\n",
      "Epoch 1132,Loss 3.502741\n",
      "\tParams: tensor([  4.9158, -14.7467])\n",
      "\tGrad:tensor([-0.0769,  0.4353])\n",
      "Epoch 1133,Loss 3.500789\n",
      "\tParams: tensor([  4.9166, -14.7511])\n",
      "\tGrad:tensor([-0.0767,  0.4346])\n",
      "Epoch 1134,Loss 3.498843\n",
      "\tParams: tensor([  4.9174, -14.7554])\n",
      "\tGrad:tensor([-0.0766,  0.4338])\n",
      "Epoch 1135,Loss 3.496905\n",
      "\tParams: tensor([  4.9181, -14.7598])\n",
      "\tGrad:tensor([-0.0765,  0.4331])\n",
      "Epoch 1136,Loss 3.494972\n",
      "\tParams: tensor([  4.9189, -14.7641])\n",
      "\tGrad:tensor([-0.0764,  0.4323])\n",
      "Epoch 1137,Loss 3.493046\n",
      "\tParams: tensor([  4.9197, -14.7684])\n",
      "\tGrad:tensor([-0.0763,  0.4316])\n",
      "Epoch 1138,Loss 3.491127\n",
      "\tParams: tensor([  4.9204, -14.7727])\n",
      "\tGrad:tensor([-0.0761,  0.4309])\n",
      "Epoch 1139,Loss 3.489214\n",
      "\tParams: tensor([  4.9212, -14.7770])\n",
      "\tGrad:tensor([-0.0760,  0.4301])\n",
      "Epoch 1140,Loss 3.487308\n",
      "\tParams: tensor([  4.9219, -14.7813])\n",
      "\tGrad:tensor([-0.0759,  0.4294])\n",
      "Epoch 1141,Loss 3.485410\n",
      "\tParams: tensor([  4.9227, -14.7856])\n",
      "\tGrad:tensor([-0.0757,  0.4287])\n",
      "Epoch 1142,Loss 3.483515\n",
      "\tParams: tensor([  4.9235, -14.7899])\n",
      "\tGrad:tensor([-0.0756,  0.4280])\n",
      "Epoch 1143,Loss 3.481627\n",
      "\tParams: tensor([  4.9242, -14.7941])\n",
      "\tGrad:tensor([-0.0755,  0.4272])\n",
      "Epoch 1144,Loss 3.479746\n",
      "\tParams: tensor([  4.9250, -14.7984])\n",
      "\tGrad:tensor([-0.0753,  0.4265])\n",
      "Epoch 1145,Loss 3.477872\n",
      "\tParams: tensor([  4.9257, -14.8027])\n",
      "\tGrad:tensor([-0.0752,  0.4258])\n",
      "Epoch 1146,Loss 3.476005\n",
      "\tParams: tensor([  4.9265, -14.8069])\n",
      "\tGrad:tensor([-0.0751,  0.4250])\n",
      "Epoch 1147,Loss 3.474143\n",
      "\tParams: tensor([  4.9272, -14.8112])\n",
      "\tGrad:tensor([-0.0750,  0.4243])\n",
      "Epoch 1148,Loss 3.472288\n",
      "\tParams: tensor([  4.9280, -14.8154])\n",
      "\tGrad:tensor([-0.0748,  0.4236])\n",
      "Epoch 1149,Loss 3.470441\n",
      "\tParams: tensor([  4.9287, -14.8196])\n",
      "\tGrad:tensor([-0.0747,  0.4229])\n",
      "Epoch 1150,Loss 3.468597\n",
      "\tParams: tensor([  4.9295, -14.8238])\n",
      "\tGrad:tensor([-0.0746,  0.4222])\n",
      "Epoch 1151,Loss 3.466762\n",
      "\tParams: tensor([  4.9302, -14.8281])\n",
      "\tGrad:tensor([-0.0745,  0.4215])\n",
      "Epoch 1152,Loss 3.464930\n",
      "\tParams: tensor([  4.9309, -14.8323])\n",
      "\tGrad:tensor([-0.0743,  0.4207])\n",
      "Epoch 1153,Loss 3.463105\n",
      "\tParams: tensor([  4.9317, -14.8365])\n",
      "\tGrad:tensor([-0.0742,  0.4200])\n",
      "Epoch 1154,Loss 3.461290\n",
      "\tParams: tensor([  4.9324, -14.8407])\n",
      "\tGrad:tensor([-0.0741,  0.4193])\n",
      "Epoch 1155,Loss 3.459477\n",
      "\tParams: tensor([  4.9332, -14.8448])\n",
      "\tGrad:tensor([-0.0739,  0.4186])\n",
      "Epoch 1156,Loss 3.457671\n",
      "\tParams: tensor([  4.9339, -14.8490])\n",
      "\tGrad:tensor([-0.0738,  0.4179])\n",
      "Epoch 1157,Loss 3.455873\n",
      "\tParams: tensor([  4.9346, -14.8532])\n",
      "\tGrad:tensor([-0.0737,  0.4172])\n",
      "Epoch 1158,Loss 3.454080\n",
      "\tParams: tensor([  4.9354, -14.8574])\n",
      "\tGrad:tensor([-0.0736,  0.4165])\n",
      "Epoch 1159,Loss 3.452293\n",
      "\tParams: tensor([  4.9361, -14.8615])\n",
      "\tGrad:tensor([-0.0734,  0.4158])\n",
      "Epoch 1160,Loss 3.450513\n",
      "\tParams: tensor([  4.9368, -14.8657])\n",
      "\tGrad:tensor([-0.0733,  0.4151])\n",
      "Epoch 1161,Loss 3.448736\n",
      "\tParams: tensor([  4.9376, -14.8698])\n",
      "\tGrad:tensor([-0.0732,  0.4143])\n",
      "Epoch 1162,Loss 3.446968\n",
      "\tParams: tensor([  4.9383, -14.8739])\n",
      "\tGrad:tensor([-0.0731,  0.4136])\n",
      "Epoch 1163,Loss 3.445203\n",
      "\tParams: tensor([  4.9390, -14.8781])\n",
      "\tGrad:tensor([-0.0730,  0.4129])\n",
      "Epoch 1164,Loss 3.443449\n",
      "\tParams: tensor([  4.9398, -14.8822])\n",
      "\tGrad:tensor([-0.0728,  0.4122])\n",
      "Epoch 1165,Loss 3.441697\n",
      "\tParams: tensor([  4.9405, -14.8863])\n",
      "\tGrad:tensor([-0.0727,  0.4115])\n",
      "Epoch 1166,Loss 3.439952\n",
      "\tParams: tensor([  4.9412, -14.8904])\n",
      "\tGrad:tensor([-0.0726,  0.4108])\n",
      "Epoch 1167,Loss 3.438210\n",
      "\tParams: tensor([  4.9419, -14.8945])\n",
      "\tGrad:tensor([-0.0725,  0.4101])\n",
      "Epoch 1168,Loss 3.436479\n",
      "\tParams: tensor([  4.9427, -14.8986])\n",
      "\tGrad:tensor([-0.0723,  0.4094])\n",
      "Epoch 1169,Loss 3.434753\n",
      "\tParams: tensor([  4.9434, -14.9027])\n",
      "\tGrad:tensor([-0.0722,  0.4087])\n",
      "Epoch 1170,Loss 3.433030\n",
      "\tParams: tensor([  4.9441, -14.9068])\n",
      "\tGrad:tensor([-0.0721,  0.4081])\n",
      "Epoch 1171,Loss 3.431314\n",
      "\tParams: tensor([  4.9448, -14.9109])\n",
      "\tGrad:tensor([-0.0720,  0.4074])\n",
      "Epoch 1172,Loss 3.429607\n",
      "\tParams: tensor([  4.9455, -14.9149])\n",
      "\tGrad:tensor([-0.0719,  0.4067])\n",
      "Epoch 1173,Loss 3.427903\n",
      "\tParams: tensor([  4.9463, -14.9190])\n",
      "\tGrad:tensor([-0.0717,  0.4060])\n",
      "Epoch 1174,Loss 3.426204\n",
      "\tParams: tensor([  4.9470, -14.9230])\n",
      "\tGrad:tensor([-0.0716,  0.4053])\n",
      "Epoch 1175,Loss 3.424510\n",
      "\tParams: tensor([  4.9477, -14.9271])\n",
      "\tGrad:tensor([-0.0715,  0.4046])\n",
      "Epoch 1176,Loss 3.422823\n",
      "\tParams: tensor([  4.9484, -14.9311])\n",
      "\tGrad:tensor([-0.0714,  0.4039])\n",
      "Epoch 1177,Loss 3.421144\n",
      "\tParams: tensor([  4.9491, -14.9352])\n",
      "\tGrad:tensor([-0.0712,  0.4032])\n",
      "Epoch 1178,Loss 3.419468\n",
      "\tParams: tensor([  4.9498, -14.9392])\n",
      "\tGrad:tensor([-0.0711,  0.4025])\n",
      "Epoch 1179,Loss 3.417798\n",
      "\tParams: tensor([  4.9505, -14.9432])\n",
      "\tGrad:tensor([-0.0710,  0.4019])\n",
      "Epoch 1180,Loss 3.416134\n",
      "\tParams: tensor([  4.9512, -14.9472])\n",
      "\tGrad:tensor([-0.0709,  0.4012])\n",
      "Epoch 1181,Loss 3.414476\n",
      "\tParams: tensor([  4.9520, -14.9512])\n",
      "\tGrad:tensor([-0.0708,  0.4005])\n",
      "Epoch 1182,Loss 3.412824\n",
      "\tParams: tensor([  4.9527, -14.9552])\n",
      "\tGrad:tensor([-0.0706,  0.3998])\n",
      "Epoch 1183,Loss 3.411176\n",
      "\tParams: tensor([  4.9534, -14.9592])\n",
      "\tGrad:tensor([-0.0705,  0.3991])\n",
      "Epoch 1184,Loss 3.409534\n",
      "\tParams: tensor([  4.9541, -14.9632])\n",
      "\tGrad:tensor([-0.0704,  0.3985])\n",
      "Epoch 1185,Loss 3.407900\n",
      "\tParams: tensor([  4.9548, -14.9672])\n",
      "\tGrad:tensor([-0.0703,  0.3978])\n",
      "Epoch 1186,Loss 3.406271\n",
      "\tParams: tensor([  4.9555, -14.9711])\n",
      "\tGrad:tensor([-0.0701,  0.3971])\n",
      "Epoch 1187,Loss 3.404645\n",
      "\tParams: tensor([  4.9562, -14.9751])\n",
      "\tGrad:tensor([-0.0700,  0.3964])\n",
      "Epoch 1188,Loss 3.403024\n",
      "\tParams: tensor([  4.9569, -14.9791])\n",
      "\tGrad:tensor([-0.0699,  0.3958])\n",
      "Epoch 1189,Loss 3.401413\n",
      "\tParams: tensor([  4.9576, -14.9830])\n",
      "\tGrad:tensor([-0.0698,  0.3951])\n",
      "Epoch 1190,Loss 3.399802\n",
      "\tParams: tensor([  4.9583, -14.9870])\n",
      "\tGrad:tensor([-0.0697,  0.3944])\n",
      "Epoch 1191,Loss 3.398200\n",
      "\tParams: tensor([  4.9590, -14.9909])\n",
      "\tGrad:tensor([-0.0696,  0.3937])\n",
      "Epoch 1192,Loss 3.396602\n",
      "\tParams: tensor([  4.9597, -14.9948])\n",
      "\tGrad:tensor([-0.0694,  0.3931])\n",
      "Epoch 1193,Loss 3.395011\n",
      "\tParams: tensor([  4.9604, -14.9988])\n",
      "\tGrad:tensor([-0.0693,  0.3924])\n",
      "Epoch 1194,Loss 3.393425\n",
      "\tParams: tensor([  4.9610, -15.0027])\n",
      "\tGrad:tensor([-0.0692,  0.3917])\n",
      "Epoch 1195,Loss 3.391844\n",
      "\tParams: tensor([  4.9617, -15.0066])\n",
      "\tGrad:tensor([-0.0691,  0.3911])\n",
      "Epoch 1196,Loss 3.390266\n",
      "\tParams: tensor([  4.9624, -15.0105])\n",
      "\tGrad:tensor([-0.0690,  0.3904])\n",
      "Epoch 1197,Loss 3.388697\n",
      "\tParams: tensor([  4.9631, -15.0144])\n",
      "\tGrad:tensor([-0.0689,  0.3897])\n",
      "Epoch 1198,Loss 3.387131\n",
      "\tParams: tensor([  4.9638, -15.0183])\n",
      "\tGrad:tensor([-0.0687,  0.3891])\n",
      "Epoch 1199,Loss 3.385571\n",
      "\tParams: tensor([  4.9645, -15.0222])\n",
      "\tGrad:tensor([-0.0686,  0.3884])\n",
      "Epoch 1200,Loss 3.384018\n",
      "\tParams: tensor([  4.9652, -15.0260])\n",
      "\tGrad:tensor([-0.0685,  0.3878])\n",
      "Epoch 1201,Loss 3.382467\n",
      "\tParams: tensor([  4.9659, -15.0299])\n",
      "\tGrad:tensor([-0.0684,  0.3871])\n",
      "Epoch 1202,Loss 3.380925\n",
      "\tParams: tensor([  4.9665, -15.0338])\n",
      "\tGrad:tensor([-0.0683,  0.3864])\n",
      "Epoch 1203,Loss 3.379385\n",
      "\tParams: tensor([  4.9672, -15.0376])\n",
      "\tGrad:tensor([-0.0681,  0.3858])\n",
      "Epoch 1204,Loss 3.377851\n",
      "\tParams: tensor([  4.9679, -15.0415])\n",
      "\tGrad:tensor([-0.0680,  0.3851])\n",
      "Epoch 1205,Loss 3.376323\n",
      "\tParams: tensor([  4.9686, -15.0453])\n",
      "\tGrad:tensor([-0.0679,  0.3845])\n",
      "Epoch 1206,Loss 3.374800\n",
      "\tParams: tensor([  4.9693, -15.0492])\n",
      "\tGrad:tensor([-0.0678,  0.3838])\n",
      "Epoch 1207,Loss 3.373284\n",
      "\tParams: tensor([  4.9699, -15.0530])\n",
      "\tGrad:tensor([-0.0677,  0.3832])\n",
      "Epoch 1208,Loss 3.371769\n",
      "\tParams: tensor([  4.9706, -15.0568])\n",
      "\tGrad:tensor([-0.0676,  0.3825])\n",
      "Epoch 1209,Loss 3.370261\n",
      "\tParams: tensor([  4.9713, -15.0606])\n",
      "\tGrad:tensor([-0.0675,  0.3819])\n",
      "Epoch 1210,Loss 3.368760\n",
      "\tParams: tensor([  4.9720, -15.0645])\n",
      "\tGrad:tensor([-0.0673,  0.3812])\n",
      "Epoch 1211,Loss 3.367262\n",
      "\tParams: tensor([  4.9726, -15.0683])\n",
      "\tGrad:tensor([-0.0672,  0.3806])\n",
      "Epoch 1212,Loss 3.365771\n",
      "\tParams: tensor([  4.9733, -15.0721])\n",
      "\tGrad:tensor([-0.0671,  0.3799])\n",
      "Epoch 1213,Loss 3.364282\n",
      "\tParams: tensor([  4.9740, -15.0758])\n",
      "\tGrad:tensor([-0.0670,  0.3793])\n",
      "Epoch 1214,Loss 3.362800\n",
      "\tParams: tensor([  4.9746, -15.0796])\n",
      "\tGrad:tensor([-0.0669,  0.3786])\n",
      "Epoch 1215,Loss 3.361324\n",
      "\tParams: tensor([  4.9753, -15.0834])\n",
      "\tGrad:tensor([-0.0668,  0.3780])\n",
      "Epoch 1216,Loss 3.359850\n",
      "\tParams: tensor([  4.9760, -15.0872])\n",
      "\tGrad:tensor([-0.0667,  0.3774])\n",
      "Epoch 1217,Loss 3.358384\n",
      "\tParams: tensor([  4.9766, -15.0910])\n",
      "\tGrad:tensor([-0.0665,  0.3767])\n",
      "Epoch 1218,Loss 3.356921\n",
      "\tParams: tensor([  4.9773, -15.0947])\n",
      "\tGrad:tensor([-0.0664,  0.3761])\n",
      "Epoch 1219,Loss 3.355464\n",
      "\tParams: tensor([  4.9780, -15.0985])\n",
      "\tGrad:tensor([-0.0663,  0.3754])\n",
      "Epoch 1220,Loss 3.354012\n",
      "\tParams: tensor([  4.9786, -15.1022])\n",
      "\tGrad:tensor([-0.0662,  0.3748])\n",
      "Epoch 1221,Loss 3.352564\n",
      "\tParams: tensor([  4.9793, -15.1060])\n",
      "\tGrad:tensor([-0.0661,  0.3742])\n",
      "Epoch 1222,Loss 3.351122\n",
      "\tParams: tensor([  4.9799, -15.1097])\n",
      "\tGrad:tensor([-0.0660,  0.3735])\n",
      "Epoch 1223,Loss 3.349685\n",
      "\tParams: tensor([  4.9806, -15.1134])\n",
      "\tGrad:tensor([-0.0659,  0.3729])\n",
      "Epoch 1224,Loss 3.348251\n",
      "\tParams: tensor([  4.9813, -15.1171])\n",
      "\tGrad:tensor([-0.0657,  0.3723])\n",
      "Epoch 1225,Loss 3.346824\n",
      "\tParams: tensor([  4.9819, -15.1209])\n",
      "\tGrad:tensor([-0.0656,  0.3716])\n",
      "Epoch 1226,Loss 3.345403\n",
      "\tParams: tensor([  4.9826, -15.1246])\n",
      "\tGrad:tensor([-0.0655,  0.3710])\n",
      "Epoch 1227,Loss 3.343982\n",
      "\tParams: tensor([  4.9832, -15.1283])\n",
      "\tGrad:tensor([-0.0654,  0.3704])\n",
      "Epoch 1228,Loss 3.342571\n",
      "\tParams: tensor([  4.9839, -15.1320])\n",
      "\tGrad:tensor([-0.0653,  0.3697])\n",
      "Epoch 1229,Loss 3.341160\n",
      "\tParams: tensor([  4.9845, -15.1357])\n",
      "\tGrad:tensor([-0.0652,  0.3691])\n",
      "Epoch 1230,Loss 3.339758\n",
      "\tParams: tensor([  4.9852, -15.1393])\n",
      "\tGrad:tensor([-0.0651,  0.3685])\n",
      "Epoch 1231,Loss 3.338359\n",
      "\tParams: tensor([  4.9858, -15.1430])\n",
      "\tGrad:tensor([-0.0650,  0.3679])\n",
      "Epoch 1232,Loss 3.336965\n",
      "\tParams: tensor([  4.9865, -15.1467])\n",
      "\tGrad:tensor([-0.0649,  0.3672])\n",
      "Epoch 1233,Loss 3.335577\n",
      "\tParams: tensor([  4.9871, -15.1504])\n",
      "\tGrad:tensor([-0.0648,  0.3666])\n",
      "Epoch 1234,Loss 3.334192\n",
      "\tParams: tensor([  4.9878, -15.1540])\n",
      "\tGrad:tensor([-0.0646,  0.3660])\n",
      "Epoch 1235,Loss 3.332811\n",
      "\tParams: tensor([  4.9884, -15.1577])\n",
      "\tGrad:tensor([-0.0645,  0.3654])\n",
      "Epoch 1236,Loss 3.331436\n",
      "\tParams: tensor([  4.9891, -15.1613])\n",
      "\tGrad:tensor([-0.0644,  0.3647])\n",
      "Epoch 1237,Loss 3.330065\n",
      "\tParams: tensor([  4.9897, -15.1650])\n",
      "\tGrad:tensor([-0.0643,  0.3641])\n",
      "Epoch 1238,Loss 3.328699\n",
      "\tParams: tensor([  4.9904, -15.1686])\n",
      "\tGrad:tensor([-0.0642,  0.3635])\n",
      "Epoch 1239,Loss 3.327339\n",
      "\tParams: tensor([  4.9910, -15.1722])\n",
      "\tGrad:tensor([-0.0641,  0.3629])\n",
      "Epoch 1240,Loss 3.325980\n",
      "\tParams: tensor([  4.9916, -15.1759])\n",
      "\tGrad:tensor([-0.0640,  0.3623])\n",
      "Epoch 1241,Loss 3.324628\n",
      "\tParams: tensor([  4.9923, -15.1795])\n",
      "\tGrad:tensor([-0.0639,  0.3617])\n",
      "Epoch 1242,Loss 3.323279\n",
      "\tParams: tensor([  4.9929, -15.1831])\n",
      "\tGrad:tensor([-0.0638,  0.3610])\n",
      "Epoch 1243,Loss 3.321935\n",
      "\tParams: tensor([  4.9936, -15.1867])\n",
      "\tGrad:tensor([-0.0637,  0.3604])\n",
      "Epoch 1244,Loss 3.320600\n",
      "\tParams: tensor([  4.9942, -15.1903])\n",
      "\tGrad:tensor([-0.0636,  0.3598])\n",
      "Epoch 1245,Loss 3.319264\n",
      "\tParams: tensor([  4.9948, -15.1939])\n",
      "\tGrad:tensor([-0.0635,  0.3592])\n",
      "Epoch 1246,Loss 3.317935\n",
      "\tParams: tensor([  4.9955, -15.1975])\n",
      "\tGrad:tensor([-0.0633,  0.3586])\n",
      "Epoch 1247,Loss 3.316611\n",
      "\tParams: tensor([  4.9961, -15.2010])\n",
      "\tGrad:tensor([-0.0633,  0.3580])\n",
      "Epoch 1248,Loss 3.315289\n",
      "\tParams: tensor([  4.9967, -15.2046])\n",
      "\tGrad:tensor([-0.0631,  0.3574])\n",
      "Epoch 1249,Loss 3.313973\n",
      "\tParams: tensor([  4.9973, -15.2082])\n",
      "\tGrad:tensor([-0.0630,  0.3568])\n",
      "Epoch 1250,Loss 3.312663\n",
      "\tParams: tensor([  4.9980, -15.2117])\n",
      "\tGrad:tensor([-0.0629,  0.3562])\n",
      "Epoch 1251,Loss 3.311353\n",
      "\tParams: tensor([  4.9986, -15.2153])\n",
      "\tGrad:tensor([-0.0628,  0.3556])\n",
      "Epoch 1252,Loss 3.310053\n",
      "\tParams: tensor([  4.9992, -15.2189])\n",
      "\tGrad:tensor([-0.0627,  0.3550])\n",
      "Epoch 1253,Loss 3.308756\n",
      "\tParams: tensor([  4.9999, -15.2224])\n",
      "\tGrad:tensor([-0.0626,  0.3543])\n",
      "Epoch 1254,Loss 3.307463\n",
      "\tParams: tensor([  5.0005, -15.2259])\n",
      "\tGrad:tensor([-0.0625,  0.3537])\n",
      "Epoch 1255,Loss 3.306170\n",
      "\tParams: tensor([  5.0011, -15.2295])\n",
      "\tGrad:tensor([-0.0624,  0.3531])\n",
      "Epoch 1256,Loss 3.304887\n",
      "\tParams: tensor([  5.0017, -15.2330])\n",
      "\tGrad:tensor([-0.0623,  0.3525])\n",
      "Epoch 1257,Loss 3.303605\n",
      "\tParams: tensor([  5.0024, -15.2365])\n",
      "\tGrad:tensor([-0.0622,  0.3519])\n",
      "Epoch 1258,Loss 3.302329\n",
      "\tParams: tensor([  5.0030, -15.2400])\n",
      "\tGrad:tensor([-0.0620,  0.3514])\n",
      "Epoch 1259,Loss 3.301057\n",
      "\tParams: tensor([  5.0036, -15.2435])\n",
      "\tGrad:tensor([-0.0620,  0.3508])\n",
      "Epoch 1260,Loss 3.299791\n",
      "\tParams: tensor([  5.0042, -15.2470])\n",
      "\tGrad:tensor([-0.0619,  0.3502])\n",
      "Epoch 1261,Loss 3.298528\n",
      "\tParams: tensor([  5.0048, -15.2505])\n",
      "\tGrad:tensor([-0.0618,  0.3496])\n",
      "Epoch 1262,Loss 3.297267\n",
      "\tParams: tensor([  5.0054, -15.2540])\n",
      "\tGrad:tensor([-0.0616,  0.3490])\n",
      "Epoch 1263,Loss 3.296014\n",
      "\tParams: tensor([  5.0061, -15.2575])\n",
      "\tGrad:tensor([-0.0615,  0.3484])\n",
      "Epoch 1264,Loss 3.294762\n",
      "\tParams: tensor([  5.0067, -15.2610])\n",
      "\tGrad:tensor([-0.0614,  0.3478])\n",
      "Epoch 1265,Loss 3.293517\n",
      "\tParams: tensor([  5.0073, -15.2645])\n",
      "\tGrad:tensor([-0.0613,  0.3472])\n",
      "Epoch 1266,Loss 3.292276\n",
      "\tParams: tensor([  5.0079, -15.2679])\n",
      "\tGrad:tensor([-0.0612,  0.3466])\n",
      "Epoch 1267,Loss 3.291036\n",
      "\tParams: tensor([  5.0085, -15.2714])\n",
      "\tGrad:tensor([-0.0611,  0.3460])\n",
      "Epoch 1268,Loss 3.289804\n",
      "\tParams: tensor([  5.0091, -15.2748])\n",
      "\tGrad:tensor([-0.0610,  0.3454])\n",
      "Epoch 1269,Loss 3.288573\n",
      "\tParams: tensor([  5.0097, -15.2783])\n",
      "\tGrad:tensor([-0.0609,  0.3448])\n",
      "Epoch 1270,Loss 3.287347\n",
      "\tParams: tensor([  5.0103, -15.2817])\n",
      "\tGrad:tensor([-0.0608,  0.3443])\n",
      "Epoch 1271,Loss 3.286129\n",
      "\tParams: tensor([  5.0109, -15.2852])\n",
      "\tGrad:tensor([-0.0607,  0.3437])\n",
      "Epoch 1272,Loss 3.284911\n",
      "\tParams: tensor([  5.0116, -15.2886])\n",
      "\tGrad:tensor([-0.0606,  0.3431])\n",
      "Epoch 1273,Loss 3.283698\n",
      "\tParams: tensor([  5.0122, -15.2920])\n",
      "\tGrad:tensor([-0.0605,  0.3425])\n",
      "Epoch 1274,Loss 3.282488\n",
      "\tParams: tensor([  5.0128, -15.2954])\n",
      "\tGrad:tensor([-0.0604,  0.3419])\n",
      "Epoch 1275,Loss 3.281284\n",
      "\tParams: tensor([  5.0134, -15.2988])\n",
      "\tGrad:tensor([-0.0603,  0.3413])\n",
      "Epoch 1276,Loss 3.280085\n",
      "\tParams: tensor([  5.0140, -15.3023])\n",
      "\tGrad:tensor([-0.0602,  0.3408])\n",
      "Epoch 1277,Loss 3.278888\n",
      "\tParams: tensor([  5.0146, -15.3057])\n",
      "\tGrad:tensor([-0.0601,  0.3402])\n",
      "Epoch 1278,Loss 3.277696\n",
      "\tParams: tensor([  5.0152, -15.3091])\n",
      "\tGrad:tensor([-0.0600,  0.3396])\n",
      "Epoch 1279,Loss 3.276506\n",
      "\tParams: tensor([  5.0158, -15.3124])\n",
      "\tGrad:tensor([-0.0599,  0.3390])\n",
      "Epoch 1280,Loss 3.275322\n",
      "\tParams: tensor([  5.0164, -15.3158])\n",
      "\tGrad:tensor([-0.0598,  0.3384])\n",
      "Epoch 1281,Loss 3.274142\n",
      "\tParams: tensor([  5.0170, -15.3192])\n",
      "\tGrad:tensor([-0.0597,  0.3379])\n",
      "Epoch 1282,Loss 3.272968\n",
      "\tParams: tensor([  5.0176, -15.3226])\n",
      "\tGrad:tensor([-0.0596,  0.3373])\n",
      "Epoch 1283,Loss 3.271793\n",
      "\tParams: tensor([  5.0182, -15.3259])\n",
      "\tGrad:tensor([-0.0595,  0.3367])\n",
      "Epoch 1284,Loss 3.270625\n",
      "\tParams: tensor([  5.0187, -15.3293])\n",
      "\tGrad:tensor([-0.0594,  0.3362])\n",
      "Epoch 1285,Loss 3.269460\n",
      "\tParams: tensor([  5.0193, -15.3327])\n",
      "\tGrad:tensor([-0.0593,  0.3356])\n",
      "Epoch 1286,Loss 3.268301\n",
      "\tParams: tensor([  5.0199, -15.3360])\n",
      "\tGrad:tensor([-0.0592,  0.3350])\n",
      "Epoch 1287,Loss 3.267143\n",
      "\tParams: tensor([  5.0205, -15.3394])\n",
      "\tGrad:tensor([-0.0591,  0.3344])\n",
      "Epoch 1288,Loss 3.265991\n",
      "\tParams: tensor([  5.0211, -15.3427])\n",
      "\tGrad:tensor([-0.0590,  0.3339])\n",
      "Epoch 1289,Loss 3.264842\n",
      "\tParams: tensor([  5.0217, -15.3460])\n",
      "\tGrad:tensor([-0.0589,  0.3333])\n",
      "Epoch 1290,Loss 3.263700\n",
      "\tParams: tensor([  5.0223, -15.3494])\n",
      "\tGrad:tensor([-0.0588,  0.3327])\n",
      "Epoch 1291,Loss 3.262556\n",
      "\tParams: tensor([  5.0229, -15.3527])\n",
      "\tGrad:tensor([-0.0587,  0.3322])\n",
      "Epoch 1292,Loss 3.261421\n",
      "\tParams: tensor([  5.0235, -15.3560])\n",
      "\tGrad:tensor([-0.0586,  0.3316])\n",
      "Epoch 1293,Loss 3.260287\n",
      "\tParams: tensor([  5.0240, -15.3593])\n",
      "\tGrad:tensor([-0.0585,  0.3311])\n",
      "Epoch 1294,Loss 3.259160\n",
      "\tParams: tensor([  5.0246, -15.3626])\n",
      "\tGrad:tensor([-0.0584,  0.3305])\n",
      "Epoch 1295,Loss 3.258033\n",
      "\tParams: tensor([  5.0252, -15.3659])\n",
      "\tGrad:tensor([-0.0583,  0.3299])\n",
      "Epoch 1296,Loss 3.256912\n",
      "\tParams: tensor([  5.0258, -15.3692])\n",
      "\tGrad:tensor([-0.0582,  0.3294])\n",
      "Epoch 1297,Loss 3.255795\n",
      "\tParams: tensor([  5.0264, -15.3725])\n",
      "\tGrad:tensor([-0.0581,  0.3288])\n",
      "Epoch 1298,Loss 3.254681\n",
      "\tParams: tensor([  5.0270, -15.3758])\n",
      "\tGrad:tensor([-0.0580,  0.3282])\n",
      "Epoch 1299,Loss 3.253569\n",
      "\tParams: tensor([  5.0275, -15.3791])\n",
      "\tGrad:tensor([-0.0579,  0.3277])\n",
      "Epoch 1300,Loss 3.252462\n",
      "\tParams: tensor([  5.0281, -15.3823])\n",
      "\tGrad:tensor([-0.0578,  0.3271])\n",
      "Epoch 1301,Loss 3.251362\n",
      "\tParams: tensor([  5.0287, -15.3856])\n",
      "\tGrad:tensor([-0.0577,  0.3266])\n",
      "Epoch 1302,Loss 3.250263\n",
      "\tParams: tensor([  5.0293, -15.3888])\n",
      "\tGrad:tensor([-0.0576,  0.3260])\n",
      "Epoch 1303,Loss 3.249168\n",
      "\tParams: tensor([  5.0298, -15.3921])\n",
      "\tGrad:tensor([-0.0575,  0.3255])\n",
      "Epoch 1304,Loss 3.248077\n",
      "\tParams: tensor([  5.0304, -15.3954])\n",
      "\tGrad:tensor([-0.0574,  0.3249])\n",
      "Epoch 1305,Loss 3.246988\n",
      "\tParams: tensor([  5.0310, -15.3986])\n",
      "\tGrad:tensor([-0.0573,  0.3244])\n",
      "Epoch 1306,Loss 3.245904\n",
      "\tParams: tensor([  5.0316, -15.4018])\n",
      "\tGrad:tensor([-0.0572,  0.3238])\n",
      "Epoch 1307,Loss 3.244824\n",
      "\tParams: tensor([  5.0321, -15.4051])\n",
      "\tGrad:tensor([-0.0571,  0.3233])\n",
      "Epoch 1308,Loss 3.243747\n",
      "\tParams: tensor([  5.0327, -15.4083])\n",
      "\tGrad:tensor([-0.0570,  0.3227])\n",
      "Epoch 1309,Loss 3.242674\n",
      "\tParams: tensor([  5.0333, -15.4115])\n",
      "\tGrad:tensor([-0.0569,  0.3222])\n",
      "Epoch 1310,Loss 3.241606\n",
      "\tParams: tensor([  5.0338, -15.4147])\n",
      "\tGrad:tensor([-0.0568,  0.3216])\n",
      "Epoch 1311,Loss 3.240538\n",
      "\tParams: tensor([  5.0344, -15.4179])\n",
      "\tGrad:tensor([-0.0567,  0.3211])\n",
      "Epoch 1312,Loss 3.239475\n",
      "\tParams: tensor([  5.0350, -15.4211])\n",
      "\tGrad:tensor([-0.0566,  0.3205])\n",
      "Epoch 1313,Loss 3.238419\n",
      "\tParams: tensor([  5.0355, -15.4243])\n",
      "\tGrad:tensor([-0.0565,  0.3200])\n",
      "Epoch 1314,Loss 3.237363\n",
      "\tParams: tensor([  5.0361, -15.4275])\n",
      "\tGrad:tensor([-0.0564,  0.3194])\n",
      "Epoch 1315,Loss 3.236314\n",
      "\tParams: tensor([  5.0367, -15.4307])\n",
      "\tGrad:tensor([-0.0563,  0.3189])\n",
      "Epoch 1316,Loss 3.235265\n",
      "\tParams: tensor([  5.0372, -15.4339])\n",
      "\tGrad:tensor([-0.0562,  0.3184])\n",
      "Epoch 1317,Loss 3.234218\n",
      "\tParams: tensor([  5.0378, -15.4371])\n",
      "\tGrad:tensor([-0.0561,  0.3178])\n",
      "Epoch 1318,Loss 3.233179\n",
      "\tParams: tensor([  5.0383, -15.4403])\n",
      "\tGrad:tensor([-0.0561,  0.3173])\n",
      "Epoch 1319,Loss 3.232143\n",
      "\tParams: tensor([  5.0389, -15.4434])\n",
      "\tGrad:tensor([-0.0560,  0.3167])\n",
      "Epoch 1320,Loss 3.231109\n",
      "\tParams: tensor([  5.0395, -15.4466])\n",
      "\tGrad:tensor([-0.0558,  0.3162])\n",
      "Epoch 1321,Loss 3.230078\n",
      "\tParams: tensor([  5.0400, -15.4498])\n",
      "\tGrad:tensor([-0.0558,  0.3157])\n",
      "Epoch 1322,Loss 3.229051\n",
      "\tParams: tensor([  5.0406, -15.4529])\n",
      "\tGrad:tensor([-0.0557,  0.3151])\n",
      "Epoch 1323,Loss 3.228027\n",
      "\tParams: tensor([  5.0411, -15.4560])\n",
      "\tGrad:tensor([-0.0556,  0.3146])\n",
      "Epoch 1324,Loss 3.227010\n",
      "\tParams: tensor([  5.0417, -15.4592])\n",
      "\tGrad:tensor([-0.0555,  0.3141])\n",
      "Epoch 1325,Loss 3.225992\n",
      "\tParams: tensor([  5.0422, -15.4623])\n",
      "\tGrad:tensor([-0.0554,  0.3135])\n",
      "Epoch 1326,Loss 3.224979\n",
      "\tParams: tensor([  5.0428, -15.4655])\n",
      "\tGrad:tensor([-0.0553,  0.3130])\n",
      "Epoch 1327,Loss 3.223971\n",
      "\tParams: tensor([  5.0433, -15.4686])\n",
      "\tGrad:tensor([-0.0552,  0.3125])\n",
      "Epoch 1328,Loss 3.222965\n",
      "\tParams: tensor([  5.0439, -15.4717])\n",
      "\tGrad:tensor([-0.0551,  0.3119])\n",
      "Epoch 1329,Loss 3.221960\n",
      "\tParams: tensor([  5.0444, -15.4748])\n",
      "\tGrad:tensor([-0.0550,  0.3114])\n",
      "Epoch 1330,Loss 3.220962\n",
      "\tParams: tensor([  5.0450, -15.4779])\n",
      "\tGrad:tensor([-0.0549,  0.3109])\n",
      "Epoch 1331,Loss 3.219967\n",
      "\tParams: tensor([  5.0455, -15.4810])\n",
      "\tGrad:tensor([-0.0548,  0.3103])\n",
      "Epoch 1332,Loss 3.218975\n",
      "\tParams: tensor([  5.0461, -15.4841])\n",
      "\tGrad:tensor([-0.0547,  0.3098])\n",
      "Epoch 1333,Loss 3.217986\n",
      "\tParams: tensor([  5.0466, -15.4872])\n",
      "\tGrad:tensor([-0.0546,  0.3093])\n",
      "Epoch 1334,Loss 3.217000\n",
      "\tParams: tensor([  5.0472, -15.4903])\n",
      "\tGrad:tensor([-0.0545,  0.3088])\n",
      "Epoch 1335,Loss 3.216017\n",
      "\tParams: tensor([  5.0477, -15.4934])\n",
      "\tGrad:tensor([-0.0544,  0.3082])\n",
      "Epoch 1336,Loss 3.215038\n",
      "\tParams: tensor([  5.0483, -15.4965])\n",
      "\tGrad:tensor([-0.0543,  0.3077])\n",
      "Epoch 1337,Loss 3.214062\n",
      "\tParams: tensor([  5.0488, -15.4995])\n",
      "\tGrad:tensor([-0.0543,  0.3072])\n",
      "Epoch 1338,Loss 3.213092\n",
      "\tParams: tensor([  5.0494, -15.5026])\n",
      "\tGrad:tensor([-0.0542,  0.3067])\n",
      "Epoch 1339,Loss 3.212122\n",
      "\tParams: tensor([  5.0499, -15.5057])\n",
      "\tGrad:tensor([-0.0541,  0.3061])\n",
      "Epoch 1340,Loss 3.211157\n",
      "\tParams: tensor([  5.0504, -15.5087])\n",
      "\tGrad:tensor([-0.0540,  0.3056])\n",
      "Epoch 1341,Loss 3.210192\n",
      "\tParams: tensor([  5.0510, -15.5118])\n",
      "\tGrad:tensor([-0.0539,  0.3051])\n",
      "Epoch 1342,Loss 3.209235\n",
      "\tParams: tensor([  5.0515, -15.5148])\n",
      "\tGrad:tensor([-0.0538,  0.3046])\n",
      "Epoch 1343,Loss 3.208279\n",
      "\tParams: tensor([  5.0521, -15.5179])\n",
      "\tGrad:tensor([-0.0537,  0.3041])\n",
      "Epoch 1344,Loss 3.207326\n",
      "\tParams: tensor([  5.0526, -15.5209])\n",
      "\tGrad:tensor([-0.0536,  0.3036])\n",
      "Epoch 1345,Loss 3.206376\n",
      "\tParams: tensor([  5.0531, -15.5239])\n",
      "\tGrad:tensor([-0.0535,  0.3030])\n",
      "Epoch 1346,Loss 3.205430\n",
      "\tParams: tensor([  5.0537, -15.5269])\n",
      "\tGrad:tensor([-0.0534,  0.3025])\n",
      "Epoch 1347,Loss 3.204488\n",
      "\tParams: tensor([  5.0542, -15.5300])\n",
      "\tGrad:tensor([-0.0533,  0.3020])\n",
      "Epoch 1348,Loss 3.203547\n",
      "\tParams: tensor([  5.0547, -15.5330])\n",
      "\tGrad:tensor([-0.0532,  0.3015])\n",
      "Epoch 1349,Loss 3.202610\n",
      "\tParams: tensor([  5.0553, -15.5360])\n",
      "\tGrad:tensor([-0.0532,  0.3010])\n",
      "Epoch 1350,Loss 3.201678\n",
      "\tParams: tensor([  5.0558, -15.5390])\n",
      "\tGrad:tensor([-0.0531,  0.3005])\n",
      "Epoch 1351,Loss 3.200747\n",
      "\tParams: tensor([  5.0563, -15.5420])\n",
      "\tGrad:tensor([-0.0530,  0.3000])\n",
      "Epoch 1352,Loss 3.199820\n",
      "\tParams: tensor([  5.0568, -15.5450])\n",
      "\tGrad:tensor([-0.0529,  0.2995])\n",
      "Epoch 1353,Loss 3.198897\n",
      "\tParams: tensor([  5.0574, -15.5480])\n",
      "\tGrad:tensor([-0.0528,  0.2989])\n",
      "Epoch 1354,Loss 3.197976\n",
      "\tParams: tensor([  5.0579, -15.5510])\n",
      "\tGrad:tensor([-0.0527,  0.2984])\n",
      "Epoch 1355,Loss 3.197060\n",
      "\tParams: tensor([  5.0584, -15.5539])\n",
      "\tGrad:tensor([-0.0526,  0.2979])\n",
      "Epoch 1356,Loss 3.196143\n",
      "\tParams: tensor([  5.0590, -15.5569])\n",
      "\tGrad:tensor([-0.0525,  0.2974])\n",
      "Epoch 1357,Loss 3.195231\n",
      "\tParams: tensor([  5.0595, -15.5599])\n",
      "\tGrad:tensor([-0.0524,  0.2969])\n",
      "Epoch 1358,Loss 3.194324\n",
      "\tParams: tensor([  5.0600, -15.5629])\n",
      "\tGrad:tensor([-0.0524,  0.2964])\n",
      "Epoch 1359,Loss 3.193420\n",
      "\tParams: tensor([  5.0605, -15.5658])\n",
      "\tGrad:tensor([-0.0523,  0.2959])\n",
      "Epoch 1360,Loss 3.192517\n",
      "\tParams: tensor([  5.0610, -15.5688])\n",
      "\tGrad:tensor([-0.0522,  0.2954])\n",
      "Epoch 1361,Loss 3.191616\n",
      "\tParams: tensor([  5.0616, -15.5717])\n",
      "\tGrad:tensor([-0.0521,  0.2949])\n",
      "Epoch 1362,Loss 3.190720\n",
      "\tParams: tensor([  5.0621, -15.5747])\n",
      "\tGrad:tensor([-0.0520,  0.2944])\n",
      "Epoch 1363,Loss 3.189829\n",
      "\tParams: tensor([  5.0626, -15.5776])\n",
      "\tGrad:tensor([-0.0519,  0.2939])\n",
      "Epoch 1364,Loss 3.188938\n",
      "\tParams: tensor([  5.0631, -15.5805])\n",
      "\tGrad:tensor([-0.0518,  0.2934])\n",
      "Epoch 1365,Loss 3.188051\n",
      "\tParams: tensor([  5.0636, -15.5835])\n",
      "\tGrad:tensor([-0.0517,  0.2929])\n",
      "Epoch 1366,Loss 3.187166\n",
      "\tParams: tensor([  5.0642, -15.5864])\n",
      "\tGrad:tensor([-0.0516,  0.2924])\n",
      "Epoch 1367,Loss 3.186287\n",
      "\tParams: tensor([  5.0647, -15.5893])\n",
      "\tGrad:tensor([-0.0516,  0.2919])\n",
      "Epoch 1368,Loss 3.185409\n",
      "\tParams: tensor([  5.0652, -15.5922])\n",
      "\tGrad:tensor([-0.0515,  0.2914])\n",
      "Epoch 1369,Loss 3.184534\n",
      "\tParams: tensor([  5.0657, -15.5951])\n",
      "\tGrad:tensor([-0.0514,  0.2909])\n",
      "Epoch 1370,Loss 3.183662\n",
      "\tParams: tensor([  5.0662, -15.5980])\n",
      "\tGrad:tensor([-0.0513,  0.2904])\n",
      "Epoch 1371,Loss 3.182792\n",
      "\tParams: tensor([  5.0667, -15.6009])\n",
      "\tGrad:tensor([-0.0512,  0.2899])\n",
      "Epoch 1372,Loss 3.181925\n",
      "\tParams: tensor([  5.0672, -15.6038])\n",
      "\tGrad:tensor([-0.0511,  0.2894])\n",
      "Epoch 1373,Loss 3.181063\n",
      "\tParams: tensor([  5.0678, -15.6067])\n",
      "\tGrad:tensor([-0.0510,  0.2890])\n",
      "Epoch 1374,Loss 3.180201\n",
      "\tParams: tensor([  5.0683, -15.6096])\n",
      "\tGrad:tensor([-0.0509,  0.2885])\n",
      "Epoch 1375,Loss 3.179347\n",
      "\tParams: tensor([  5.0688, -15.6125])\n",
      "\tGrad:tensor([-0.0509,  0.2880])\n",
      "Epoch 1376,Loss 3.178490\n",
      "\tParams: tensor([  5.0693, -15.6154])\n",
      "\tGrad:tensor([-0.0508,  0.2875])\n",
      "Epoch 1377,Loss 3.177638\n",
      "\tParams: tensor([  5.0698, -15.6182])\n",
      "\tGrad:tensor([-0.0507,  0.2870])\n",
      "Epoch 1378,Loss 3.176789\n",
      "\tParams: tensor([  5.0703, -15.6211])\n",
      "\tGrad:tensor([-0.0506,  0.2865])\n",
      "Epoch 1379,Loss 3.175945\n",
      "\tParams: tensor([  5.0708, -15.6240])\n",
      "\tGrad:tensor([-0.0505,  0.2860])\n",
      "Epoch 1380,Loss 3.175101\n",
      "\tParams: tensor([  5.0713, -15.6268])\n",
      "\tGrad:tensor([-0.0504,  0.2855])\n",
      "Epoch 1381,Loss 3.174262\n",
      "\tParams: tensor([  5.0718, -15.6297])\n",
      "\tGrad:tensor([-0.0504,  0.2850])\n",
      "Epoch 1382,Loss 3.173425\n",
      "\tParams: tensor([  5.0723, -15.6325])\n",
      "\tGrad:tensor([-0.0503,  0.2846])\n",
      "Epoch 1383,Loss 3.172590\n",
      "\tParams: tensor([  5.0728, -15.6353])\n",
      "\tGrad:tensor([-0.0502,  0.2841])\n",
      "Epoch 1384,Loss 3.171759\n",
      "\tParams: tensor([  5.0733, -15.6382])\n",
      "\tGrad:tensor([-0.0501,  0.2836])\n",
      "Epoch 1385,Loss 3.170929\n",
      "\tParams: tensor([  5.0738, -15.6410])\n",
      "\tGrad:tensor([-0.0500,  0.2831])\n",
      "Epoch 1386,Loss 3.170103\n",
      "\tParams: tensor([  5.0743, -15.6438])\n",
      "\tGrad:tensor([-0.0499,  0.2826])\n",
      "Epoch 1387,Loss 3.169280\n",
      "\tParams: tensor([  5.0748, -15.6467])\n",
      "\tGrad:tensor([-0.0498,  0.2822])\n",
      "Epoch 1388,Loss 3.168462\n",
      "\tParams: tensor([  5.0753, -15.6495])\n",
      "\tGrad:tensor([-0.0498,  0.2817])\n",
      "Epoch 1389,Loss 3.167644\n",
      "\tParams: tensor([  5.0758, -15.6523])\n",
      "\tGrad:tensor([-0.0497,  0.2812])\n",
      "Epoch 1390,Loss 3.166827\n",
      "\tParams: tensor([  5.0763, -15.6551])\n",
      "\tGrad:tensor([-0.0496,  0.2807])\n",
      "Epoch 1391,Loss 3.166017\n",
      "\tParams: tensor([  5.0768, -15.6579])\n",
      "\tGrad:tensor([-0.0495,  0.2802])\n",
      "Epoch 1392,Loss 3.165207\n",
      "\tParams: tensor([  5.0773, -15.6607])\n",
      "\tGrad:tensor([-0.0494,  0.2798])\n",
      "Epoch 1393,Loss 3.164401\n",
      "\tParams: tensor([  5.0778, -15.6635])\n",
      "\tGrad:tensor([-0.0493,  0.2793])\n",
      "Epoch 1394,Loss 3.163594\n",
      "\tParams: tensor([  5.0783, -15.6663])\n",
      "\tGrad:tensor([-0.0492,  0.2788])\n",
      "Epoch 1395,Loss 3.162795\n",
      "\tParams: tensor([  5.0788, -15.6691])\n",
      "\tGrad:tensor([-0.0492,  0.2783])\n",
      "Epoch 1396,Loss 3.161996\n",
      "\tParams: tensor([  5.0793, -15.6718])\n",
      "\tGrad:tensor([-0.0491,  0.2779])\n",
      "Epoch 1397,Loss 3.161201\n",
      "\tParams: tensor([  5.0797, -15.6746])\n",
      "\tGrad:tensor([-0.0490,  0.2774])\n",
      "Epoch 1398,Loss 3.160410\n",
      "\tParams: tensor([  5.0802, -15.6774])\n",
      "\tGrad:tensor([-0.0489,  0.2769])\n",
      "Epoch 1399,Loss 3.159618\n",
      "\tParams: tensor([  5.0807, -15.6802])\n",
      "\tGrad:tensor([-0.0488,  0.2765])\n",
      "Epoch 1400,Loss 3.158830\n",
      "\tParams: tensor([  5.0812, -15.6829])\n",
      "\tGrad:tensor([-0.0488,  0.2760])\n",
      "Epoch 1401,Loss 3.158046\n",
      "\tParams: tensor([  5.0817, -15.6857])\n",
      "\tGrad:tensor([-0.0487,  0.2755])\n",
      "Epoch 1402,Loss 3.157263\n",
      "\tParams: tensor([  5.0822, -15.6884])\n",
      "\tGrad:tensor([-0.0486,  0.2751])\n",
      "Epoch 1403,Loss 3.156484\n",
      "\tParams: tensor([  5.0827, -15.6912])\n",
      "\tGrad:tensor([-0.0485,  0.2746])\n",
      "Epoch 1404,Loss 3.155708\n",
      "\tParams: tensor([  5.0832, -15.6939])\n",
      "\tGrad:tensor([-0.0484,  0.2741])\n",
      "Epoch 1405,Loss 3.154933\n",
      "\tParams: tensor([  5.0836, -15.6966])\n",
      "\tGrad:tensor([-0.0483,  0.2736])\n",
      "Epoch 1406,Loss 3.154162\n",
      "\tParams: tensor([  5.0841, -15.6994])\n",
      "\tGrad:tensor([-0.0483,  0.2732])\n",
      "Epoch 1407,Loss 3.153393\n",
      "\tParams: tensor([  5.0846, -15.7021])\n",
      "\tGrad:tensor([-0.0482,  0.2727])\n",
      "Epoch 1408,Loss 3.152628\n",
      "\tParams: tensor([  5.0851, -15.7048])\n",
      "\tGrad:tensor([-0.0481,  0.2723])\n",
      "Epoch 1409,Loss 3.151865\n",
      "\tParams: tensor([  5.0856, -15.7075])\n",
      "\tGrad:tensor([-0.0480,  0.2718])\n",
      "Epoch 1410,Loss 3.151101\n",
      "\tParams: tensor([  5.0860, -15.7103])\n",
      "\tGrad:tensor([-0.0479,  0.2713])\n",
      "Epoch 1411,Loss 3.150343\n",
      "\tParams: tensor([  5.0865, -15.7130])\n",
      "\tGrad:tensor([-0.0479,  0.2709])\n",
      "Epoch 1412,Loss 3.149587\n",
      "\tParams: tensor([  5.0870, -15.7157])\n",
      "\tGrad:tensor([-0.0478,  0.2704])\n",
      "Epoch 1413,Loss 3.148833\n",
      "\tParams: tensor([  5.0875, -15.7184])\n",
      "\tGrad:tensor([-0.0477,  0.2700])\n",
      "Epoch 1414,Loss 3.148083\n",
      "\tParams: tensor([  5.0879, -15.7211])\n",
      "\tGrad:tensor([-0.0476,  0.2695])\n",
      "Epoch 1415,Loss 3.147335\n",
      "\tParams: tensor([  5.0884, -15.7238])\n",
      "\tGrad:tensor([-0.0475,  0.2690])\n",
      "Epoch 1416,Loss 3.146588\n",
      "\tParams: tensor([  5.0889, -15.7264])\n",
      "\tGrad:tensor([-0.0474,  0.2686])\n",
      "Epoch 1417,Loss 3.145845\n",
      "\tParams: tensor([  5.0894, -15.7291])\n",
      "\tGrad:tensor([-0.0474,  0.2681])\n",
      "Epoch 1418,Loss 3.145105\n",
      "\tParams: tensor([  5.0898, -15.7318])\n",
      "\tGrad:tensor([-0.0473,  0.2677])\n",
      "Epoch 1419,Loss 3.144367\n",
      "\tParams: tensor([  5.0903, -15.7345])\n",
      "\tGrad:tensor([-0.0472,  0.2672])\n",
      "Epoch 1420,Loss 3.143630\n",
      "\tParams: tensor([  5.0908, -15.7371])\n",
      "\tGrad:tensor([-0.0471,  0.2668])\n",
      "Epoch 1421,Loss 3.142899\n",
      "\tParams: tensor([  5.0913, -15.7398])\n",
      "\tGrad:tensor([-0.0470,  0.2663])\n",
      "Epoch 1422,Loss 3.142166\n",
      "\tParams: tensor([  5.0917, -15.7425])\n",
      "\tGrad:tensor([-0.0469,  0.2659])\n",
      "Epoch 1423,Loss 3.141439\n",
      "\tParams: tensor([  5.0922, -15.7451])\n",
      "\tGrad:tensor([-0.0469,  0.2654])\n",
      "Epoch 1424,Loss 3.140712\n",
      "\tParams: tensor([  5.0927, -15.7478])\n",
      "\tGrad:tensor([-0.0468,  0.2649])\n",
      "Epoch 1425,Loss 3.139989\n",
      "\tParams: tensor([  5.0931, -15.7504])\n",
      "\tGrad:tensor([-0.0467,  0.2645])\n",
      "Epoch 1426,Loss 3.139271\n",
      "\tParams: tensor([  5.0936, -15.7530])\n",
      "\tGrad:tensor([-0.0466,  0.2641])\n",
      "Epoch 1427,Loss 3.138551\n",
      "\tParams: tensor([  5.0941, -15.7557])\n",
      "\tGrad:tensor([-0.0466,  0.2636])\n",
      "Epoch 1428,Loss 3.137835\n",
      "\tParams: tensor([  5.0945, -15.7583])\n",
      "\tGrad:tensor([-0.0465,  0.2632])\n",
      "Epoch 1429,Loss 3.137121\n",
      "\tParams: tensor([  5.0950, -15.7609])\n",
      "\tGrad:tensor([-0.0464,  0.2627])\n",
      "Epoch 1430,Loss 3.136409\n",
      "\tParams: tensor([  5.0955, -15.7636])\n",
      "\tGrad:tensor([-0.0463,  0.2623])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1431,Loss 3.135702\n",
      "\tParams: tensor([  5.0959, -15.7662])\n",
      "\tGrad:tensor([-0.0462,  0.2618])\n",
      "Epoch 1432,Loss 3.134994\n",
      "\tParams: tensor([  5.0964, -15.7688])\n",
      "\tGrad:tensor([-0.0461,  0.2614])\n",
      "Epoch 1433,Loss 3.134292\n",
      "\tParams: tensor([  5.0968, -15.7714])\n",
      "\tGrad:tensor([-0.0461,  0.2609])\n",
      "Epoch 1434,Loss 3.133590\n",
      "\tParams: tensor([  5.0973, -15.7740])\n",
      "\tGrad:tensor([-0.0460,  0.2605])\n",
      "Epoch 1435,Loss 3.132889\n",
      "\tParams: tensor([  5.0978, -15.7766])\n",
      "\tGrad:tensor([-0.0459,  0.2600])\n",
      "Epoch 1436,Loss 3.132194\n",
      "\tParams: tensor([  5.0982, -15.7792])\n",
      "\tGrad:tensor([-0.0459,  0.2596])\n",
      "Epoch 1437,Loss 3.131500\n",
      "\tParams: tensor([  5.0987, -15.7818])\n",
      "\tGrad:tensor([-0.0458,  0.2592])\n",
      "Epoch 1438,Loss 3.130810\n",
      "\tParams: tensor([  5.0991, -15.7844])\n",
      "\tGrad:tensor([-0.0457,  0.2587])\n",
      "Epoch 1439,Loss 3.130119\n",
      "\tParams: tensor([  5.0996, -15.7870])\n",
      "\tGrad:tensor([-0.0456,  0.2583])\n",
      "Epoch 1440,Loss 3.129432\n",
      "\tParams: tensor([  5.1000, -15.7895])\n",
      "\tGrad:tensor([-0.0455,  0.2578])\n",
      "Epoch 1441,Loss 3.128746\n",
      "\tParams: tensor([  5.1005, -15.7921])\n",
      "\tGrad:tensor([-0.0455,  0.2574])\n",
      "Epoch 1442,Loss 3.128064\n",
      "\tParams: tensor([  5.1010, -15.7947])\n",
      "\tGrad:tensor([-0.0454,  0.2570])\n",
      "Epoch 1443,Loss 3.127382\n",
      "\tParams: tensor([  5.1014, -15.7973])\n",
      "\tGrad:tensor([-0.0453,  0.2565])\n",
      "Epoch 1444,Loss 3.126705\n",
      "\tParams: tensor([  5.1019, -15.7998])\n",
      "\tGrad:tensor([-0.0453,  0.2561])\n",
      "Epoch 1445,Loss 3.126030\n",
      "\tParams: tensor([  5.1023, -15.8024])\n",
      "\tGrad:tensor([-0.0452,  0.2557])\n",
      "Epoch 1446,Loss 3.125356\n",
      "\tParams: tensor([  5.1028, -15.8049])\n",
      "\tGrad:tensor([-0.0451,  0.2552])\n",
      "Epoch 1447,Loss 3.124683\n",
      "\tParams: tensor([  5.1032, -15.8075])\n",
      "\tGrad:tensor([-0.0450,  0.2548])\n",
      "Epoch 1448,Loss 3.124016\n",
      "\tParams: tensor([  5.1037, -15.8100])\n",
      "\tGrad:tensor([-0.0449,  0.2544])\n",
      "Epoch 1449,Loss 3.123349\n",
      "\tParams: tensor([  5.1041, -15.8126])\n",
      "\tGrad:tensor([-0.0449,  0.2539])\n",
      "Epoch 1450,Loss 3.122686\n",
      "\tParams: tensor([  5.1046, -15.8151])\n",
      "\tGrad:tensor([-0.0448,  0.2535])\n",
      "Epoch 1451,Loss 3.122022\n",
      "\tParams: tensor([  5.1050, -15.8176])\n",
      "\tGrad:tensor([-0.0447,  0.2531])\n",
      "Epoch 1452,Loss 3.121362\n",
      "\tParams: tensor([  5.1055, -15.8201])\n",
      "\tGrad:tensor([-0.0446,  0.2526])\n",
      "Epoch 1453,Loss 3.120707\n",
      "\tParams: tensor([  5.1059, -15.8227])\n",
      "\tGrad:tensor([-0.0445,  0.2522])\n",
      "Epoch 1454,Loss 3.120049\n",
      "\tParams: tensor([  5.1063, -15.8252])\n",
      "\tGrad:tensor([-0.0445,  0.2518])\n",
      "Epoch 1455,Loss 3.119397\n",
      "\tParams: tensor([  5.1068, -15.8277])\n",
      "\tGrad:tensor([-0.0444,  0.2513])\n",
      "Epoch 1456,Loss 3.118746\n",
      "\tParams: tensor([  5.1072, -15.8302])\n",
      "\tGrad:tensor([-0.0443,  0.2509])\n",
      "Epoch 1457,Loss 3.118098\n",
      "\tParams: tensor([  5.1077, -15.8327])\n",
      "\tGrad:tensor([-0.0442,  0.2505])\n",
      "Epoch 1458,Loss 3.117451\n",
      "\tParams: tensor([  5.1081, -15.8352])\n",
      "\tGrad:tensor([-0.0442,  0.2501])\n",
      "Epoch 1459,Loss 3.116805\n",
      "\tParams: tensor([  5.1086, -15.8377])\n",
      "\tGrad:tensor([-0.0441,  0.2496])\n",
      "Epoch 1460,Loss 3.116164\n",
      "\tParams: tensor([  5.1090, -15.8402])\n",
      "\tGrad:tensor([-0.0440,  0.2492])\n",
      "Epoch 1461,Loss 3.115525\n",
      "\tParams: tensor([  5.1094, -15.8427])\n",
      "\tGrad:tensor([-0.0439,  0.2488])\n",
      "Epoch 1462,Loss 3.114886\n",
      "\tParams: tensor([  5.1099, -15.8452])\n",
      "\tGrad:tensor([-0.0439,  0.2484])\n",
      "Epoch 1463,Loss 3.114251\n",
      "\tParams: tensor([  5.1103, -15.8477])\n",
      "\tGrad:tensor([-0.0438,  0.2480])\n",
      "Epoch 1464,Loss 3.113617\n",
      "\tParams: tensor([  5.1107, -15.8501])\n",
      "\tGrad:tensor([-0.0437,  0.2475])\n",
      "Epoch 1465,Loss 3.112985\n",
      "\tParams: tensor([  5.1112, -15.8526])\n",
      "\tGrad:tensor([-0.0437,  0.2471])\n",
      "Epoch 1466,Loss 3.112358\n",
      "\tParams: tensor([  5.1116, -15.8551])\n",
      "\tGrad:tensor([-0.0436,  0.2467])\n",
      "Epoch 1467,Loss 3.111731\n",
      "\tParams: tensor([  5.1121, -15.8575])\n",
      "\tGrad:tensor([-0.0435,  0.2463])\n",
      "Epoch 1468,Loss 3.111103\n",
      "\tParams: tensor([  5.1125, -15.8600])\n",
      "\tGrad:tensor([-0.0434,  0.2459])\n",
      "Epoch 1469,Loss 3.110484\n",
      "\tParams: tensor([  5.1129, -15.8624])\n",
      "\tGrad:tensor([-0.0433,  0.2454])\n",
      "Epoch 1470,Loss 3.109859\n",
      "\tParams: tensor([  5.1134, -15.8649])\n",
      "\tGrad:tensor([-0.0433,  0.2450])\n",
      "Epoch 1471,Loss 3.109243\n",
      "\tParams: tensor([  5.1138, -15.8673])\n",
      "\tGrad:tensor([-0.0432,  0.2446])\n",
      "Epoch 1472,Loss 3.108627\n",
      "\tParams: tensor([  5.1142, -15.8698])\n",
      "\tGrad:tensor([-0.0431,  0.2442])\n",
      "Epoch 1473,Loss 3.108011\n",
      "\tParams: tensor([  5.1147, -15.8722])\n",
      "\tGrad:tensor([-0.0430,  0.2438])\n",
      "Epoch 1474,Loss 3.107401\n",
      "\tParams: tensor([  5.1151, -15.8747])\n",
      "\tGrad:tensor([-0.0430,  0.2434])\n",
      "Epoch 1475,Loss 3.106791\n",
      "\tParams: tensor([  5.1155, -15.8771])\n",
      "\tGrad:tensor([-0.0429,  0.2429])\n",
      "Epoch 1476,Loss 3.106180\n",
      "\tParams: tensor([  5.1159, -15.8795])\n",
      "\tGrad:tensor([-0.0428,  0.2425])\n",
      "Epoch 1477,Loss 3.105575\n",
      "\tParams: tensor([  5.1164, -15.8819])\n",
      "\tGrad:tensor([-0.0428,  0.2421])\n",
      "Epoch 1478,Loss 3.104972\n",
      "\tParams: tensor([  5.1168, -15.8843])\n",
      "\tGrad:tensor([-0.0427,  0.2417])\n",
      "Epoch 1479,Loss 3.104370\n",
      "\tParams: tensor([  5.1172, -15.8868])\n",
      "\tGrad:tensor([-0.0426,  0.2413])\n",
      "Epoch 1480,Loss 3.103770\n",
      "\tParams: tensor([  5.1176, -15.8892])\n",
      "\tGrad:tensor([-0.0425,  0.2409])\n",
      "Epoch 1481,Loss 3.103172\n",
      "\tParams: tensor([  5.1181, -15.8916])\n",
      "\tGrad:tensor([-0.0425,  0.2405])\n",
      "Epoch 1482,Loss 3.102576\n",
      "\tParams: tensor([  5.1185, -15.8940])\n",
      "\tGrad:tensor([-0.0424,  0.2401])\n",
      "Epoch 1483,Loss 3.101982\n",
      "\tParams: tensor([  5.1189, -15.8964])\n",
      "\tGrad:tensor([-0.0423,  0.2397])\n",
      "Epoch 1484,Loss 3.101390\n",
      "\tParams: tensor([  5.1193, -15.8988])\n",
      "\tGrad:tensor([-0.0423,  0.2393])\n",
      "Epoch 1485,Loss 3.100802\n",
      "\tParams: tensor([  5.1198, -15.9011])\n",
      "\tGrad:tensor([-0.0422,  0.2388])\n",
      "Epoch 1486,Loss 3.100213\n",
      "\tParams: tensor([  5.1202, -15.9035])\n",
      "\tGrad:tensor([-0.0421,  0.2384])\n",
      "Epoch 1487,Loss 3.099627\n",
      "\tParams: tensor([  5.1206, -15.9059])\n",
      "\tGrad:tensor([-0.0421,  0.2380])\n",
      "Epoch 1488,Loss 3.099044\n",
      "\tParams: tensor([  5.1210, -15.9083])\n",
      "\tGrad:tensor([-0.0420,  0.2376])\n",
      "Epoch 1489,Loss 3.098463\n",
      "\tParams: tensor([  5.1214, -15.9107])\n",
      "\tGrad:tensor([-0.0419,  0.2372])\n",
      "Epoch 1490,Loss 3.097883\n",
      "\tParams: tensor([  5.1219, -15.9130])\n",
      "\tGrad:tensor([-0.0418,  0.2368])\n",
      "Epoch 1491,Loss 3.097302\n",
      "\tParams: tensor([  5.1223, -15.9154])\n",
      "\tGrad:tensor([-0.0418,  0.2364])\n",
      "Epoch 1492,Loss 3.096727\n",
      "\tParams: tensor([  5.1227, -15.9178])\n",
      "\tGrad:tensor([-0.0417,  0.2360])\n",
      "Epoch 1493,Loss 3.096153\n",
      "\tParams: tensor([  5.1231, -15.9201])\n",
      "\tGrad:tensor([-0.0416,  0.2356])\n",
      "Epoch 1494,Loss 3.095583\n",
      "\tParams: tensor([  5.1235, -15.9225])\n",
      "\tGrad:tensor([-0.0416,  0.2352])\n",
      "Epoch 1495,Loss 3.095011\n",
      "\tParams: tensor([  5.1239, -15.9248])\n",
      "\tGrad:tensor([-0.0415,  0.2348])\n",
      "Epoch 1496,Loss 3.094444\n",
      "\tParams: tensor([  5.1244, -15.9272])\n",
      "\tGrad:tensor([-0.0414,  0.2344])\n",
      "Epoch 1497,Loss 3.093877\n",
      "\tParams: tensor([  5.1248, -15.9295])\n",
      "\tGrad:tensor([-0.0413,  0.2340])\n",
      "Epoch 1498,Loss 3.093314\n",
      "\tParams: tensor([  5.1252, -15.9318])\n",
      "\tGrad:tensor([-0.0413,  0.2336])\n",
      "Epoch 1499,Loss 3.092751\n",
      "\tParams: tensor([  5.1256, -15.9342])\n",
      "\tGrad:tensor([-0.0412,  0.2332])\n",
      "Epoch 1500,Loss 3.092191\n",
      "\tParams: tensor([  5.1260, -15.9365])\n",
      "\tGrad:tensor([-0.0411,  0.2328])\n",
      "Epoch 1501,Loss 3.091630\n",
      "\tParams: tensor([  5.1264, -15.9388])\n",
      "\tGrad:tensor([-0.0411,  0.2324])\n",
      "Epoch 1502,Loss 3.091074\n",
      "\tParams: tensor([  5.1268, -15.9411])\n",
      "\tGrad:tensor([-0.0410,  0.2320])\n",
      "Epoch 1503,Loss 3.090520\n",
      "\tParams: tensor([  5.1272, -15.9435])\n",
      "\tGrad:tensor([-0.0409,  0.2317])\n",
      "Epoch 1504,Loss 3.089969\n",
      "\tParams: tensor([  5.1276, -15.9458])\n",
      "\tGrad:tensor([-0.0408,  0.2313])\n",
      "Epoch 1505,Loss 3.089417\n",
      "\tParams: tensor([  5.1281, -15.9481])\n",
      "\tGrad:tensor([-0.0408,  0.2309])\n",
      "Epoch 1506,Loss 3.088867\n",
      "\tParams: tensor([  5.1285, -15.9504])\n",
      "\tGrad:tensor([-0.0407,  0.2305])\n",
      "Epoch 1507,Loss 3.088320\n",
      "\tParams: tensor([  5.1289, -15.9527])\n",
      "\tGrad:tensor([-0.0406,  0.2301])\n",
      "Epoch 1508,Loss 3.087775\n",
      "\tParams: tensor([  5.1293, -15.9550])\n",
      "\tGrad:tensor([-0.0406,  0.2297])\n",
      "Epoch 1509,Loss 3.087232\n",
      "\tParams: tensor([  5.1297, -15.9573])\n",
      "\tGrad:tensor([-0.0405,  0.2293])\n",
      "Epoch 1510,Loss 3.086690\n",
      "\tParams: tensor([  5.1301, -15.9596])\n",
      "\tGrad:tensor([-0.0404,  0.2289])\n",
      "Epoch 1511,Loss 3.086150\n",
      "\tParams: tensor([  5.1305, -15.9618])\n",
      "\tGrad:tensor([-0.0404,  0.2285])\n",
      "Epoch 1512,Loss 3.085612\n",
      "\tParams: tensor([  5.1309, -15.9641])\n",
      "\tGrad:tensor([-0.0403,  0.2281])\n",
      "Epoch 1513,Loss 3.085075\n",
      "\tParams: tensor([  5.1313, -15.9664])\n",
      "\tGrad:tensor([-0.0402,  0.2277])\n",
      "Epoch 1514,Loss 3.084542\n",
      "\tParams: tensor([  5.1317, -15.9687])\n",
      "\tGrad:tensor([-0.0402,  0.2274])\n",
      "Epoch 1515,Loss 3.084009\n",
      "\tParams: tensor([  5.1321, -15.9709])\n",
      "\tGrad:tensor([-0.0401,  0.2270])\n",
      "Epoch 1516,Loss 3.083478\n",
      "\tParams: tensor([  5.1325, -15.9732])\n",
      "\tGrad:tensor([-0.0400,  0.2266])\n",
      "Epoch 1517,Loss 3.082948\n",
      "\tParams: tensor([  5.1329, -15.9755])\n",
      "\tGrad:tensor([-0.0400,  0.2262])\n",
      "Epoch 1518,Loss 3.082422\n",
      "\tParams: tensor([  5.1333, -15.9777])\n",
      "\tGrad:tensor([-0.0399,  0.2258])\n",
      "Epoch 1519,Loss 3.081897\n",
      "\tParams: tensor([  5.1337, -15.9800])\n",
      "\tGrad:tensor([-0.0398,  0.2254])\n",
      "Epoch 1520,Loss 3.081373\n",
      "\tParams: tensor([  5.1341, -15.9822])\n",
      "\tGrad:tensor([-0.0398,  0.2250])\n",
      "Epoch 1521,Loss 3.080850\n",
      "\tParams: tensor([  5.1345, -15.9845])\n",
      "\tGrad:tensor([-0.0397,  0.2247])\n",
      "Epoch 1522,Loss 3.080331\n",
      "\tParams: tensor([  5.1349, -15.9867])\n",
      "\tGrad:tensor([-0.0396,  0.2243])\n",
      "Epoch 1523,Loss 3.079811\n",
      "\tParams: tensor([  5.1353, -15.9890])\n",
      "\tGrad:tensor([-0.0396,  0.2239])\n",
      "Epoch 1524,Loss 3.079296\n",
      "\tParams: tensor([  5.1357, -15.9912])\n",
      "\tGrad:tensor([-0.0395,  0.2235])\n",
      "Epoch 1525,Loss 3.078781\n",
      "\tParams: tensor([  5.1361, -15.9934])\n",
      "\tGrad:tensor([-0.0394,  0.2231])\n",
      "Epoch 1526,Loss 3.078268\n",
      "\tParams: tensor([  5.1365, -15.9957])\n",
      "\tGrad:tensor([-0.0394,  0.2228])\n",
      "Epoch 1527,Loss 3.077758\n",
      "\tParams: tensor([  5.1369, -15.9979])\n",
      "\tGrad:tensor([-0.0393,  0.2224])\n",
      "Epoch 1528,Loss 3.077248\n",
      "\tParams: tensor([  5.1372, -16.0001])\n",
      "\tGrad:tensor([-0.0392,  0.2220])\n",
      "Epoch 1529,Loss 3.076739\n",
      "\tParams: tensor([  5.1376, -16.0023])\n",
      "\tGrad:tensor([-0.0391,  0.2216])\n",
      "Epoch 1530,Loss 3.076232\n",
      "\tParams: tensor([  5.1380, -16.0045])\n",
      "\tGrad:tensor([-0.0391,  0.2213])\n",
      "Epoch 1531,Loss 3.075729\n",
      "\tParams: tensor([  5.1384, -16.0067])\n",
      "\tGrad:tensor([-0.0390,  0.2209])\n",
      "Epoch 1532,Loss 3.075225\n",
      "\tParams: tensor([  5.1388, -16.0089])\n",
      "\tGrad:tensor([-0.0390,  0.2205])\n",
      "Epoch 1533,Loss 3.074724\n",
      "\tParams: tensor([  5.1392, -16.0111])\n",
      "\tGrad:tensor([-0.0389,  0.2201])\n",
      "Epoch 1534,Loss 3.074227\n",
      "\tParams: tensor([  5.1396, -16.0133])\n",
      "\tGrad:tensor([-0.0388,  0.2198])\n",
      "Epoch 1535,Loss 3.073726\n",
      "\tParams: tensor([  5.1400, -16.0155])\n",
      "\tGrad:tensor([-0.0387,  0.2194])\n",
      "Epoch 1536,Loss 3.073232\n",
      "\tParams: tensor([  5.1404, -16.0177])\n",
      "\tGrad:tensor([-0.0387,  0.2190])\n",
      "Epoch 1537,Loss 3.072739\n",
      "\tParams: tensor([  5.1407, -16.0199])\n",
      "\tGrad:tensor([-0.0386,  0.2186])\n",
      "Epoch 1538,Loss 3.072245\n",
      "\tParams: tensor([  5.1411, -16.0221])\n",
      "\tGrad:tensor([-0.0385,  0.2183])\n",
      "Epoch 1539,Loss 3.071753\n",
      "\tParams: tensor([  5.1415, -16.0243])\n",
      "\tGrad:tensor([-0.0385,  0.2179])\n",
      "Epoch 1540,Loss 3.071265\n",
      "\tParams: tensor([  5.1419, -16.0264])\n",
      "\tGrad:tensor([-0.0384,  0.2175])\n",
      "Epoch 1541,Loss 3.070778\n",
      "\tParams: tensor([  5.1423, -16.0286])\n",
      "\tGrad:tensor([-0.0383,  0.2172])\n",
      "Epoch 1542,Loss 3.070293\n",
      "\tParams: tensor([  5.1427, -16.0308])\n",
      "\tGrad:tensor([-0.0383,  0.2168])\n",
      "Epoch 1543,Loss 3.069808\n",
      "\tParams: tensor([  5.1430, -16.0330])\n",
      "\tGrad:tensor([-0.0382,  0.2164])\n",
      "Epoch 1544,Loss 3.069326\n",
      "\tParams: tensor([  5.1434, -16.0351])\n",
      "\tGrad:tensor([-0.0382,  0.2161])\n",
      "Epoch 1545,Loss 3.068845\n",
      "\tParams: tensor([  5.1438, -16.0373])\n",
      "\tGrad:tensor([-0.0381,  0.2157])\n",
      "Epoch 1546,Loss 3.068366\n",
      "\tParams: tensor([  5.1442, -16.0394])\n",
      "\tGrad:tensor([-0.0380,  0.2153])\n",
      "Epoch 1547,Loss 3.067887\n",
      "\tParams: tensor([  5.1446, -16.0416])\n",
      "\tGrad:tensor([-0.0380,  0.2150])\n",
      "Epoch 1548,Loss 3.067412\n",
      "\tParams: tensor([  5.1449, -16.0437])\n",
      "\tGrad:tensor([-0.0379,  0.2146])\n",
      "Epoch 1549,Loss 3.066937\n",
      "\tParams: tensor([  5.1453, -16.0459])\n",
      "\tGrad:tensor([-0.0378,  0.2142])\n",
      "Epoch 1550,Loss 3.066463\n",
      "\tParams: tensor([  5.1457, -16.0480])\n",
      "\tGrad:tensor([-0.0378,  0.2139])\n",
      "Epoch 1551,Loss 3.065993\n",
      "\tParams: tensor([  5.1461, -16.0501])\n",
      "\tGrad:tensor([-0.0377,  0.2135])\n",
      "Epoch 1552,Loss 3.065524\n",
      "\tParams: tensor([  5.1465, -16.0523])\n",
      "\tGrad:tensor([-0.0376,  0.2131])\n",
      "Epoch 1553,Loss 3.065055\n",
      "\tParams: tensor([  5.1468, -16.0544])\n",
      "\tGrad:tensor([-0.0376,  0.2128])\n",
      "Epoch 1554,Loss 3.064588\n",
      "\tParams: tensor([  5.1472, -16.0565])\n",
      "\tGrad:tensor([-0.0375,  0.2124])\n",
      "Epoch 1555,Loss 3.064123\n",
      "\tParams: tensor([  5.1476, -16.0586])\n",
      "\tGrad:tensor([-0.0375,  0.2120])\n",
      "Epoch 1556,Loss 3.063660\n",
      "\tParams: tensor([  5.1480, -16.0608])\n",
      "\tGrad:tensor([-0.0374,  0.2117])\n",
      "Epoch 1557,Loss 3.063199\n",
      "\tParams: tensor([  5.1483, -16.0629])\n",
      "\tGrad:tensor([-0.0373,  0.2113])\n",
      "Epoch 1558,Loss 3.062738\n",
      "\tParams: tensor([  5.1487, -16.0650])\n",
      "\tGrad:tensor([-0.0373,  0.2110])\n",
      "Epoch 1559,Loss 3.062280\n",
      "\tParams: tensor([  5.1491, -16.0671])\n",
      "\tGrad:tensor([-0.0372,  0.2106])\n",
      "Epoch 1560,Loss 3.061822\n",
      "\tParams: tensor([  5.1494, -16.0692])\n",
      "\tGrad:tensor([-0.0371,  0.2103])\n",
      "Epoch 1561,Loss 3.061367\n",
      "\tParams: tensor([  5.1498, -16.0713])\n",
      "\tGrad:tensor([-0.0371,  0.2099])\n",
      "Epoch 1562,Loss 3.060913\n",
      "\tParams: tensor([  5.1502, -16.0734])\n",
      "\tGrad:tensor([-0.0370,  0.2095])\n",
      "Epoch 1563,Loss 3.060462\n",
      "\tParams: tensor([  5.1506, -16.0755])\n",
      "\tGrad:tensor([-0.0370,  0.2092])\n",
      "Epoch 1564,Loss 3.060011\n",
      "\tParams: tensor([  5.1509, -16.0776])\n",
      "\tGrad:tensor([-0.0369,  0.2088])\n",
      "Epoch 1565,Loss 3.059561\n",
      "\tParams: tensor([  5.1513, -16.0796])\n",
      "\tGrad:tensor([-0.0368,  0.2085])\n",
      "Epoch 1566,Loss 3.059114\n",
      "\tParams: tensor([  5.1517, -16.0817])\n",
      "\tGrad:tensor([-0.0368,  0.2081])\n",
      "Epoch 1567,Loss 3.058668\n",
      "\tParams: tensor([  5.1520, -16.0838])\n",
      "\tGrad:tensor([-0.0367,  0.2078])\n",
      "Epoch 1568,Loss 3.058221\n",
      "\tParams: tensor([  5.1524, -16.0859])\n",
      "\tGrad:tensor([-0.0366,  0.2074])\n",
      "Epoch 1569,Loss 3.057781\n",
      "\tParams: tensor([  5.1528, -16.0880])\n",
      "\tGrad:tensor([-0.0366,  0.2071])\n",
      "Epoch 1570,Loss 3.057338\n",
      "\tParams: tensor([  5.1531, -16.0900])\n",
      "\tGrad:tensor([-0.0365,  0.2067])\n",
      "Epoch 1571,Loss 3.056898\n",
      "\tParams: tensor([  5.1535, -16.0921])\n",
      "\tGrad:tensor([-0.0364,  0.2064])\n",
      "Epoch 1572,Loss 3.056458\n",
      "\tParams: tensor([  5.1539, -16.0941])\n",
      "\tGrad:tensor([-0.0364,  0.2060])\n",
      "Epoch 1573,Loss 3.056019\n",
      "\tParams: tensor([  5.1542, -16.0962])\n",
      "\tGrad:tensor([-0.0363,  0.2057])\n",
      "Epoch 1574,Loss 3.055585\n",
      "\tParams: tensor([  5.1546, -16.0983])\n",
      "\tGrad:tensor([-0.0363,  0.2053])\n",
      "Epoch 1575,Loss 3.055151\n",
      "\tParams: tensor([  5.1549, -16.1003])\n",
      "\tGrad:tensor([-0.0362,  0.2050])\n",
      "Epoch 1576,Loss 3.054717\n",
      "\tParams: tensor([  5.1553, -16.1023])\n",
      "\tGrad:tensor([-0.0361,  0.2046])\n",
      "Epoch 1577,Loss 3.054286\n",
      "\tParams: tensor([  5.1557, -16.1044])\n",
      "\tGrad:tensor([-0.0361,  0.2043])\n",
      "Epoch 1578,Loss 3.053857\n",
      "\tParams: tensor([  5.1560, -16.1064])\n",
      "\tGrad:tensor([-0.0360,  0.2039])\n",
      "Epoch 1579,Loss 3.053427\n",
      "\tParams: tensor([  5.1564, -16.1085])\n",
      "\tGrad:tensor([-0.0360,  0.2036])\n",
      "Epoch 1580,Loss 3.053000\n",
      "\tParams: tensor([  5.1567, -16.1105])\n",
      "\tGrad:tensor([-0.0359,  0.2032])\n",
      "Epoch 1581,Loss 3.052576\n",
      "\tParams: tensor([  5.1571, -16.1125])\n",
      "\tGrad:tensor([-0.0358,  0.2029])\n",
      "Epoch 1582,Loss 3.052152\n",
      "\tParams: tensor([  5.1575, -16.1146])\n",
      "\tGrad:tensor([-0.0358,  0.2025])\n",
      "Epoch 1583,Loss 3.051730\n",
      "\tParams: tensor([  5.1578, -16.1166])\n",
      "\tGrad:tensor([-0.0357,  0.2022])\n",
      "Epoch 1584,Loss 3.051306\n",
      "\tParams: tensor([  5.1582, -16.1186])\n",
      "\tGrad:tensor([-0.0357,  0.2018])\n",
      "Epoch 1585,Loss 3.050888\n",
      "\tParams: tensor([  5.1585, -16.1206])\n",
      "\tGrad:tensor([-0.0356,  0.2015])\n",
      "Epoch 1586,Loss 3.050471\n",
      "\tParams: tensor([  5.1589, -16.1226])\n",
      "\tGrad:tensor([-0.0355,  0.2012])\n",
      "Epoch 1587,Loss 3.050052\n",
      "\tParams: tensor([  5.1592, -16.1246])\n",
      "\tGrad:tensor([-0.0355,  0.2008])\n",
      "Epoch 1588,Loss 3.049639\n",
      "\tParams: tensor([  5.1596, -16.1266])\n",
      "\tGrad:tensor([-0.0354,  0.2005])\n",
      "Epoch 1589,Loss 3.049223\n",
      "\tParams: tensor([  5.1599, -16.1286])\n",
      "\tGrad:tensor([-0.0354,  0.2001])\n",
      "Epoch 1590,Loss 3.048811\n",
      "\tParams: tensor([  5.1603, -16.1306])\n",
      "\tGrad:tensor([-0.0353,  0.1998])\n",
      "Epoch 1591,Loss 3.048398\n",
      "\tParams: tensor([  5.1607, -16.1326])\n",
      "\tGrad:tensor([-0.0353,  0.1995])\n",
      "Epoch 1592,Loss 3.047991\n",
      "\tParams: tensor([  5.1610, -16.1346])\n",
      "\tGrad:tensor([-0.0352,  0.1991])\n",
      "Epoch 1593,Loss 3.047581\n",
      "\tParams: tensor([  5.1614, -16.1366])\n",
      "\tGrad:tensor([-0.0351,  0.1988])\n",
      "Epoch 1594,Loss 3.047173\n",
      "\tParams: tensor([  5.1617, -16.1386])\n",
      "\tGrad:tensor([-0.0351,  0.1984])\n",
      "Epoch 1595,Loss 3.046768\n",
      "\tParams: tensor([  5.1621, -16.1406])\n",
      "\tGrad:tensor([-0.0350,  0.1981])\n",
      "Epoch 1596,Loss 3.046362\n",
      "\tParams: tensor([  5.1624, -16.1425])\n",
      "\tGrad:tensor([-0.0349,  0.1978])\n",
      "Epoch 1597,Loss 3.045960\n",
      "\tParams: tensor([  5.1628, -16.1445])\n",
      "\tGrad:tensor([-0.0349,  0.1974])\n",
      "Epoch 1598,Loss 3.045559\n",
      "\tParams: tensor([  5.1631, -16.1465])\n",
      "\tGrad:tensor([-0.0348,  0.1971])\n",
      "Epoch 1599,Loss 3.045160\n",
      "\tParams: tensor([  5.1635, -16.1485])\n",
      "\tGrad:tensor([-0.0348,  0.1968])\n",
      "Epoch 1600,Loss 3.044759\n",
      "\tParams: tensor([  5.1638, -16.1504])\n",
      "\tGrad:tensor([-0.0347,  0.1964])\n",
      "Epoch 1601,Loss 3.044361\n",
      "\tParams: tensor([  5.1641, -16.1524])\n",
      "\tGrad:tensor([-0.0346,  0.1961])\n",
      "Epoch 1602,Loss 3.043966\n",
      "\tParams: tensor([  5.1645, -16.1543])\n",
      "\tGrad:tensor([-0.0346,  0.1958])\n",
      "Epoch 1603,Loss 3.043571\n",
      "\tParams: tensor([  5.1648, -16.1563])\n",
      "\tGrad:tensor([-0.0345,  0.1954])\n",
      "Epoch 1604,Loss 3.043176\n",
      "\tParams: tensor([  5.1652, -16.1582])\n",
      "\tGrad:tensor([-0.0345,  0.1951])\n",
      "Epoch 1605,Loss 3.042785\n",
      "\tParams: tensor([  5.1655, -16.1602])\n",
      "\tGrad:tensor([-0.0344,  0.1948])\n",
      "Epoch 1606,Loss 3.042395\n",
      "\tParams: tensor([  5.1659, -16.1621])\n",
      "\tGrad:tensor([-0.0343,  0.1944])\n",
      "Epoch 1607,Loss 3.042005\n",
      "\tParams: tensor([  5.1662, -16.1641])\n",
      "\tGrad:tensor([-0.0343,  0.1941])\n",
      "Epoch 1608,Loss 3.041615\n",
      "\tParams: tensor([  5.1666, -16.1660])\n",
      "\tGrad:tensor([-0.0342,  0.1938])\n",
      "Epoch 1609,Loss 3.041230\n",
      "\tParams: tensor([  5.1669, -16.1680])\n",
      "\tGrad:tensor([-0.0342,  0.1934])\n",
      "Epoch 1610,Loss 3.040844\n",
      "\tParams: tensor([  5.1672, -16.1699])\n",
      "\tGrad:tensor([-0.0341,  0.1931])\n",
      "Epoch 1611,Loss 3.040461\n",
      "\tParams: tensor([  5.1676, -16.1718])\n",
      "\tGrad:tensor([-0.0341,  0.1928])\n",
      "Epoch 1612,Loss 3.040077\n",
      "\tParams: tensor([  5.1679, -16.1737])\n",
      "\tGrad:tensor([-0.0340,  0.1925])\n",
      "Epoch 1613,Loss 3.039695\n",
      "\tParams: tensor([  5.1683, -16.1757])\n",
      "\tGrad:tensor([-0.0339,  0.1921])\n",
      "Epoch 1614,Loss 3.039314\n",
      "\tParams: tensor([  5.1686, -16.1776])\n",
      "\tGrad:tensor([-0.0339,  0.1918])\n",
      "Epoch 1615,Loss 3.038934\n",
      "\tParams: tensor([  5.1689, -16.1795])\n",
      "\tGrad:tensor([-0.0338,  0.1915])\n",
      "Epoch 1616,Loss 3.038557\n",
      "\tParams: tensor([  5.1693, -16.1814])\n",
      "\tGrad:tensor([-0.0338,  0.1912])\n",
      "Epoch 1617,Loss 3.038181\n",
      "\tParams: tensor([  5.1696, -16.1833])\n",
      "\tGrad:tensor([-0.0337,  0.1908])\n",
      "Epoch 1618,Loss 3.037805\n",
      "\tParams: tensor([  5.1699, -16.1852])\n",
      "\tGrad:tensor([-0.0337,  0.1905])\n",
      "Epoch 1619,Loss 3.037432\n",
      "\tParams: tensor([  5.1703, -16.1871])\n",
      "\tGrad:tensor([-0.0336,  0.1902])\n",
      "Epoch 1620,Loss 3.037059\n",
      "\tParams: tensor([  5.1706, -16.1890])\n",
      "\tGrad:tensor([-0.0335,  0.1899])\n",
      "Epoch 1621,Loss 3.036689\n",
      "\tParams: tensor([  5.1710, -16.1909])\n",
      "\tGrad:tensor([-0.0335,  0.1895])\n",
      "Epoch 1622,Loss 3.036319\n",
      "\tParams: tensor([  5.1713, -16.1928])\n",
      "\tGrad:tensor([-0.0334,  0.1892])\n",
      "Epoch 1623,Loss 3.035949\n",
      "\tParams: tensor([  5.1716, -16.1947])\n",
      "\tGrad:tensor([-0.0334,  0.1889])\n",
      "Epoch 1624,Loss 3.035583\n",
      "\tParams: tensor([  5.1720, -16.1966])\n",
      "\tGrad:tensor([-0.0333,  0.1886])\n",
      "Epoch 1625,Loss 3.035216\n",
      "\tParams: tensor([  5.1723, -16.1985])\n",
      "\tGrad:tensor([-0.0333,  0.1883])\n",
      "Epoch 1626,Loss 3.034849\n",
      "\tParams: tensor([  5.1726, -16.2003])\n",
      "\tGrad:tensor([-0.0332,  0.1879])\n",
      "Epoch 1627,Loss 3.034485\n",
      "\tParams: tensor([  5.1729, -16.2022])\n",
      "\tGrad:tensor([-0.0331,  0.1876])\n",
      "Epoch 1628,Loss 3.034123\n",
      "\tParams: tensor([  5.1733, -16.2041])\n",
      "\tGrad:tensor([-0.0331,  0.1873])\n",
      "Epoch 1629,Loss 3.033762\n",
      "\tParams: tensor([  5.1736, -16.2060])\n",
      "\tGrad:tensor([-0.0330,  0.1870])\n",
      "Epoch 1630,Loss 3.033402\n",
      "\tParams: tensor([  5.1739, -16.2078])\n",
      "\tGrad:tensor([-0.0330,  0.1867])\n",
      "Epoch 1631,Loss 3.033041\n",
      "\tParams: tensor([  5.1743, -16.2097])\n",
      "\tGrad:tensor([-0.0329,  0.1863])\n",
      "Epoch 1632,Loss 3.032685\n",
      "\tParams: tensor([  5.1746, -16.2116])\n",
      "\tGrad:tensor([-0.0329,  0.1860])\n",
      "Epoch 1633,Loss 3.032329\n",
      "\tParams: tensor([  5.1749, -16.2134])\n",
      "\tGrad:tensor([-0.0328,  0.1857])\n",
      "Epoch 1634,Loss 3.031973\n",
      "\tParams: tensor([  5.1753, -16.2153])\n",
      "\tGrad:tensor([-0.0327,  0.1854])\n",
      "Epoch 1635,Loss 3.031619\n",
      "\tParams: tensor([  5.1756, -16.2171])\n",
      "\tGrad:tensor([-0.0327,  0.1851])\n",
      "Epoch 1636,Loss 3.031265\n",
      "\tParams: tensor([  5.1759, -16.2190])\n",
      "\tGrad:tensor([-0.0326,  0.1848])\n",
      "Epoch 1637,Loss 3.030913\n",
      "\tParams: tensor([  5.1762, -16.2208])\n",
      "\tGrad:tensor([-0.0326,  0.1845])\n",
      "Epoch 1638,Loss 3.030564\n",
      "\tParams: tensor([  5.1766, -16.2226])\n",
      "\tGrad:tensor([-0.0325,  0.1841])\n",
      "Epoch 1639,Loss 3.030215\n",
      "\tParams: tensor([  5.1769, -16.2245])\n",
      "\tGrad:tensor([-0.0325,  0.1838])\n",
      "Epoch 1640,Loss 3.029867\n",
      "\tParams: tensor([  5.1772, -16.2263])\n",
      "\tGrad:tensor([-0.0324,  0.1835])\n",
      "Epoch 1641,Loss 3.029518\n",
      "\tParams: tensor([  5.1775, -16.2282])\n",
      "\tGrad:tensor([-0.0324,  0.1832])\n",
      "Epoch 1642,Loss 3.029173\n",
      "\tParams: tensor([  5.1779, -16.2300])\n",
      "\tGrad:tensor([-0.0323,  0.1829])\n",
      "Epoch 1643,Loss 3.028828\n",
      "\tParams: tensor([  5.1782, -16.2318])\n",
      "\tGrad:tensor([-0.0323,  0.1826])\n",
      "Epoch 1644,Loss 3.028486\n",
      "\tParams: tensor([  5.1785, -16.2336])\n",
      "\tGrad:tensor([-0.0322,  0.1823])\n",
      "Epoch 1645,Loss 3.028142\n",
      "\tParams: tensor([  5.1788, -16.2355])\n",
      "\tGrad:tensor([-0.0321,  0.1820])\n",
      "Epoch 1646,Loss 3.027802\n",
      "\tParams: tensor([  5.1791, -16.2373])\n",
      "\tGrad:tensor([-0.0321,  0.1817])\n",
      "Epoch 1647,Loss 3.027463\n",
      "\tParams: tensor([  5.1795, -16.2391])\n",
      "\tGrad:tensor([-0.0320,  0.1813])\n",
      "Epoch 1648,Loss 3.027122\n",
      "\tParams: tensor([  5.1798, -16.2409])\n",
      "\tGrad:tensor([-0.0320,  0.1810])\n",
      "Epoch 1649,Loss 3.026784\n",
      "\tParams: tensor([  5.1801, -16.2427])\n",
      "\tGrad:tensor([-0.0319,  0.1807])\n",
      "Epoch 1650,Loss 3.026447\n",
      "\tParams: tensor([  5.1804, -16.2445])\n",
      "\tGrad:tensor([-0.0319,  0.1804])\n",
      "Epoch 1651,Loss 3.026111\n",
      "\tParams: tensor([  5.1807, -16.2463])\n",
      "\tGrad:tensor([-0.0318,  0.1801])\n",
      "Epoch 1652,Loss 3.025780\n",
      "\tParams: tensor([  5.1811, -16.2481])\n",
      "\tGrad:tensor([-0.0318,  0.1798])\n",
      "Epoch 1653,Loss 3.025447\n",
      "\tParams: tensor([  5.1814, -16.2499])\n",
      "\tGrad:tensor([-0.0317,  0.1795])\n",
      "Epoch 1654,Loss 3.025114\n",
      "\tParams: tensor([  5.1817, -16.2517])\n",
      "\tGrad:tensor([-0.0317,  0.1792])\n",
      "Epoch 1655,Loss 3.024782\n",
      "\tParams: tensor([  5.1820, -16.2535])\n",
      "\tGrad:tensor([-0.0316,  0.1789])\n",
      "Epoch 1656,Loss 3.024452\n",
      "\tParams: tensor([  5.1823, -16.2553])\n",
      "\tGrad:tensor([-0.0316,  0.1786])\n",
      "Epoch 1657,Loss 3.024125\n",
      "\tParams: tensor([  5.1826, -16.2570])\n",
      "\tGrad:tensor([-0.0315,  0.1783])\n",
      "Epoch 1658,Loss 3.023796\n",
      "\tParams: tensor([  5.1829, -16.2588])\n",
      "\tGrad:tensor([-0.0315,  0.1780])\n",
      "Epoch 1659,Loss 3.023471\n",
      "\tParams: tensor([  5.1833, -16.2606])\n",
      "\tGrad:tensor([-0.0314,  0.1777])\n",
      "Epoch 1660,Loss 3.023145\n",
      "\tParams: tensor([  5.1836, -16.2624])\n",
      "\tGrad:tensor([-0.0313,  0.1774])\n",
      "Epoch 1661,Loss 3.022820\n",
      "\tParams: tensor([  5.1839, -16.2641])\n",
      "\tGrad:tensor([-0.0313,  0.1771])\n",
      "Epoch 1662,Loss 3.022498\n",
      "\tParams: tensor([  5.1842, -16.2659])\n",
      "\tGrad:tensor([-0.0312,  0.1768])\n",
      "Epoch 1663,Loss 3.022177\n",
      "\tParams: tensor([  5.1845, -16.2677])\n",
      "\tGrad:tensor([-0.0312,  0.1765])\n",
      "Epoch 1664,Loss 3.021855\n",
      "\tParams: tensor([  5.1848, -16.2694])\n",
      "\tGrad:tensor([-0.0311,  0.1762])\n",
      "Epoch 1665,Loss 3.021534\n",
      "\tParams: tensor([  5.1851, -16.2712])\n",
      "\tGrad:tensor([-0.0311,  0.1759])\n",
      "Epoch 1666,Loss 3.021217\n",
      "\tParams: tensor([  5.1854, -16.2730])\n",
      "\tGrad:tensor([-0.0310,  0.1756])\n",
      "Epoch 1667,Loss 3.020898\n",
      "\tParams: tensor([  5.1858, -16.2747])\n",
      "\tGrad:tensor([-0.0310,  0.1753])\n",
      "Epoch 1668,Loss 3.020582\n",
      "\tParams: tensor([  5.1861, -16.2765])\n",
      "\tGrad:tensor([-0.0309,  0.1750])\n",
      "Epoch 1669,Loss 3.020265\n",
      "\tParams: tensor([  5.1864, -16.2782])\n",
      "\tGrad:tensor([-0.0309,  0.1747])\n",
      "Epoch 1670,Loss 3.019952\n",
      "\tParams: tensor([  5.1867, -16.2800])\n",
      "\tGrad:tensor([-0.0308,  0.1744])\n",
      "Epoch 1671,Loss 3.019639\n",
      "\tParams: tensor([  5.1870, -16.2817])\n",
      "\tGrad:tensor([-0.0308,  0.1741])\n",
      "Epoch 1672,Loss 3.019325\n",
      "\tParams: tensor([  5.1873, -16.2834])\n",
      "\tGrad:tensor([-0.0307,  0.1738])\n",
      "Epoch 1673,Loss 3.019016\n",
      "\tParams: tensor([  5.1876, -16.2852])\n",
      "\tGrad:tensor([-0.0307,  0.1735])\n",
      "Epoch 1674,Loss 3.018706\n",
      "\tParams: tensor([  5.1879, -16.2869])\n",
      "\tGrad:tensor([-0.0306,  0.1732])\n",
      "Epoch 1675,Loss 3.018395\n",
      "\tParams: tensor([  5.1882, -16.2886])\n",
      "\tGrad:tensor([-0.0305,  0.1729])\n",
      "Epoch 1676,Loss 3.018089\n",
      "\tParams: tensor([  5.1885, -16.2904])\n",
      "\tGrad:tensor([-0.0305,  0.1726])\n",
      "Epoch 1677,Loss 3.017780\n",
      "\tParams: tensor([  5.1888, -16.2921])\n",
      "\tGrad:tensor([-0.0304,  0.1723])\n",
      "Epoch 1678,Loss 3.017475\n",
      "\tParams: tensor([  5.1891, -16.2938])\n",
      "\tGrad:tensor([-0.0304,  0.1720])\n",
      "Epoch 1679,Loss 3.017170\n",
      "\tParams: tensor([  5.1894, -16.2955])\n",
      "\tGrad:tensor([-0.0303,  0.1717])\n",
      "Epoch 1680,Loss 3.016867\n",
      "\tParams: tensor([  5.1897, -16.2972])\n",
      "\tGrad:tensor([-0.0303,  0.1715])\n",
      "Epoch 1681,Loss 3.016564\n",
      "\tParams: tensor([  5.1900, -16.2989])\n",
      "\tGrad:tensor([-0.0302,  0.1712])\n",
      "Epoch 1682,Loss 3.016262\n",
      "\tParams: tensor([  5.1903, -16.3006])\n",
      "\tGrad:tensor([-0.0302,  0.1709])\n",
      "Epoch 1683,Loss 3.015959\n",
      "\tParams: tensor([  5.1906, -16.3024])\n",
      "\tGrad:tensor([-0.0301,  0.1706])\n",
      "Epoch 1684,Loss 3.015662\n",
      "\tParams: tensor([  5.1909, -16.3041])\n",
      "\tGrad:tensor([-0.0301,  0.1703])\n",
      "Epoch 1685,Loss 3.015361\n",
      "\tParams: tensor([  5.1912, -16.3058])\n",
      "\tGrad:tensor([-0.0300,  0.1700])\n",
      "Epoch 1686,Loss 3.015064\n",
      "\tParams: tensor([  5.1915, -16.3075])\n",
      "\tGrad:tensor([-0.0300,  0.1697])\n",
      "Epoch 1687,Loss 3.014768\n",
      "\tParams: tensor([  5.1918, -16.3091])\n",
      "\tGrad:tensor([-0.0299,  0.1694])\n",
      "Epoch 1688,Loss 3.014472\n",
      "\tParams: tensor([  5.1921, -16.3108])\n",
      "\tGrad:tensor([-0.0299,  0.1691])\n",
      "Epoch 1689,Loss 3.014179\n",
      "\tParams: tensor([  5.1924, -16.3125])\n",
      "\tGrad:tensor([-0.0298,  0.1688])\n",
      "Epoch 1690,Loss 3.013884\n",
      "\tParams: tensor([  5.1927, -16.3142])\n",
      "\tGrad:tensor([-0.0298,  0.1686])\n",
      "Epoch 1691,Loss 3.013591\n",
      "\tParams: tensor([  5.1930, -16.3159])\n",
      "\tGrad:tensor([-0.0297,  0.1683])\n",
      "Epoch 1692,Loss 3.013299\n",
      "\tParams: tensor([  5.1933, -16.3176])\n",
      "\tGrad:tensor([-0.0297,  0.1680])\n",
      "Epoch 1693,Loss 3.013008\n",
      "\tParams: tensor([  5.1936, -16.3193])\n",
      "\tGrad:tensor([-0.0296,  0.1677])\n",
      "Epoch 1694,Loss 3.012719\n",
      "\tParams: tensor([  5.1939, -16.3209])\n",
      "\tGrad:tensor([-0.0296,  0.1674])\n",
      "Epoch 1695,Loss 3.012431\n",
      "\tParams: tensor([  5.1942, -16.3226])\n",
      "\tGrad:tensor([-0.0295,  0.1671])\n",
      "Epoch 1696,Loss 3.012141\n",
      "\tParams: tensor([  5.1945, -16.3243])\n",
      "\tGrad:tensor([-0.0295,  0.1668])\n",
      "Epoch 1697,Loss 3.011855\n",
      "\tParams: tensor([  5.1948, -16.3259])\n",
      "\tGrad:tensor([-0.0294,  0.1666])\n",
      "Epoch 1698,Loss 3.011570\n",
      "\tParams: tensor([  5.1951, -16.3276])\n",
      "\tGrad:tensor([-0.0294,  0.1663])\n",
      "Epoch 1699,Loss 3.011284\n",
      "\tParams: tensor([  5.1954, -16.3293])\n",
      "\tGrad:tensor([-0.0293,  0.1660])\n",
      "Epoch 1700,Loss 3.011001\n",
      "\tParams: tensor([  5.1957, -16.3309])\n",
      "\tGrad:tensor([-0.0293,  0.1657])\n",
      "Epoch 1701,Loss 3.010718\n",
      "\tParams: tensor([  5.1960, -16.3326])\n",
      "\tGrad:tensor([-0.0292,  0.1654])\n",
      "Epoch 1702,Loss 3.010436\n",
      "\tParams: tensor([  5.1963, -16.3342])\n",
      "\tGrad:tensor([-0.0292,  0.1652])\n",
      "Epoch 1703,Loss 3.010156\n",
      "\tParams: tensor([  5.1966, -16.3359])\n",
      "\tGrad:tensor([-0.0291,  0.1649])\n",
      "Epoch 1704,Loss 3.009876\n",
      "\tParams: tensor([  5.1968, -16.3375])\n",
      "\tGrad:tensor([-0.0291,  0.1646])\n",
      "Epoch 1705,Loss 3.009595\n",
      "\tParams: tensor([  5.1971, -16.3392])\n",
      "\tGrad:tensor([-0.0290,  0.1643])\n",
      "Epoch 1706,Loss 3.009319\n",
      "\tParams: tensor([  5.1974, -16.3408])\n",
      "\tGrad:tensor([-0.0290,  0.1640])\n",
      "Epoch 1707,Loss 3.009040\n",
      "\tParams: tensor([  5.1977, -16.3424])\n",
      "\tGrad:tensor([-0.0289,  0.1638])\n",
      "Epoch 1708,Loss 3.008763\n",
      "\tParams: tensor([  5.1980, -16.3441])\n",
      "\tGrad:tensor([-0.0289,  0.1635])\n",
      "Epoch 1709,Loss 3.008487\n",
      "\tParams: tensor([  5.1983, -16.3457])\n",
      "\tGrad:tensor([-0.0288,  0.1632])\n",
      "Epoch 1710,Loss 3.008215\n",
      "\tParams: tensor([  5.1986, -16.3473])\n",
      "\tGrad:tensor([-0.0288,  0.1629])\n",
      "Epoch 1711,Loss 3.007941\n",
      "\tParams: tensor([  5.1989, -16.3490])\n",
      "\tGrad:tensor([-0.0287,  0.1626])\n",
      "Epoch 1712,Loss 3.007668\n",
      "\tParams: tensor([  5.1992, -16.3506])\n",
      "\tGrad:tensor([-0.0287,  0.1624])\n",
      "Epoch 1713,Loss 3.007397\n",
      "\tParams: tensor([  5.1994, -16.3522])\n",
      "\tGrad:tensor([-0.0286,  0.1621])\n",
      "Epoch 1714,Loss 3.007126\n",
      "\tParams: tensor([  5.1997, -16.3538])\n",
      "\tGrad:tensor([-0.0286,  0.1618])\n",
      "Epoch 1715,Loss 3.006857\n",
      "\tParams: tensor([  5.2000, -16.3554])\n",
      "\tGrad:tensor([-0.0285,  0.1615])\n",
      "Epoch 1716,Loss 3.006586\n",
      "\tParams: tensor([  5.2003, -16.3570])\n",
      "\tGrad:tensor([-0.0285,  0.1613])\n",
      "Epoch 1717,Loss 3.006318\n",
      "\tParams: tensor([  5.2006, -16.3587])\n",
      "\tGrad:tensor([-0.0284,  0.1610])\n",
      "Epoch 1718,Loss 3.006052\n",
      "\tParams: tensor([  5.2009, -16.3603])\n",
      "\tGrad:tensor([-0.0284,  0.1607])\n",
      "Epoch 1719,Loss 3.005785\n",
      "\tParams: tensor([  5.2012, -16.3619])\n",
      "\tGrad:tensor([-0.0284,  0.1604])\n",
      "Epoch 1720,Loss 3.005521\n",
      "\tParams: tensor([  5.2014, -16.3635])\n",
      "\tGrad:tensor([-0.0283,  0.1602])\n",
      "Epoch 1721,Loss 3.005256\n",
      "\tParams: tensor([  5.2017, -16.3651])\n",
      "\tGrad:tensor([-0.0283,  0.1599])\n",
      "Epoch 1722,Loss 3.004993\n",
      "\tParams: tensor([  5.2020, -16.3667])\n",
      "\tGrad:tensor([-0.0282,  0.1596])\n",
      "Epoch 1723,Loss 3.004729\n",
      "\tParams: tensor([  5.2023, -16.3683])\n",
      "\tGrad:tensor([-0.0281,  0.1594])\n",
      "Epoch 1724,Loss 3.004467\n",
      "\tParams: tensor([  5.2026, -16.3699])\n",
      "\tGrad:tensor([-0.0281,  0.1591])\n",
      "Epoch 1725,Loss 3.004207\n",
      "\tParams: tensor([  5.2028, -16.3714])\n",
      "\tGrad:tensor([-0.0280,  0.1588])\n",
      "Epoch 1726,Loss 3.003947\n",
      "\tParams: tensor([  5.2031, -16.3730])\n",
      "\tGrad:tensor([-0.0280,  0.1586])\n",
      "Epoch 1727,Loss 3.003690\n",
      "\tParams: tensor([  5.2034, -16.3746])\n",
      "\tGrad:tensor([-0.0280,  0.1583])\n",
      "Epoch 1728,Loss 3.003431\n",
      "\tParams: tensor([  5.2037, -16.3762])\n",
      "\tGrad:tensor([-0.0279,  0.1580])\n",
      "Epoch 1729,Loss 3.003174\n",
      "\tParams: tensor([  5.2040, -16.3778])\n",
      "\tGrad:tensor([-0.0279,  0.1577])\n",
      "Epoch 1730,Loss 3.002918\n",
      "\tParams: tensor([  5.2042, -16.3793])\n",
      "\tGrad:tensor([-0.0278,  0.1575])\n",
      "Epoch 1731,Loss 3.002661\n",
      "\tParams: tensor([  5.2045, -16.3809])\n",
      "\tGrad:tensor([-0.0278,  0.1572])\n",
      "Epoch 1732,Loss 3.002406\n",
      "\tParams: tensor([  5.2048, -16.3825])\n",
      "\tGrad:tensor([-0.0277,  0.1569])\n",
      "Epoch 1733,Loss 3.002152\n",
      "\tParams: tensor([  5.2051, -16.3840])\n",
      "\tGrad:tensor([-0.0277,  0.1567])\n",
      "Epoch 1734,Loss 3.001901\n",
      "\tParams: tensor([  5.2053, -16.3856])\n",
      "\tGrad:tensor([-0.0276,  0.1564])\n",
      "Epoch 1735,Loss 3.001649\n",
      "\tParams: tensor([  5.2056, -16.3872])\n",
      "\tGrad:tensor([-0.0276,  0.1561])\n",
      "Epoch 1736,Loss 3.001395\n",
      "\tParams: tensor([  5.2059, -16.3887])\n",
      "\tGrad:tensor([-0.0275,  0.1559])\n",
      "Epoch 1737,Loss 3.001145\n",
      "\tParams: tensor([  5.2062, -16.3903])\n",
      "\tGrad:tensor([-0.0275,  0.1556])\n",
      "Epoch 1738,Loss 3.000898\n",
      "\tParams: tensor([  5.2064, -16.3918])\n",
      "\tGrad:tensor([-0.0274,  0.1553])\n",
      "Epoch 1739,Loss 3.000648\n",
      "\tParams: tensor([  5.2067, -16.3934])\n",
      "\tGrad:tensor([-0.0274,  0.1551])\n",
      "Epoch 1740,Loss 3.000400\n",
      "\tParams: tensor([  5.2070, -16.3949])\n",
      "\tGrad:tensor([-0.0273,  0.1548])\n",
      "Epoch 1741,Loss 3.000154\n",
      "\tParams: tensor([  5.2073, -16.3965])\n",
      "\tGrad:tensor([-0.0273,  0.1546])\n",
      "Epoch 1742,Loss 2.999907\n",
      "\tParams: tensor([  5.2075, -16.3980])\n",
      "\tGrad:tensor([-0.0273,  0.1543])\n",
      "Epoch 1743,Loss 2.999662\n",
      "\tParams: tensor([  5.2078, -16.3996])\n",
      "\tGrad:tensor([-0.0272,  0.1540])\n",
      "Epoch 1744,Loss 2.999417\n",
      "\tParams: tensor([  5.2081, -16.4011])\n",
      "\tGrad:tensor([-0.0272,  0.1538])\n",
      "Epoch 1745,Loss 2.999174\n",
      "\tParams: tensor([  5.2084, -16.4026])\n",
      "\tGrad:tensor([-0.0271,  0.1535])\n",
      "Epoch 1746,Loss 2.998930\n",
      "\tParams: tensor([  5.2086, -16.4042])\n",
      "\tGrad:tensor([-0.0271,  0.1533])\n",
      "Epoch 1747,Loss 2.998688\n",
      "\tParams: tensor([  5.2089, -16.4057])\n",
      "\tGrad:tensor([-0.0270,  0.1530])\n",
      "Epoch 1748,Loss 2.998448\n",
      "\tParams: tensor([  5.2092, -16.4072])\n",
      "\tGrad:tensor([-0.0270,  0.1527])\n",
      "Epoch 1749,Loss 2.998208\n",
      "\tParams: tensor([  5.2094, -16.4088])\n",
      "\tGrad:tensor([-0.0269,  0.1525])\n",
      "Epoch 1750,Loss 2.997968\n",
      "\tParams: tensor([  5.2097, -16.4103])\n",
      "\tGrad:tensor([-0.0269,  0.1522])\n",
      "Epoch 1751,Loss 2.997730\n",
      "\tParams: tensor([  5.2100, -16.4118])\n",
      "\tGrad:tensor([-0.0268,  0.1520])\n",
      "Epoch 1752,Loss 2.997490\n",
      "\tParams: tensor([  5.2102, -16.4133])\n",
      "\tGrad:tensor([-0.0268,  0.1517])\n",
      "Epoch 1753,Loss 2.997254\n",
      "\tParams: tensor([  5.2105, -16.4148])\n",
      "\tGrad:tensor([-0.0267,  0.1514])\n",
      "Epoch 1754,Loss 2.997018\n",
      "\tParams: tensor([  5.2108, -16.4163])\n",
      "\tGrad:tensor([-0.0267,  0.1512])\n",
      "Epoch 1755,Loss 2.996782\n",
      "\tParams: tensor([  5.2110, -16.4179])\n",
      "\tGrad:tensor([-0.0266,  0.1509])\n",
      "Epoch 1756,Loss 2.996548\n",
      "\tParams: tensor([  5.2113, -16.4194])\n",
      "\tGrad:tensor([-0.0266,  0.1507])\n",
      "Epoch 1757,Loss 2.996313\n",
      "\tParams: tensor([  5.2116, -16.4209])\n",
      "\tGrad:tensor([-0.0266,  0.1504])\n",
      "Epoch 1758,Loss 2.996081\n",
      "\tParams: tensor([  5.2118, -16.4224])\n",
      "\tGrad:tensor([-0.0265,  0.1502])\n",
      "Epoch 1759,Loss 2.995847\n",
      "\tParams: tensor([  5.2121, -16.4239])\n",
      "\tGrad:tensor([-0.0265,  0.1499])\n",
      "Epoch 1760,Loss 2.995615\n",
      "\tParams: tensor([  5.2124, -16.4254])\n",
      "\tGrad:tensor([-0.0264,  0.1496])\n",
      "Epoch 1761,Loss 2.995387\n",
      "\tParams: tensor([  5.2126, -16.4269])\n",
      "\tGrad:tensor([-0.0264,  0.1494])\n",
      "Epoch 1762,Loss 2.995156\n",
      "\tParams: tensor([  5.2129, -16.4283])\n",
      "\tGrad:tensor([-0.0263,  0.1491])\n",
      "Epoch 1763,Loss 2.994928\n",
      "\tParams: tensor([  5.2132, -16.4298])\n",
      "\tGrad:tensor([-0.0263,  0.1489])\n",
      "Epoch 1764,Loss 2.994699\n",
      "\tParams: tensor([  5.2134, -16.4313])\n",
      "\tGrad:tensor([-0.0263,  0.1486])\n",
      "Epoch 1765,Loss 2.994471\n",
      "\tParams: tensor([  5.2137, -16.4328])\n",
      "\tGrad:tensor([-0.0262,  0.1484])\n",
      "Epoch 1766,Loss 2.994245\n",
      "\tParams: tensor([  5.2139, -16.4343])\n",
      "\tGrad:tensor([-0.0262,  0.1481])\n",
      "Epoch 1767,Loss 2.994019\n",
      "\tParams: tensor([  5.2142, -16.4358])\n",
      "\tGrad:tensor([-0.0261,  0.1479])\n",
      "Epoch 1768,Loss 2.993794\n",
      "\tParams: tensor([  5.2145, -16.4372])\n",
      "\tGrad:tensor([-0.0261,  0.1476])\n",
      "Epoch 1769,Loss 2.993569\n",
      "\tParams: tensor([  5.2147, -16.4387])\n",
      "\tGrad:tensor([-0.0260,  0.1474])\n",
      "Epoch 1770,Loss 2.993344\n",
      "\tParams: tensor([  5.2150, -16.4402])\n",
      "\tGrad:tensor([-0.0260,  0.1471])\n",
      "Epoch 1771,Loss 2.993121\n",
      "\tParams: tensor([  5.2152, -16.4417])\n",
      "\tGrad:tensor([-0.0260,  0.1469])\n",
      "Epoch 1772,Loss 2.992900\n",
      "\tParams: tensor([  5.2155, -16.4431])\n",
      "\tGrad:tensor([-0.0259,  0.1466])\n",
      "Epoch 1773,Loss 2.992678\n",
      "\tParams: tensor([  5.2158, -16.4446])\n",
      "\tGrad:tensor([-0.0259,  0.1464])\n",
      "Epoch 1774,Loss 2.992457\n",
      "\tParams: tensor([  5.2160, -16.4460])\n",
      "\tGrad:tensor([-0.0258,  0.1461])\n",
      "Epoch 1775,Loss 2.992237\n",
      "\tParams: tensor([  5.2163, -16.4475])\n",
      "\tGrad:tensor([-0.0258,  0.1459])\n",
      "Epoch 1776,Loss 2.992017\n",
      "\tParams: tensor([  5.2165, -16.4490])\n",
      "\tGrad:tensor([-0.0257,  0.1456])\n",
      "Epoch 1777,Loss 2.991798\n",
      "\tParams: tensor([  5.2168, -16.4504])\n",
      "\tGrad:tensor([-0.0257,  0.1454])\n",
      "Epoch 1778,Loss 2.991582\n",
      "\tParams: tensor([  5.2170, -16.4519])\n",
      "\tGrad:tensor([-0.0256,  0.1451])\n",
      "Epoch 1779,Loss 2.991366\n",
      "\tParams: tensor([  5.2173, -16.4533])\n",
      "\tGrad:tensor([-0.0256,  0.1449])\n",
      "Epoch 1780,Loss 2.991146\n",
      "\tParams: tensor([  5.2176, -16.4548])\n",
      "\tGrad:tensor([-0.0256,  0.1446])\n",
      "Epoch 1781,Loss 2.990932\n",
      "\tParams: tensor([  5.2178, -16.4562])\n",
      "\tGrad:tensor([-0.0255,  0.1444])\n",
      "Epoch 1782,Loss 2.990719\n",
      "\tParams: tensor([  5.2181, -16.4576])\n",
      "\tGrad:tensor([-0.0255,  0.1442])\n",
      "Epoch 1783,Loss 2.990503\n",
      "\tParams: tensor([  5.2183, -16.4591])\n",
      "\tGrad:tensor([-0.0254,  0.1439])\n",
      "Epoch 1784,Loss 2.990288\n",
      "\tParams: tensor([  5.2186, -16.4605])\n",
      "\tGrad:tensor([-0.0254,  0.1437])\n",
      "Epoch 1785,Loss 2.990078\n",
      "\tParams: tensor([  5.2188, -16.4620])\n",
      "\tGrad:tensor([-0.0253,  0.1434])\n",
      "Epoch 1786,Loss 2.989866\n",
      "\tParams: tensor([  5.2191, -16.4634])\n",
      "\tGrad:tensor([-0.0253,  0.1432])\n",
      "Epoch 1787,Loss 2.989655\n",
      "\tParams: tensor([  5.2193, -16.4648])\n",
      "\tGrad:tensor([-0.0252,  0.1429])\n",
      "Epoch 1788,Loss 2.989443\n",
      "\tParams: tensor([  5.2196, -16.4662])\n",
      "\tGrad:tensor([-0.0252,  0.1427])\n",
      "Epoch 1789,Loss 2.989233\n",
      "\tParams: tensor([  5.2198, -16.4677])\n",
      "\tGrad:tensor([-0.0252,  0.1424])\n",
      "Epoch 1790,Loss 2.989025\n",
      "\tParams: tensor([  5.2201, -16.4691])\n",
      "\tGrad:tensor([-0.0251,  0.1422])\n",
      "Epoch 1791,Loss 2.988817\n",
      "\tParams: tensor([  5.2203, -16.4705])\n",
      "\tGrad:tensor([-0.0251,  0.1420])\n",
      "Epoch 1792,Loss 2.988609\n",
      "\tParams: tensor([  5.2206, -16.4719])\n",
      "\tGrad:tensor([-0.0250,  0.1417])\n",
      "Epoch 1793,Loss 2.988401\n",
      "\tParams: tensor([  5.2208, -16.4733])\n",
      "\tGrad:tensor([-0.0250,  0.1415])\n",
      "Epoch 1794,Loss 2.988195\n",
      "\tParams: tensor([  5.2211, -16.4748])\n",
      "\tGrad:tensor([-0.0249,  0.1412])\n",
      "Epoch 1795,Loss 2.987989\n",
      "\tParams: tensor([  5.2213, -16.4762])\n",
      "\tGrad:tensor([-0.0249,  0.1410])\n",
      "Epoch 1796,Loss 2.987785\n",
      "\tParams: tensor([  5.2216, -16.4776])\n",
      "\tGrad:tensor([-0.0249,  0.1408])\n",
      "Epoch 1797,Loss 2.987582\n",
      "\tParams: tensor([  5.2218, -16.4790])\n",
      "\tGrad:tensor([-0.0248,  0.1405])\n",
      "Epoch 1798,Loss 2.987377\n",
      "\tParams: tensor([  5.2221, -16.4804])\n",
      "\tGrad:tensor([-0.0248,  0.1403])\n",
      "Epoch 1799,Loss 2.987174\n",
      "\tParams: tensor([  5.2223, -16.4818])\n",
      "\tGrad:tensor([-0.0247,  0.1400])\n",
      "Epoch 1800,Loss 2.986974\n",
      "\tParams: tensor([  5.2226, -16.4832])\n",
      "\tGrad:tensor([-0.0247,  0.1398])\n",
      "Epoch 1801,Loss 2.986771\n",
      "\tParams: tensor([  5.2228, -16.4846])\n",
      "\tGrad:tensor([-0.0246,  0.1396])\n",
      "Epoch 1802,Loss 2.986570\n",
      "\tParams: tensor([  5.2231, -16.4860])\n",
      "\tGrad:tensor([-0.0246,  0.1393])\n",
      "Epoch 1803,Loss 2.986371\n",
      "\tParams: tensor([  5.2233, -16.4874])\n",
      "\tGrad:tensor([-0.0246,  0.1391])\n",
      "Epoch 1804,Loss 2.986171\n",
      "\tParams: tensor([  5.2236, -16.4888])\n",
      "\tGrad:tensor([-0.0245,  0.1389])\n",
      "Epoch 1805,Loss 2.985972\n",
      "\tParams: tensor([  5.2238, -16.4901])\n",
      "\tGrad:tensor([-0.0245,  0.1386])\n",
      "Epoch 1806,Loss 2.985774\n",
      "\tParams: tensor([  5.2241, -16.4915])\n",
      "\tGrad:tensor([-0.0245,  0.1384])\n",
      "Epoch 1807,Loss 2.985578\n",
      "\tParams: tensor([  5.2243, -16.4929])\n",
      "\tGrad:tensor([-0.0244,  0.1382])\n",
      "Epoch 1808,Loss 2.985381\n",
      "\tParams: tensor([  5.2245, -16.4943])\n",
      "\tGrad:tensor([-0.0244,  0.1379])\n",
      "Epoch 1809,Loss 2.985184\n",
      "\tParams: tensor([  5.2248, -16.4957])\n",
      "\tGrad:tensor([-0.0243,  0.1377])\n",
      "Epoch 1810,Loss 2.984989\n",
      "\tParams: tensor([  5.2250, -16.4970])\n",
      "\tGrad:tensor([-0.0243,  0.1374])\n",
      "Epoch 1811,Loss 2.984793\n",
      "\tParams: tensor([  5.2253, -16.4984])\n",
      "\tGrad:tensor([-0.0243,  0.1372])\n",
      "Epoch 1812,Loss 2.984601\n",
      "\tParams: tensor([  5.2255, -16.4998])\n",
      "\tGrad:tensor([-0.0242,  0.1370])\n",
      "Epoch 1813,Loss 2.984407\n",
      "\tParams: tensor([  5.2258, -16.5011])\n",
      "\tGrad:tensor([-0.0242,  0.1368])\n",
      "Epoch 1814,Loss 2.984215\n",
      "\tParams: tensor([  5.2260, -16.5025])\n",
      "\tGrad:tensor([-0.0241,  0.1365])\n",
      "Epoch 1815,Loss 2.984022\n",
      "\tParams: tensor([  5.2262, -16.5039])\n",
      "\tGrad:tensor([-0.0241,  0.1363])\n",
      "Epoch 1816,Loss 2.983831\n",
      "\tParams: tensor([  5.2265, -16.5052])\n",
      "\tGrad:tensor([-0.0240,  0.1361])\n",
      "Epoch 1817,Loss 2.983639\n",
      "\tParams: tensor([  5.2267, -16.5066])\n",
      "\tGrad:tensor([-0.0240,  0.1358])\n",
      "Epoch 1818,Loss 2.983449\n",
      "\tParams: tensor([  5.2270, -16.5079])\n",
      "\tGrad:tensor([-0.0239,  0.1356])\n",
      "Epoch 1819,Loss 2.983259\n",
      "\tParams: tensor([  5.2272, -16.5093])\n",
      "\tGrad:tensor([-0.0239,  0.1354])\n",
      "Epoch 1820,Loss 2.983073\n",
      "\tParams: tensor([  5.2274, -16.5107])\n",
      "\tGrad:tensor([-0.0239,  0.1351])\n",
      "Epoch 1821,Loss 2.982884\n",
      "\tParams: tensor([  5.2277, -16.5120])\n",
      "\tGrad:tensor([-0.0238,  0.1349])\n",
      "Epoch 1822,Loss 2.982697\n",
      "\tParams: tensor([  5.2279, -16.5133])\n",
      "\tGrad:tensor([-0.0238,  0.1347])\n",
      "Epoch 1823,Loss 2.982510\n",
      "\tParams: tensor([  5.2281, -16.5147])\n",
      "\tGrad:tensor([-0.0237,  0.1344])\n",
      "Epoch 1824,Loss 2.982322\n",
      "\tParams: tensor([  5.2284, -16.5160])\n",
      "\tGrad:tensor([-0.0237,  0.1342])\n",
      "Epoch 1825,Loss 2.982137\n",
      "\tParams: tensor([  5.2286, -16.5174])\n",
      "\tGrad:tensor([-0.0237,  0.1340])\n",
      "Epoch 1826,Loss 2.981953\n",
      "\tParams: tensor([  5.2289, -16.5187])\n",
      "\tGrad:tensor([-0.0236,  0.1338])\n",
      "Epoch 1827,Loss 2.981769\n",
      "\tParams: tensor([  5.2291, -16.5200])\n",
      "\tGrad:tensor([-0.0236,  0.1335])\n",
      "Epoch 1828,Loss 2.981586\n",
      "\tParams: tensor([  5.2293, -16.5214])\n",
      "\tGrad:tensor([-0.0236,  0.1333])\n",
      "Epoch 1829,Loss 2.981402\n",
      "\tParams: tensor([  5.2296, -16.5227])\n",
      "\tGrad:tensor([-0.0235,  0.1331])\n",
      "Epoch 1830,Loss 2.981219\n",
      "\tParams: tensor([  5.2298, -16.5240])\n",
      "\tGrad:tensor([-0.0235,  0.1329])\n",
      "Epoch 1831,Loss 2.981037\n",
      "\tParams: tensor([  5.2300, -16.5254])\n",
      "\tGrad:tensor([-0.0235,  0.1326])\n",
      "Epoch 1832,Loss 2.980856\n",
      "\tParams: tensor([  5.2303, -16.5267])\n",
      "\tGrad:tensor([-0.0234,  0.1324])\n",
      "Epoch 1833,Loss 2.980675\n",
      "\tParams: tensor([  5.2305, -16.5280])\n",
      "\tGrad:tensor([-0.0234,  0.1322])\n",
      "Epoch 1834,Loss 2.980495\n",
      "\tParams: tensor([  5.2307, -16.5293])\n",
      "\tGrad:tensor([-0.0233,  0.1320])\n",
      "Epoch 1835,Loss 2.980315\n",
      "\tParams: tensor([  5.2310, -16.5306])\n",
      "\tGrad:tensor([-0.0233,  0.1317])\n",
      "Epoch 1836,Loss 2.980137\n",
      "\tParams: tensor([  5.2312, -16.5320])\n",
      "\tGrad:tensor([-0.0232,  0.1315])\n",
      "Epoch 1837,Loss 2.979958\n",
      "\tParams: tensor([  5.2314, -16.5333])\n",
      "\tGrad:tensor([-0.0232,  0.1313])\n",
      "Epoch 1838,Loss 2.979782\n",
      "\tParams: tensor([  5.2317, -16.5346])\n",
      "\tGrad:tensor([-0.0232,  0.1311])\n",
      "Epoch 1839,Loss 2.979604\n",
      "\tParams: tensor([  5.2319, -16.5359])\n",
      "\tGrad:tensor([-0.0231,  0.1308])\n",
      "Epoch 1840,Loss 2.979428\n",
      "\tParams: tensor([  5.2321, -16.5372])\n",
      "\tGrad:tensor([-0.0231,  0.1306])\n",
      "Epoch 1841,Loss 2.979253\n",
      "\tParams: tensor([  5.2324, -16.5385])\n",
      "\tGrad:tensor([-0.0230,  0.1304])\n",
      "Epoch 1842,Loss 2.979078\n",
      "\tParams: tensor([  5.2326, -16.5398])\n",
      "\tGrad:tensor([-0.0230,  0.1302])\n",
      "Epoch 1843,Loss 2.978902\n",
      "\tParams: tensor([  5.2328, -16.5411])\n",
      "\tGrad:tensor([-0.0229,  0.1300])\n",
      "Epoch 1844,Loss 2.978729\n",
      "\tParams: tensor([  5.2330, -16.5424])\n",
      "\tGrad:tensor([-0.0229,  0.1297])\n",
      "Epoch 1845,Loss 2.978556\n",
      "\tParams: tensor([  5.2333, -16.5437])\n",
      "\tGrad:tensor([-0.0229,  0.1295])\n",
      "Epoch 1846,Loss 2.978382\n",
      "\tParams: tensor([  5.2335, -16.5450])\n",
      "\tGrad:tensor([-0.0228,  0.1293])\n",
      "Epoch 1847,Loss 2.978211\n",
      "\tParams: tensor([  5.2337, -16.5463])\n",
      "\tGrad:tensor([-0.0228,  0.1291])\n",
      "Epoch 1848,Loss 2.978039\n",
      "\tParams: tensor([  5.2340, -16.5476])\n",
      "\tGrad:tensor([-0.0228,  0.1288])\n",
      "Epoch 1849,Loss 2.977867\n",
      "\tParams: tensor([  5.2342, -16.5489])\n",
      "\tGrad:tensor([-0.0227,  0.1286])\n",
      "Epoch 1850,Loss 2.977696\n",
      "\tParams: tensor([  5.2344, -16.5501])\n",
      "\tGrad:tensor([-0.0227,  0.1284])\n",
      "Epoch 1851,Loss 2.977527\n",
      "\tParams: tensor([  5.2346, -16.5514])\n",
      "\tGrad:tensor([-0.0227,  0.1282])\n",
      "Epoch 1852,Loss 2.977357\n",
      "\tParams: tensor([  5.2349, -16.5527])\n",
      "\tGrad:tensor([-0.0226,  0.1280])\n",
      "Epoch 1853,Loss 2.977188\n",
      "\tParams: tensor([  5.2351, -16.5540])\n",
      "\tGrad:tensor([-0.0226,  0.1278])\n",
      "Epoch 1854,Loss 2.977021\n",
      "\tParams: tensor([  5.2353, -16.5553])\n",
      "\tGrad:tensor([-0.0225,  0.1275])\n",
      "Epoch 1855,Loss 2.976853\n",
      "\tParams: tensor([  5.2355, -16.5565])\n",
      "\tGrad:tensor([-0.0225,  0.1273])\n",
      "Epoch 1856,Loss 2.976687\n",
      "\tParams: tensor([  5.2358, -16.5578])\n",
      "\tGrad:tensor([-0.0225,  0.1271])\n",
      "Epoch 1857,Loss 2.976520\n",
      "\tParams: tensor([  5.2360, -16.5591])\n",
      "\tGrad:tensor([-0.0224,  0.1269])\n",
      "Epoch 1858,Loss 2.976354\n",
      "\tParams: tensor([  5.2362, -16.5603])\n",
      "\tGrad:tensor([-0.0224,  0.1267])\n",
      "Epoch 1859,Loss 2.976189\n",
      "\tParams: tensor([  5.2364, -16.5616])\n",
      "\tGrad:tensor([-0.0223,  0.1265])\n",
      "Epoch 1860,Loss 2.976023\n",
      "\tParams: tensor([  5.2367, -16.5629])\n",
      "\tGrad:tensor([-0.0223,  0.1263])\n",
      "Epoch 1861,Loss 2.975860\n",
      "\tParams: tensor([  5.2369, -16.5641])\n",
      "\tGrad:tensor([-0.0223,  0.1260])\n",
      "Epoch 1862,Loss 2.975697\n",
      "\tParams: tensor([  5.2371, -16.5654])\n",
      "\tGrad:tensor([-0.0222,  0.1258])\n",
      "Epoch 1863,Loss 2.975533\n",
      "\tParams: tensor([  5.2373, -16.5666])\n",
      "\tGrad:tensor([-0.0222,  0.1256])\n",
      "Epoch 1864,Loss 2.975369\n",
      "\tParams: tensor([  5.2375, -16.5679])\n",
      "\tGrad:tensor([-0.0222,  0.1254])\n",
      "Epoch 1865,Loss 2.975208\n",
      "\tParams: tensor([  5.2378, -16.5691])\n",
      "\tGrad:tensor([-0.0221,  0.1252])\n",
      "Epoch 1866,Loss 2.975046\n",
      "\tParams: tensor([  5.2380, -16.5704])\n",
      "\tGrad:tensor([-0.0221,  0.1250])\n",
      "Epoch 1867,Loss 2.974886\n",
      "\tParams: tensor([  5.2382, -16.5716])\n",
      "\tGrad:tensor([-0.0220,  0.1248])\n",
      "Epoch 1868,Loss 2.974725\n",
      "\tParams: tensor([  5.2384, -16.5729])\n",
      "\tGrad:tensor([-0.0220,  0.1245])\n",
      "Epoch 1869,Loss 2.974565\n",
      "\tParams: tensor([  5.2386, -16.5741])\n",
      "\tGrad:tensor([-0.0220,  0.1243])\n",
      "Epoch 1870,Loss 2.974406\n",
      "\tParams: tensor([  5.2389, -16.5754])\n",
      "\tGrad:tensor([-0.0219,  0.1241])\n",
      "Epoch 1871,Loss 2.974248\n",
      "\tParams: tensor([  5.2391, -16.5766])\n",
      "\tGrad:tensor([-0.0219,  0.1239])\n",
      "Epoch 1872,Loss 2.974088\n",
      "\tParams: tensor([  5.2393, -16.5778])\n",
      "\tGrad:tensor([-0.0219,  0.1237])\n",
      "Epoch 1873,Loss 2.973930\n",
      "\tParams: tensor([  5.2395, -16.5791])\n",
      "\tGrad:tensor([-0.0218,  0.1235])\n",
      "Epoch 1874,Loss 2.973776\n",
      "\tParams: tensor([  5.2397, -16.5803])\n",
      "\tGrad:tensor([-0.0218,  0.1233])\n",
      "Epoch 1875,Loss 2.973618\n",
      "\tParams: tensor([  5.2400, -16.5815])\n",
      "\tGrad:tensor([-0.0217,  0.1231])\n",
      "Epoch 1876,Loss 2.973463\n",
      "\tParams: tensor([  5.2402, -16.5828])\n",
      "\tGrad:tensor([-0.0217,  0.1229])\n",
      "Epoch 1877,Loss 2.973307\n",
      "\tParams: tensor([  5.2404, -16.5840])\n",
      "\tGrad:tensor([-0.0217,  0.1227])\n",
      "Epoch 1878,Loss 2.973151\n",
      "\tParams: tensor([  5.2406, -16.5852])\n",
      "\tGrad:tensor([-0.0216,  0.1224])\n",
      "Epoch 1879,Loss 2.972996\n",
      "\tParams: tensor([  5.2408, -16.5864])\n",
      "\tGrad:tensor([-0.0216,  0.1222])\n",
      "Epoch 1880,Loss 2.972843\n",
      "\tParams: tensor([  5.2410, -16.5877])\n",
      "\tGrad:tensor([-0.0215,  0.1220])\n",
      "Epoch 1881,Loss 2.972690\n",
      "\tParams: tensor([  5.2413, -16.5889])\n",
      "\tGrad:tensor([-0.0215,  0.1218])\n",
      "Epoch 1882,Loss 2.972536\n",
      "\tParams: tensor([  5.2415, -16.5901])\n",
      "\tGrad:tensor([-0.0215,  0.1216])\n",
      "Epoch 1883,Loss 2.972383\n",
      "\tParams: tensor([  5.2417, -16.5913])\n",
      "\tGrad:tensor([-0.0214,  0.1214])\n",
      "Epoch 1884,Loss 2.972232\n",
      "\tParams: tensor([  5.2419, -16.5925])\n",
      "\tGrad:tensor([-0.0214,  0.1212])\n",
      "Epoch 1885,Loss 2.972081\n",
      "\tParams: tensor([  5.2421, -16.5937])\n",
      "\tGrad:tensor([-0.0214,  0.1210])\n",
      "Epoch 1886,Loss 2.971931\n",
      "\tParams: tensor([  5.2423, -16.5949])\n",
      "\tGrad:tensor([-0.0213,  0.1208])\n",
      "Epoch 1887,Loss 2.971780\n",
      "\tParams: tensor([  5.2425, -16.5961])\n",
      "\tGrad:tensor([-0.0213,  0.1206])\n",
      "Epoch 1888,Loss 2.971630\n",
      "\tParams: tensor([  5.2427, -16.5974])\n",
      "\tGrad:tensor([-0.0213,  0.1204])\n",
      "Epoch 1889,Loss 2.971481\n",
      "\tParams: tensor([  5.2430, -16.5986])\n",
      "\tGrad:tensor([-0.0212,  0.1202])\n",
      "Epoch 1890,Loss 2.971332\n",
      "\tParams: tensor([  5.2432, -16.5998])\n",
      "\tGrad:tensor([-0.0212,  0.1200])\n",
      "Epoch 1891,Loss 2.971184\n",
      "\tParams: tensor([  5.2434, -16.6010])\n",
      "\tGrad:tensor([-0.0212,  0.1198])\n",
      "Epoch 1892,Loss 2.971035\n",
      "\tParams: tensor([  5.2436, -16.6021])\n",
      "\tGrad:tensor([-0.0211,  0.1196])\n",
      "Epoch 1893,Loss 2.970888\n",
      "\tParams: tensor([  5.2438, -16.6033])\n",
      "\tGrad:tensor([-0.0211,  0.1194])\n",
      "Epoch 1894,Loss 2.970741\n",
      "\tParams: tensor([  5.2440, -16.6045])\n",
      "\tGrad:tensor([-0.0211,  0.1192])\n",
      "Epoch 1895,Loss 2.970596\n",
      "\tParams: tensor([  5.2442, -16.6057])\n",
      "\tGrad:tensor([-0.0210,  0.1190])\n",
      "Epoch 1896,Loss 2.970449\n",
      "\tParams: tensor([  5.2444, -16.6069])\n",
      "\tGrad:tensor([-0.0210,  0.1188])\n",
      "Epoch 1897,Loss 2.970304\n",
      "\tParams: tensor([  5.2446, -16.6081])\n",
      "\tGrad:tensor([-0.0209,  0.1186])\n",
      "Epoch 1898,Loss 2.970159\n",
      "\tParams: tensor([  5.2449, -16.6093])\n",
      "\tGrad:tensor([-0.0209,  0.1183])\n",
      "Epoch 1899,Loss 2.970016\n",
      "\tParams: tensor([  5.2451, -16.6105])\n",
      "\tGrad:tensor([-0.0209,  0.1182])\n",
      "Epoch 1900,Loss 2.969871\n",
      "\tParams: tensor([  5.2453, -16.6116])\n",
      "\tGrad:tensor([-0.0208,  0.1180])\n",
      "Epoch 1901,Loss 2.969727\n",
      "\tParams: tensor([  5.2455, -16.6128])\n",
      "\tGrad:tensor([-0.0208,  0.1178])\n",
      "Epoch 1902,Loss 2.969586\n",
      "\tParams: tensor([  5.2457, -16.6140])\n",
      "\tGrad:tensor([-0.0208,  0.1175])\n",
      "Epoch 1903,Loss 2.969443\n",
      "\tParams: tensor([  5.2459, -16.6152])\n",
      "\tGrad:tensor([-0.0207,  0.1173])\n",
      "Epoch 1904,Loss 2.969302\n",
      "\tParams: tensor([  5.2461, -16.6163])\n",
      "\tGrad:tensor([-0.0207,  0.1172])\n",
      "Epoch 1905,Loss 2.969160\n",
      "\tParams: tensor([  5.2463, -16.6175])\n",
      "\tGrad:tensor([-0.0206,  0.1170])\n",
      "Epoch 1906,Loss 2.969017\n",
      "\tParams: tensor([  5.2465, -16.6187])\n",
      "\tGrad:tensor([-0.0206,  0.1168])\n",
      "Epoch 1907,Loss 2.968879\n",
      "\tParams: tensor([  5.2467, -16.6198])\n",
      "\tGrad:tensor([-0.0206,  0.1166])\n",
      "Epoch 1908,Loss 2.968739\n",
      "\tParams: tensor([  5.2469, -16.6210])\n",
      "\tGrad:tensor([-0.0205,  0.1164])\n",
      "Epoch 1909,Loss 2.968599\n",
      "\tParams: tensor([  5.2471, -16.6222])\n",
      "\tGrad:tensor([-0.0205,  0.1162])\n",
      "Epoch 1910,Loss 2.968460\n",
      "\tParams: tensor([  5.2473, -16.6233])\n",
      "\tGrad:tensor([-0.0205,  0.1160])\n",
      "Epoch 1911,Loss 2.968321\n",
      "\tParams: tensor([  5.2475, -16.6245])\n",
      "\tGrad:tensor([-0.0204,  0.1158])\n",
      "Epoch 1912,Loss 2.968183\n",
      "\tParams: tensor([  5.2477, -16.6256])\n",
      "\tGrad:tensor([-0.0204,  0.1156])\n",
      "Epoch 1913,Loss 2.968046\n",
      "\tParams: tensor([  5.2479, -16.6268])\n",
      "\tGrad:tensor([-0.0204,  0.1154])\n",
      "Epoch 1914,Loss 2.967908\n",
      "\tParams: tensor([  5.2482, -16.6279])\n",
      "\tGrad:tensor([-0.0204,  0.1152])\n",
      "Epoch 1915,Loss 2.967772\n",
      "\tParams: tensor([  5.2484, -16.6291])\n",
      "\tGrad:tensor([-0.0203,  0.1150])\n",
      "Epoch 1916,Loss 2.967636\n",
      "\tParams: tensor([  5.2486, -16.6302])\n",
      "\tGrad:tensor([-0.0203,  0.1148])\n",
      "Epoch 1917,Loss 2.967499\n",
      "\tParams: tensor([  5.2488, -16.6314])\n",
      "\tGrad:tensor([-0.0202,  0.1146])\n",
      "Epoch 1918,Loss 2.967365\n",
      "\tParams: tensor([  5.2490, -16.6325])\n",
      "\tGrad:tensor([-0.0202,  0.1144])\n",
      "Epoch 1919,Loss 2.967230\n",
      "\tParams: tensor([  5.2492, -16.6337])\n",
      "\tGrad:tensor([-0.0202,  0.1142])\n",
      "Epoch 1920,Loss 2.967095\n",
      "\tParams: tensor([  5.2494, -16.6348])\n",
      "\tGrad:tensor([-0.0202,  0.1140])\n",
      "Epoch 1921,Loss 2.966961\n",
      "\tParams: tensor([  5.2496, -16.6360])\n",
      "\tGrad:tensor([-0.0201,  0.1138])\n",
      "Epoch 1922,Loss 2.966828\n",
      "\tParams: tensor([  5.2498, -16.6371])\n",
      "\tGrad:tensor([-0.0201,  0.1136])\n",
      "Epoch 1923,Loss 2.966693\n",
      "\tParams: tensor([  5.2500, -16.6382])\n",
      "\tGrad:tensor([-0.0200,  0.1134])\n",
      "Epoch 1924,Loss 2.966561\n",
      "\tParams: tensor([  5.2502, -16.6394])\n",
      "\tGrad:tensor([-0.0200,  0.1132])\n",
      "Epoch 1925,Loss 2.966429\n",
      "\tParams: tensor([  5.2504, -16.6405])\n",
      "\tGrad:tensor([-0.0200,  0.1130])\n",
      "Epoch 1926,Loss 2.966297\n",
      "\tParams: tensor([  5.2506, -16.6416])\n",
      "\tGrad:tensor([-0.0199,  0.1128])\n",
      "Epoch 1927,Loss 2.966168\n",
      "\tParams: tensor([  5.2508, -16.6427])\n",
      "\tGrad:tensor([-0.0199,  0.1127])\n",
      "Epoch 1928,Loss 2.966036\n",
      "\tParams: tensor([  5.2510, -16.6439])\n",
      "\tGrad:tensor([-0.0199,  0.1125])\n",
      "Epoch 1929,Loss 2.965904\n",
      "\tParams: tensor([  5.2512, -16.6450])\n",
      "\tGrad:tensor([-0.0198,  0.1123])\n",
      "Epoch 1930,Loss 2.965777\n",
      "\tParams: tensor([  5.2514, -16.6461])\n",
      "\tGrad:tensor([-0.0198,  0.1121])\n",
      "Epoch 1931,Loss 2.965647\n",
      "\tParams: tensor([  5.2516, -16.6472])\n",
      "\tGrad:tensor([-0.0198,  0.1119])\n",
      "Epoch 1932,Loss 2.965516\n",
      "\tParams: tensor([  5.2518, -16.6484])\n",
      "\tGrad:tensor([-0.0197,  0.1117])\n",
      "Epoch 1933,Loss 2.965388\n",
      "\tParams: tensor([  5.2520, -16.6495])\n",
      "\tGrad:tensor([-0.0197,  0.1115])\n",
      "Epoch 1934,Loss 2.965261\n",
      "\tParams: tensor([  5.2522, -16.6506])\n",
      "\tGrad:tensor([-0.0197,  0.1113])\n",
      "Epoch 1935,Loss 2.965131\n",
      "\tParams: tensor([  5.2523, -16.6517])\n",
      "\tGrad:tensor([-0.0196,  0.1111])\n",
      "Epoch 1936,Loss 2.965006\n",
      "\tParams: tensor([  5.2525, -16.6528])\n",
      "\tGrad:tensor([-0.0196,  0.1109])\n",
      "Epoch 1937,Loss 2.964877\n",
      "\tParams: tensor([  5.2527, -16.6539])\n",
      "\tGrad:tensor([-0.0196,  0.1108])\n",
      "Epoch 1938,Loss 2.964751\n",
      "\tParams: tensor([  5.2529, -16.6550])\n",
      "\tGrad:tensor([-0.0195,  0.1106])\n",
      "Epoch 1939,Loss 2.964625\n",
      "\tParams: tensor([  5.2531, -16.6561])\n",
      "\tGrad:tensor([-0.0195,  0.1104])\n",
      "Epoch 1940,Loss 2.964500\n",
      "\tParams: tensor([  5.2533, -16.6572])\n",
      "\tGrad:tensor([-0.0195,  0.1102])\n",
      "Epoch 1941,Loss 2.964375\n",
      "\tParams: tensor([  5.2535, -16.6583])\n",
      "\tGrad:tensor([-0.0195,  0.1100])\n",
      "Epoch 1942,Loss 2.964250\n",
      "\tParams: tensor([  5.2537, -16.6594])\n",
      "\tGrad:tensor([-0.0194,  0.1098])\n",
      "Epoch 1943,Loss 2.964126\n",
      "\tParams: tensor([  5.2539, -16.6605])\n",
      "\tGrad:tensor([-0.0194,  0.1096])\n",
      "Epoch 1944,Loss 2.964001\n",
      "\tParams: tensor([  5.2541, -16.6616])\n",
      "\tGrad:tensor([-0.0194,  0.1094])\n",
      "Epoch 1945,Loss 2.963879\n",
      "\tParams: tensor([  5.2543, -16.6627])\n",
      "\tGrad:tensor([-0.0193,  0.1093])\n",
      "Epoch 1946,Loss 2.963756\n",
      "\tParams: tensor([  5.2545, -16.6638])\n",
      "\tGrad:tensor([-0.0193,  0.1091])\n",
      "Epoch 1947,Loss 2.963632\n",
      "\tParams: tensor([  5.2547, -16.6649])\n",
      "\tGrad:tensor([-0.0192,  0.1089])\n",
      "Epoch 1948,Loss 2.963511\n",
      "\tParams: tensor([  5.2549, -16.6660])\n",
      "\tGrad:tensor([-0.0192,  0.1087])\n",
      "Epoch 1949,Loss 2.963388\n",
      "\tParams: tensor([  5.2551, -16.6671])\n",
      "\tGrad:tensor([-0.0192,  0.1085])\n",
      "Epoch 1950,Loss 2.963266\n",
      "\tParams: tensor([  5.2553, -16.6681])\n",
      "\tGrad:tensor([-0.0191,  0.1083])\n",
      "Epoch 1951,Loss 2.963149\n",
      "\tParams: tensor([  5.2554, -16.6692])\n",
      "\tGrad:tensor([-0.0191,  0.1081])\n",
      "Epoch 1952,Loss 2.963026\n",
      "\tParams: tensor([  5.2556, -16.6703])\n",
      "\tGrad:tensor([-0.0191,  0.1080])\n",
      "Epoch 1953,Loss 2.962907\n",
      "\tParams: tensor([  5.2558, -16.6714])\n",
      "\tGrad:tensor([-0.0190,  0.1078])\n",
      "Epoch 1954,Loss 2.962788\n",
      "\tParams: tensor([  5.2560, -16.6725])\n",
      "\tGrad:tensor([-0.0190,  0.1076])\n",
      "Epoch 1955,Loss 2.962667\n",
      "\tParams: tensor([  5.2562, -16.6735])\n",
      "\tGrad:tensor([-0.0190,  0.1074])\n",
      "Epoch 1956,Loss 2.962547\n",
      "\tParams: tensor([  5.2564, -16.6746])\n",
      "\tGrad:tensor([-0.0189,  0.1072])\n",
      "Epoch 1957,Loss 2.962429\n",
      "\tParams: tensor([  5.2566, -16.6757])\n",
      "\tGrad:tensor([-0.0189,  0.1071])\n",
      "Epoch 1958,Loss 2.962312\n",
      "\tParams: tensor([  5.2568, -16.6767])\n",
      "\tGrad:tensor([-0.0189,  0.1069])\n",
      "Epoch 1959,Loss 2.962195\n",
      "\tParams: tensor([  5.2570, -16.6778])\n",
      "\tGrad:tensor([-0.0188,  0.1067])\n",
      "Epoch 1960,Loss 2.962078\n",
      "\tParams: tensor([  5.2572, -16.6789])\n",
      "\tGrad:tensor([-0.0188,  0.1065])\n",
      "Epoch 1961,Loss 2.961959\n",
      "\tParams: tensor([  5.2573, -16.6799])\n",
      "\tGrad:tensor([-0.0188,  0.1063])\n",
      "Epoch 1962,Loss 2.961843\n",
      "\tParams: tensor([  5.2575, -16.6810])\n",
      "\tGrad:tensor([-0.0187,  0.1062])\n",
      "Epoch 1963,Loss 2.961728\n",
      "\tParams: tensor([  5.2577, -16.6821])\n",
      "\tGrad:tensor([-0.0187,  0.1060])\n",
      "Epoch 1964,Loss 2.961611\n",
      "\tParams: tensor([  5.2579, -16.6831])\n",
      "\tGrad:tensor([-0.0187,  0.1058])\n",
      "Epoch 1965,Loss 2.961496\n",
      "\tParams: tensor([  5.2581, -16.6842])\n",
      "\tGrad:tensor([-0.0187,  0.1056])\n",
      "Epoch 1966,Loss 2.961382\n",
      "\tParams: tensor([  5.2583, -16.6852])\n",
      "\tGrad:tensor([-0.0186,  0.1054])\n",
      "Epoch 1967,Loss 2.961267\n",
      "\tParams: tensor([  5.2585, -16.6863])\n",
      "\tGrad:tensor([-0.0186,  0.1052])\n",
      "Epoch 1968,Loss 2.961153\n",
      "\tParams: tensor([  5.2586, -16.6873])\n",
      "\tGrad:tensor([-0.0186,  0.1051])\n",
      "Epoch 1969,Loss 2.961038\n",
      "\tParams: tensor([  5.2588, -16.6884])\n",
      "\tGrad:tensor([-0.0185,  0.1049])\n",
      "Epoch 1970,Loss 2.960926\n",
      "\tParams: tensor([  5.2590, -16.6894])\n",
      "\tGrad:tensor([-0.0185,  0.1047])\n",
      "Epoch 1971,Loss 2.960813\n",
      "\tParams: tensor([  5.2592, -16.6905])\n",
      "\tGrad:tensor([-0.0185,  0.1045])\n",
      "Epoch 1972,Loss 2.960700\n",
      "\tParams: tensor([  5.2594, -16.6915])\n",
      "\tGrad:tensor([-0.0184,  0.1044])\n",
      "Epoch 1973,Loss 2.960587\n",
      "\tParams: tensor([  5.2596, -16.6926])\n",
      "\tGrad:tensor([-0.0184,  0.1042])\n",
      "Epoch 1974,Loss 2.960475\n",
      "\tParams: tensor([  5.2598, -16.6936])\n",
      "\tGrad:tensor([-0.0184,  0.1040])\n",
      "Epoch 1975,Loss 2.960365\n",
      "\tParams: tensor([  5.2599, -16.6946])\n",
      "\tGrad:tensor([-0.0183,  0.1038])\n",
      "Epoch 1976,Loss 2.960255\n",
      "\tParams: tensor([  5.2601, -16.6957])\n",
      "\tGrad:tensor([-0.0183,  0.1037])\n",
      "Epoch 1977,Loss 2.960143\n",
      "\tParams: tensor([  5.2603, -16.6967])\n",
      "\tGrad:tensor([-0.0183,  0.1035])\n",
      "Epoch 1978,Loss 2.960033\n",
      "\tParams: tensor([  5.2605, -16.6977])\n",
      "\tGrad:tensor([-0.0182,  0.1033])\n",
      "Epoch 1979,Loss 2.959923\n",
      "\tParams: tensor([  5.2607, -16.6988])\n",
      "\tGrad:tensor([-0.0182,  0.1031])\n",
      "Epoch 1980,Loss 2.959812\n",
      "\tParams: tensor([  5.2608, -16.6998])\n",
      "\tGrad:tensor([-0.0182,  0.1029])\n",
      "Epoch 1981,Loss 2.959703\n",
      "\tParams: tensor([  5.2610, -16.7008])\n",
      "\tGrad:tensor([-0.0182,  0.1028])\n",
      "Epoch 1982,Loss 2.959594\n",
      "\tParams: tensor([  5.2612, -16.7019])\n",
      "\tGrad:tensor([-0.0181,  0.1026])\n",
      "Epoch 1983,Loss 2.959486\n",
      "\tParams: tensor([  5.2614, -16.7029])\n",
      "\tGrad:tensor([-0.0181,  0.1024])\n",
      "Epoch 1984,Loss 2.959378\n",
      "\tParams: tensor([  5.2616, -16.7039])\n",
      "\tGrad:tensor([-0.0181,  0.1022])\n",
      "Epoch 1985,Loss 2.959271\n",
      "\tParams: tensor([  5.2618, -16.7049])\n",
      "\tGrad:tensor([-0.0180,  0.1021])\n",
      "Epoch 1986,Loss 2.959162\n",
      "\tParams: tensor([  5.2619, -16.7059])\n",
      "\tGrad:tensor([-0.0180,  0.1019])\n",
      "Epoch 1987,Loss 2.959055\n",
      "\tParams: tensor([  5.2621, -16.7070])\n",
      "\tGrad:tensor([-0.0180,  0.1017])\n",
      "Epoch 1988,Loss 2.958950\n",
      "\tParams: tensor([  5.2623, -16.7080])\n",
      "\tGrad:tensor([-0.0179,  0.1016])\n",
      "Epoch 1989,Loss 2.958842\n",
      "\tParams: tensor([  5.2625, -16.7090])\n",
      "\tGrad:tensor([-0.0179,  0.1014])\n",
      "Epoch 1990,Loss 2.958738\n",
      "\tParams: tensor([  5.2626, -16.7100])\n",
      "\tGrad:tensor([-0.0179,  0.1012])\n",
      "Epoch 1991,Loss 2.958632\n",
      "\tParams: tensor([  5.2628, -16.7110])\n",
      "\tGrad:tensor([-0.0179,  0.1010])\n",
      "Epoch 1992,Loss 2.958526\n",
      "\tParams: tensor([  5.2630, -16.7120])\n",
      "\tGrad:tensor([-0.0178,  0.1009])\n",
      "Epoch 1993,Loss 2.958422\n",
      "\tParams: tensor([  5.2632, -16.7130])\n",
      "\tGrad:tensor([-0.0178,  0.1007])\n",
      "Epoch 1994,Loss 2.958317\n",
      "\tParams: tensor([  5.2634, -16.7140])\n",
      "\tGrad:tensor([-0.0178,  0.1005])\n",
      "Epoch 1995,Loss 2.958212\n",
      "\tParams: tensor([  5.2635, -16.7150])\n",
      "\tGrad:tensor([-0.0177,  0.1004])\n",
      "Epoch 1996,Loss 2.958109\n",
      "\tParams: tensor([  5.2637, -16.7160])\n",
      "\tGrad:tensor([-0.0177,  0.1002])\n",
      "Epoch 1997,Loss 2.958006\n",
      "\tParams: tensor([  5.2639, -16.7170])\n",
      "\tGrad:tensor([-0.0176,  0.1000])\n",
      "Epoch 1998,Loss 2.957904\n",
      "\tParams: tensor([  5.2641, -16.7180])\n",
      "\tGrad:tensor([-0.0176,  0.0998])\n",
      "Epoch 1999,Loss 2.957801\n",
      "\tParams: tensor([  5.2642, -16.7190])\n",
      "\tGrad:tensor([-0.0176,  0.0997])\n",
      "Epoch 2000,Loss 2.957698\n",
      "\tParams: tensor([  5.2644, -16.7200])\n",
      "\tGrad:tensor([-0.0176,  0.0995])\n",
      "Epoch 2001,Loss 2.957596\n",
      "\tParams: tensor([  5.2646, -16.7210])\n",
      "\tGrad:tensor([-0.0176,  0.0993])\n",
      "Epoch 2002,Loss 2.957494\n",
      "\tParams: tensor([  5.2648, -16.7220])\n",
      "\tGrad:tensor([-0.0175,  0.0992])\n",
      "Epoch 2003,Loss 2.957393\n",
      "\tParams: tensor([  5.2649, -16.7230])\n",
      "\tGrad:tensor([-0.0175,  0.0990])\n",
      "Epoch 2004,Loss 2.957292\n",
      "\tParams: tensor([  5.2651, -16.7240])\n",
      "\tGrad:tensor([-0.0174,  0.0988])\n",
      "Epoch 2005,Loss 2.957193\n",
      "\tParams: tensor([  5.2653, -16.7250])\n",
      "\tGrad:tensor([-0.0174,  0.0987])\n",
      "Epoch 2006,Loss 2.957091\n",
      "\tParams: tensor([  5.2655, -16.7260])\n",
      "\tGrad:tensor([-0.0174,  0.0985])\n",
      "Epoch 2007,Loss 2.956992\n",
      "\tParams: tensor([  5.2656, -16.7269])\n",
      "\tGrad:tensor([-0.0174,  0.0983])\n",
      "Epoch 2008,Loss 2.956892\n",
      "\tParams: tensor([  5.2658, -16.7279])\n",
      "\tGrad:tensor([-0.0173,  0.0982])\n",
      "Epoch 2009,Loss 2.956792\n",
      "\tParams: tensor([  5.2660, -16.7289])\n",
      "\tGrad:tensor([-0.0173,  0.0980])\n",
      "Epoch 2010,Loss 2.956694\n",
      "\tParams: tensor([  5.2662, -16.7299])\n",
      "\tGrad:tensor([-0.0173,  0.0978])\n",
      "Epoch 2011,Loss 2.956595\n",
      "\tParams: tensor([  5.2663, -16.7309])\n",
      "\tGrad:tensor([-0.0172,  0.0977])\n",
      "Epoch 2012,Loss 2.956496\n",
      "\tParams: tensor([  5.2665, -16.7318])\n",
      "\tGrad:tensor([-0.0172,  0.0975])\n",
      "Epoch 2013,Loss 2.956397\n",
      "\tParams: tensor([  5.2667, -16.7328])\n",
      "\tGrad:tensor([-0.0172,  0.0973])\n",
      "Epoch 2014,Loss 2.956300\n",
      "\tParams: tensor([  5.2668, -16.7338])\n",
      "\tGrad:tensor([-0.0172,  0.0972])\n",
      "Epoch 2015,Loss 2.956204\n",
      "\tParams: tensor([  5.2670, -16.7348])\n",
      "\tGrad:tensor([-0.0171,  0.0970])\n",
      "Epoch 2016,Loss 2.956108\n",
      "\tParams: tensor([  5.2672, -16.7357])\n",
      "\tGrad:tensor([-0.0171,  0.0968])\n",
      "Epoch 2017,Loss 2.956010\n",
      "\tParams: tensor([  5.2674, -16.7367])\n",
      "\tGrad:tensor([-0.0171,  0.0967])\n",
      "Epoch 2018,Loss 2.955914\n",
      "\tParams: tensor([  5.2675, -16.7377])\n",
      "\tGrad:tensor([-0.0171,  0.0965])\n",
      "Epoch 2019,Loss 2.955817\n",
      "\tParams: tensor([  5.2677, -16.7386])\n",
      "\tGrad:tensor([-0.0170,  0.0963])\n",
      "Epoch 2020,Loss 2.955722\n",
      "\tParams: tensor([  5.2679, -16.7396])\n",
      "\tGrad:tensor([-0.0170,  0.0962])\n",
      "Epoch 2021,Loss 2.955627\n",
      "\tParams: tensor([  5.2680, -16.7405])\n",
      "\tGrad:tensor([-0.0170,  0.0960])\n",
      "Epoch 2022,Loss 2.955533\n",
      "\tParams: tensor([  5.2682, -16.7415])\n",
      "\tGrad:tensor([-0.0169,  0.0959])\n",
      "Epoch 2023,Loss 2.955436\n",
      "\tParams: tensor([  5.2684, -16.7425])\n",
      "\tGrad:tensor([-0.0169,  0.0957])\n",
      "Epoch 2024,Loss 2.955343\n",
      "\tParams: tensor([  5.2686, -16.7434])\n",
      "\tGrad:tensor([-0.0169,  0.0955])\n",
      "Epoch 2025,Loss 2.955250\n",
      "\tParams: tensor([  5.2687, -16.7444])\n",
      "\tGrad:tensor([-0.0169,  0.0954])\n",
      "Epoch 2026,Loss 2.955154\n",
      "\tParams: tensor([  5.2689, -16.7453])\n",
      "\tGrad:tensor([-0.0168,  0.0952])\n",
      "Epoch 2027,Loss 2.955062\n",
      "\tParams: tensor([  5.2691, -16.7463])\n",
      "\tGrad:tensor([-0.0168,  0.0950])\n",
      "Epoch 2028,Loss 2.954969\n",
      "\tParams: tensor([  5.2692, -16.7472])\n",
      "\tGrad:tensor([-0.0168,  0.0949])\n",
      "Epoch 2029,Loss 2.954875\n",
      "\tParams: tensor([  5.2694, -16.7482])\n",
      "\tGrad:tensor([-0.0167,  0.0947])\n",
      "Epoch 2030,Loss 2.954783\n",
      "\tParams: tensor([  5.2696, -16.7491])\n",
      "\tGrad:tensor([-0.0167,  0.0946])\n",
      "Epoch 2031,Loss 2.954691\n",
      "\tParams: tensor([  5.2697, -16.7501])\n",
      "\tGrad:tensor([-0.0167,  0.0944])\n",
      "Epoch 2032,Loss 2.954600\n",
      "\tParams: tensor([  5.2699, -16.7510])\n",
      "\tGrad:tensor([-0.0167,  0.0942])\n",
      "Epoch 2033,Loss 2.954507\n",
      "\tParams: tensor([  5.2701, -16.7519])\n",
      "\tGrad:tensor([-0.0166,  0.0941])\n",
      "Epoch 2034,Loss 2.954417\n",
      "\tParams: tensor([  5.2702, -16.7529])\n",
      "\tGrad:tensor([-0.0166,  0.0939])\n",
      "Epoch 2035,Loss 2.954326\n",
      "\tParams: tensor([  5.2704, -16.7538])\n",
      "\tGrad:tensor([-0.0165,  0.0938])\n",
      "Epoch 2036,Loss 2.954235\n",
      "\tParams: tensor([  5.2706, -16.7547])\n",
      "\tGrad:tensor([-0.0165,  0.0936])\n",
      "Epoch 2037,Loss 2.954145\n",
      "\tParams: tensor([  5.2707, -16.7557])\n",
      "\tGrad:tensor([-0.0165,  0.0934])\n",
      "Epoch 2038,Loss 2.954055\n",
      "\tParams: tensor([  5.2709, -16.7566])\n",
      "\tGrad:tensor([-0.0165,  0.0933])\n",
      "Epoch 2039,Loss 2.953966\n",
      "\tParams: tensor([  5.2710, -16.7575])\n",
      "\tGrad:tensor([-0.0164,  0.0931])\n",
      "Epoch 2040,Loss 2.953876\n",
      "\tParams: tensor([  5.2712, -16.7585])\n",
      "\tGrad:tensor([-0.0164,  0.0930])\n",
      "Epoch 2041,Loss 2.953787\n",
      "\tParams: tensor([  5.2714, -16.7594])\n",
      "\tGrad:tensor([-0.0164,  0.0928])\n",
      "Epoch 2042,Loss 2.953698\n",
      "\tParams: tensor([  5.2715, -16.7603])\n",
      "\tGrad:tensor([-0.0164,  0.0926])\n",
      "Epoch 2043,Loss 2.953610\n",
      "\tParams: tensor([  5.2717, -16.7613])\n",
      "\tGrad:tensor([-0.0163,  0.0925])\n",
      "Epoch 2044,Loss 2.953521\n",
      "\tParams: tensor([  5.2719, -16.7622])\n",
      "\tGrad:tensor([-0.0163,  0.0923])\n",
      "Epoch 2045,Loss 2.953434\n",
      "\tParams: tensor([  5.2720, -16.7631])\n",
      "\tGrad:tensor([-0.0163,  0.0922])\n",
      "Epoch 2046,Loss 2.953346\n",
      "\tParams: tensor([  5.2722, -16.7640])\n",
      "\tGrad:tensor([-0.0163,  0.0920])\n",
      "Epoch 2047,Loss 2.953259\n",
      "\tParams: tensor([  5.2724, -16.7649])\n",
      "\tGrad:tensor([-0.0162,  0.0919])\n",
      "Epoch 2048,Loss 2.953171\n",
      "\tParams: tensor([  5.2725, -16.7659])\n",
      "\tGrad:tensor([-0.0162,  0.0917])\n",
      "Epoch 2049,Loss 2.953085\n",
      "\tParams: tensor([  5.2727, -16.7668])\n",
      "\tGrad:tensor([-0.0162,  0.0915])\n",
      "Epoch 2050,Loss 2.953000\n",
      "\tParams: tensor([  5.2728, -16.7677])\n",
      "\tGrad:tensor([-0.0162,  0.0914])\n",
      "Epoch 2051,Loss 2.952913\n",
      "\tParams: tensor([  5.2730, -16.7686])\n",
      "\tGrad:tensor([-0.0161,  0.0912])\n",
      "Epoch 2052,Loss 2.952828\n",
      "\tParams: tensor([  5.2732, -16.7695])\n",
      "\tGrad:tensor([-0.0161,  0.0911])\n",
      "Epoch 2053,Loss 2.952742\n",
      "\tParams: tensor([  5.2733, -16.7704])\n",
      "\tGrad:tensor([-0.0161,  0.0909])\n",
      "Epoch 2054,Loss 2.952657\n",
      "\tParams: tensor([  5.2735, -16.7713])\n",
      "\tGrad:tensor([-0.0160,  0.0908])\n",
      "Epoch 2055,Loss 2.952571\n",
      "\tParams: tensor([  5.2736, -16.7722])\n",
      "\tGrad:tensor([-0.0160,  0.0906])\n",
      "Epoch 2056,Loss 2.952487\n",
      "\tParams: tensor([  5.2738, -16.7731])\n",
      "\tGrad:tensor([-0.0160,  0.0905])\n",
      "Epoch 2057,Loss 2.952403\n",
      "\tParams: tensor([  5.2740, -16.7740])\n",
      "\tGrad:tensor([-0.0160,  0.0903])\n",
      "Epoch 2058,Loss 2.952318\n",
      "\tParams: tensor([  5.2741, -16.7749])\n",
      "\tGrad:tensor([-0.0159,  0.0902])\n",
      "Epoch 2059,Loss 2.952235\n",
      "\tParams: tensor([  5.2743, -16.7758])\n",
      "\tGrad:tensor([-0.0159,  0.0900])\n",
      "Epoch 2060,Loss 2.952152\n",
      "\tParams: tensor([  5.2744, -16.7767])\n",
      "\tGrad:tensor([-0.0159,  0.0899])\n",
      "Epoch 2061,Loss 2.952068\n",
      "\tParams: tensor([  5.2746, -16.7776])\n",
      "\tGrad:tensor([-0.0158,  0.0897])\n",
      "Epoch 2062,Loss 2.951985\n",
      "\tParams: tensor([  5.2748, -16.7785])\n",
      "\tGrad:tensor([-0.0158,  0.0895])\n",
      "Epoch 2063,Loss 2.951902\n",
      "\tParams: tensor([  5.2749, -16.7794])\n",
      "\tGrad:tensor([-0.0158,  0.0894])\n",
      "Epoch 2064,Loss 2.951820\n",
      "\tParams: tensor([  5.2751, -16.7803])\n",
      "\tGrad:tensor([-0.0158,  0.0892])\n",
      "Epoch 2065,Loss 2.951738\n",
      "\tParams: tensor([  5.2752, -16.7812])\n",
      "\tGrad:tensor([-0.0157,  0.0891])\n",
      "Epoch 2066,Loss 2.951656\n",
      "\tParams: tensor([  5.2754, -16.7821])\n",
      "\tGrad:tensor([-0.0157,  0.0889])\n",
      "Epoch 2067,Loss 2.951576\n",
      "\tParams: tensor([  5.2755, -16.7830])\n",
      "\tGrad:tensor([-0.0157,  0.0888])\n",
      "Epoch 2068,Loss 2.951494\n",
      "\tParams: tensor([  5.2757, -16.7839])\n",
      "\tGrad:tensor([-0.0157,  0.0886])\n",
      "Epoch 2069,Loss 2.951413\n",
      "\tParams: tensor([  5.2759, -16.7848])\n",
      "\tGrad:tensor([-0.0157,  0.0885])\n",
      "Epoch 2070,Loss 2.951333\n",
      "\tParams: tensor([  5.2760, -16.7856])\n",
      "\tGrad:tensor([-0.0156,  0.0883])\n",
      "Epoch 2071,Loss 2.951252\n",
      "\tParams: tensor([  5.2762, -16.7865])\n",
      "\tGrad:tensor([-0.0156,  0.0882])\n",
      "Epoch 2072,Loss 2.951171\n",
      "\tParams: tensor([  5.2763, -16.7874])\n",
      "\tGrad:tensor([-0.0155,  0.0880])\n",
      "Epoch 2073,Loss 2.951093\n",
      "\tParams: tensor([  5.2765, -16.7883])\n",
      "\tGrad:tensor([-0.0155,  0.0879])\n",
      "Epoch 2074,Loss 2.951012\n",
      "\tParams: tensor([  5.2766, -16.7892])\n",
      "\tGrad:tensor([-0.0155,  0.0877])\n",
      "Epoch 2075,Loss 2.950932\n",
      "\tParams: tensor([  5.2768, -16.7900])\n",
      "\tGrad:tensor([-0.0155,  0.0876])\n",
      "Epoch 2076,Loss 2.950853\n",
      "\tParams: tensor([  5.2769, -16.7909])\n",
      "\tGrad:tensor([-0.0154,  0.0874])\n",
      "Epoch 2077,Loss 2.950774\n",
      "\tParams: tensor([  5.2771, -16.7918])\n",
      "\tGrad:tensor([-0.0154,  0.0873])\n",
      "Epoch 2078,Loss 2.950697\n",
      "\tParams: tensor([  5.2772, -16.7927])\n",
      "\tGrad:tensor([-0.0154,  0.0871])\n",
      "Epoch 2079,Loss 2.950618\n",
      "\tParams: tensor([  5.2774, -16.7935])\n",
      "\tGrad:tensor([-0.0154,  0.0870])\n",
      "Epoch 2080,Loss 2.950540\n",
      "\tParams: tensor([  5.2776, -16.7944])\n",
      "\tGrad:tensor([-0.0154,  0.0868])\n",
      "Epoch 2081,Loss 2.950463\n",
      "\tParams: tensor([  5.2777, -16.7953])\n",
      "\tGrad:tensor([-0.0153,  0.0867])\n",
      "Epoch 2082,Loss 2.950385\n",
      "\tParams: tensor([  5.2779, -16.7961])\n",
      "\tGrad:tensor([-0.0153,  0.0866])\n",
      "Epoch 2083,Loss 2.950308\n",
      "\tParams: tensor([  5.2780, -16.7970])\n",
      "\tGrad:tensor([-0.0153,  0.0864])\n",
      "Epoch 2084,Loss 2.950231\n",
      "\tParams: tensor([  5.2782, -16.7979])\n",
      "\tGrad:tensor([-0.0152,  0.0863])\n",
      "Epoch 2085,Loss 2.950154\n",
      "\tParams: tensor([  5.2783, -16.7987])\n",
      "\tGrad:tensor([-0.0152,  0.0861])\n",
      "Epoch 2086,Loss 2.950078\n",
      "\tParams: tensor([  5.2785, -16.7996])\n",
      "\tGrad:tensor([-0.0152,  0.0860])\n",
      "Epoch 2087,Loss 2.950003\n",
      "\tParams: tensor([  5.2786, -16.8004])\n",
      "\tGrad:tensor([-0.0152,  0.0858])\n",
      "Epoch 2088,Loss 2.949925\n",
      "\tParams: tensor([  5.2788, -16.8013])\n",
      "\tGrad:tensor([-0.0152,  0.0857])\n",
      "Epoch 2089,Loss 2.949850\n",
      "\tParams: tensor([  5.2789, -16.8021])\n",
      "\tGrad:tensor([-0.0151,  0.0855])\n",
      "Epoch 2090,Loss 2.949776\n",
      "\tParams: tensor([  5.2791, -16.8030])\n",
      "\tGrad:tensor([-0.0151,  0.0854])\n",
      "Epoch 2091,Loss 2.949699\n",
      "\tParams: tensor([  5.2792, -16.8039])\n",
      "\tGrad:tensor([-0.0151,  0.0852])\n",
      "Epoch 2092,Loss 2.949626\n",
      "\tParams: tensor([  5.2794, -16.8047])\n",
      "\tGrad:tensor([-0.0150,  0.0851])\n",
      "Epoch 2093,Loss 2.949550\n",
      "\tParams: tensor([  5.2795, -16.8056])\n",
      "\tGrad:tensor([-0.0150,  0.0850])\n",
      "Epoch 2094,Loss 2.949476\n",
      "\tParams: tensor([  5.2797, -16.8064])\n",
      "\tGrad:tensor([-0.0150,  0.0848])\n",
      "Epoch 2095,Loss 2.949401\n",
      "\tParams: tensor([  5.2798, -16.8072])\n",
      "\tGrad:tensor([-0.0149,  0.0847])\n",
      "Epoch 2096,Loss 2.949328\n",
      "\tParams: tensor([  5.2800, -16.8081])\n",
      "\tGrad:tensor([-0.0150,  0.0845])\n",
      "Epoch 2097,Loss 2.949254\n",
      "\tParams: tensor([  5.2801, -16.8089])\n",
      "\tGrad:tensor([-0.0149,  0.0844])\n",
      "Epoch 2098,Loss 2.949182\n",
      "\tParams: tensor([  5.2803, -16.8098])\n",
      "\tGrad:tensor([-0.0149,  0.0842])\n",
      "Epoch 2099,Loss 2.949108\n",
      "\tParams: tensor([  5.2804, -16.8106])\n",
      "\tGrad:tensor([-0.0149,  0.0841])\n",
      "Epoch 2100,Loss 2.949035\n",
      "\tParams: tensor([  5.2806, -16.8115])\n",
      "\tGrad:tensor([-0.0148,  0.0839])\n",
      "Epoch 2101,Loss 2.948962\n",
      "\tParams: tensor([  5.2807, -16.8123])\n",
      "\tGrad:tensor([-0.0148,  0.0838])\n",
      "Epoch 2102,Loss 2.948890\n",
      "\tParams: tensor([  5.2809, -16.8131])\n",
      "\tGrad:tensor([-0.0148,  0.0837])\n",
      "Epoch 2103,Loss 2.948818\n",
      "\tParams: tensor([  5.2810, -16.8140])\n",
      "\tGrad:tensor([-0.0148,  0.0835])\n",
      "Epoch 2104,Loss 2.948745\n",
      "\tParams: tensor([  5.2812, -16.8148])\n",
      "\tGrad:tensor([-0.0148,  0.0834])\n",
      "Epoch 2105,Loss 2.948675\n",
      "\tParams: tensor([  5.2813, -16.8156])\n",
      "\tGrad:tensor([-0.0147,  0.0832])\n",
      "Epoch 2106,Loss 2.948602\n",
      "\tParams: tensor([  5.2815, -16.8165])\n",
      "\tGrad:tensor([-0.0147,  0.0831])\n",
      "Epoch 2107,Loss 2.948532\n",
      "\tParams: tensor([  5.2816, -16.8173])\n",
      "\tGrad:tensor([-0.0146,  0.0830])\n",
      "Epoch 2108,Loss 2.948462\n",
      "\tParams: tensor([  5.2817, -16.8181])\n",
      "\tGrad:tensor([-0.0146,  0.0828])\n",
      "Epoch 2109,Loss 2.948391\n",
      "\tParams: tensor([  5.2819, -16.8189])\n",
      "\tGrad:tensor([-0.0146,  0.0827])\n",
      "Epoch 2110,Loss 2.948321\n",
      "\tParams: tensor([  5.2820, -16.8198])\n",
      "\tGrad:tensor([-0.0146,  0.0825])\n",
      "Epoch 2111,Loss 2.948250\n",
      "\tParams: tensor([  5.2822, -16.8206])\n",
      "\tGrad:tensor([-0.0145,  0.0824])\n",
      "Epoch 2112,Loss 2.948181\n",
      "\tParams: tensor([  5.2823, -16.8214])\n",
      "\tGrad:tensor([-0.0145,  0.0823])\n",
      "Epoch 2113,Loss 2.948109\n",
      "\tParams: tensor([  5.2825, -16.8222])\n",
      "\tGrad:tensor([-0.0145,  0.0821])\n",
      "Epoch 2114,Loss 2.948041\n",
      "\tParams: tensor([  5.2826, -16.8231])\n",
      "\tGrad:tensor([-0.0145,  0.0820])\n",
      "Epoch 2115,Loss 2.947971\n",
      "\tParams: tensor([  5.2828, -16.8239])\n",
      "\tGrad:tensor([-0.0144,  0.0818])\n",
      "Epoch 2116,Loss 2.947902\n",
      "\tParams: tensor([  5.2829, -16.8247])\n",
      "\tGrad:tensor([-0.0144,  0.0817])\n",
      "Epoch 2117,Loss 2.947833\n",
      "\tParams: tensor([  5.2831, -16.8255])\n",
      "\tGrad:tensor([-0.0144,  0.0816])\n",
      "Epoch 2118,Loss 2.947765\n",
      "\tParams: tensor([  5.2832, -16.8263])\n",
      "\tGrad:tensor([-0.0144,  0.0814])\n",
      "Epoch 2119,Loss 2.947696\n",
      "\tParams: tensor([  5.2833, -16.8271])\n",
      "\tGrad:tensor([-0.0144,  0.0813])\n",
      "Epoch 2120,Loss 2.947628\n",
      "\tParams: tensor([  5.2835, -16.8280])\n",
      "\tGrad:tensor([-0.0143,  0.0811])\n",
      "Epoch 2121,Loss 2.947560\n",
      "\tParams: tensor([  5.2836, -16.8288])\n",
      "\tGrad:tensor([-0.0143,  0.0810])\n",
      "Epoch 2122,Loss 2.947494\n",
      "\tParams: tensor([  5.2838, -16.8296])\n",
      "\tGrad:tensor([-0.0143,  0.0809])\n",
      "Epoch 2123,Loss 2.947426\n",
      "\tParams: tensor([  5.2839, -16.8304])\n",
      "\tGrad:tensor([-0.0143,  0.0807])\n",
      "Epoch 2124,Loss 2.947357\n",
      "\tParams: tensor([  5.2841, -16.8312])\n",
      "\tGrad:tensor([-0.0142,  0.0806])\n",
      "Epoch 2125,Loss 2.947293\n",
      "\tParams: tensor([  5.2842, -16.8320])\n",
      "\tGrad:tensor([-0.0142,  0.0805])\n",
      "Epoch 2126,Loss 2.947225\n",
      "\tParams: tensor([  5.2843, -16.8328])\n",
      "\tGrad:tensor([-0.0142,  0.0803])\n",
      "Epoch 2127,Loss 2.947158\n",
      "\tParams: tensor([  5.2845, -16.8336])\n",
      "\tGrad:tensor([-0.0142,  0.0802])\n",
      "Epoch 2128,Loss 2.947092\n",
      "\tParams: tensor([  5.2846, -16.8344])\n",
      "\tGrad:tensor([-0.0141,  0.0800])\n",
      "Epoch 2129,Loss 2.947026\n",
      "\tParams: tensor([  5.2848, -16.8352])\n",
      "\tGrad:tensor([-0.0141,  0.0799])\n",
      "Epoch 2130,Loss 2.946960\n",
      "\tParams: tensor([  5.2849, -16.8360])\n",
      "\tGrad:tensor([-0.0141,  0.0798])\n",
      "Epoch 2131,Loss 2.946895\n",
      "\tParams: tensor([  5.2850, -16.8368])\n",
      "\tGrad:tensor([-0.0141,  0.0796])\n",
      "Epoch 2132,Loss 2.946830\n",
      "\tParams: tensor([  5.2852, -16.8376])\n",
      "\tGrad:tensor([-0.0141,  0.0795])\n",
      "Epoch 2133,Loss 2.946764\n",
      "\tParams: tensor([  5.2853, -16.8384])\n",
      "\tGrad:tensor([-0.0140,  0.0794])\n",
      "Epoch 2134,Loss 2.946700\n",
      "\tParams: tensor([  5.2855, -16.8392])\n",
      "\tGrad:tensor([-0.0140,  0.0792])\n",
      "Epoch 2135,Loss 2.946635\n",
      "\tParams: tensor([  5.2856, -16.8400])\n",
      "\tGrad:tensor([-0.0140,  0.0791])\n",
      "Epoch 2136,Loss 2.946571\n",
      "\tParams: tensor([  5.2857, -16.8407])\n",
      "\tGrad:tensor([-0.0139,  0.0790])\n",
      "Epoch 2137,Loss 2.946507\n",
      "\tParams: tensor([  5.2859, -16.8415])\n",
      "\tGrad:tensor([-0.0139,  0.0788])\n",
      "Epoch 2138,Loss 2.946442\n",
      "\tParams: tensor([  5.2860, -16.8423])\n",
      "\tGrad:tensor([-0.0139,  0.0787])\n",
      "Epoch 2139,Loss 2.946378\n",
      "\tParams: tensor([  5.2862, -16.8431])\n",
      "\tGrad:tensor([-0.0139,  0.0786])\n",
      "Epoch 2140,Loss 2.946314\n",
      "\tParams: tensor([  5.2863, -16.8439])\n",
      "\tGrad:tensor([-0.0138,  0.0784])\n",
      "Epoch 2141,Loss 2.946251\n",
      "\tParams: tensor([  5.2864, -16.8447])\n",
      "\tGrad:tensor([-0.0138,  0.0783])\n",
      "Epoch 2142,Loss 2.946189\n",
      "\tParams: tensor([  5.2866, -16.8455])\n",
      "\tGrad:tensor([-0.0138,  0.0782])\n",
      "Epoch 2143,Loss 2.946126\n",
      "\tParams: tensor([  5.2867, -16.8462])\n",
      "\tGrad:tensor([-0.0138,  0.0780])\n",
      "Epoch 2144,Loss 2.946063\n",
      "\tParams: tensor([  5.2869, -16.8470])\n",
      "\tGrad:tensor([-0.0138,  0.0779])\n",
      "Epoch 2145,Loss 2.946001\n",
      "\tParams: tensor([  5.2870, -16.8478])\n",
      "\tGrad:tensor([-0.0137,  0.0778])\n",
      "Epoch 2146,Loss 2.945937\n",
      "\tParams: tensor([  5.2871, -16.8486])\n",
      "\tGrad:tensor([-0.0137,  0.0776])\n",
      "Epoch 2147,Loss 2.945876\n",
      "\tParams: tensor([  5.2873, -16.8493])\n",
      "\tGrad:tensor([-0.0137,  0.0775])\n",
      "Epoch 2148,Loss 2.945815\n",
      "\tParams: tensor([  5.2874, -16.8501])\n",
      "\tGrad:tensor([-0.0137,  0.0774])\n",
      "Epoch 2149,Loss 2.945753\n",
      "\tParams: tensor([  5.2875, -16.8509])\n",
      "\tGrad:tensor([-0.0136,  0.0772])\n",
      "Epoch 2150,Loss 2.945690\n",
      "\tParams: tensor([  5.2877, -16.8517])\n",
      "\tGrad:tensor([-0.0136,  0.0771])\n",
      "Epoch 2151,Loss 2.945630\n",
      "\tParams: tensor([  5.2878, -16.8524])\n",
      "\tGrad:tensor([-0.0136,  0.0770])\n",
      "Epoch 2152,Loss 2.945567\n",
      "\tParams: tensor([  5.2879, -16.8532])\n",
      "\tGrad:tensor([-0.0136,  0.0768])\n",
      "Epoch 2153,Loss 2.945508\n",
      "\tParams: tensor([  5.2881, -16.8540])\n",
      "\tGrad:tensor([-0.0135,  0.0767])\n",
      "Epoch 2154,Loss 2.945447\n",
      "\tParams: tensor([  5.2882, -16.8547])\n",
      "\tGrad:tensor([-0.0135,  0.0766])\n",
      "Epoch 2155,Loss 2.945385\n",
      "\tParams: tensor([  5.2884, -16.8555])\n",
      "\tGrad:tensor([-0.0135,  0.0765])\n",
      "Epoch 2156,Loss 2.945325\n",
      "\tParams: tensor([  5.2885, -16.8563])\n",
      "\tGrad:tensor([-0.0135,  0.0763])\n",
      "Epoch 2157,Loss 2.945267\n",
      "\tParams: tensor([  5.2886, -16.8570])\n",
      "\tGrad:tensor([-0.0135,  0.0762])\n",
      "Epoch 2158,Loss 2.945206\n",
      "\tParams: tensor([  5.2888, -16.8578])\n",
      "\tGrad:tensor([-0.0134,  0.0761])\n",
      "Epoch 2159,Loss 2.945146\n",
      "\tParams: tensor([  5.2889, -16.8585])\n",
      "\tGrad:tensor([-0.0134,  0.0759])\n",
      "Epoch 2160,Loss 2.945088\n",
      "\tParams: tensor([  5.2890, -16.8593])\n",
      "\tGrad:tensor([-0.0134,  0.0758])\n",
      "Epoch 2161,Loss 2.945028\n",
      "\tParams: tensor([  5.2892, -16.8601])\n",
      "\tGrad:tensor([-0.0134,  0.0757])\n",
      "Epoch 2162,Loss 2.944969\n",
      "\tParams: tensor([  5.2893, -16.8608])\n",
      "\tGrad:tensor([-0.0133,  0.0755])\n",
      "Epoch 2163,Loss 2.944911\n",
      "\tParams: tensor([  5.2894, -16.8616])\n",
      "\tGrad:tensor([-0.0133,  0.0754])\n",
      "Epoch 2164,Loss 2.944852\n",
      "\tParams: tensor([  5.2896, -16.8623])\n",
      "\tGrad:tensor([-0.0133,  0.0753])\n",
      "Epoch 2165,Loss 2.944792\n",
      "\tParams: tensor([  5.2897, -16.8631])\n",
      "\tGrad:tensor([-0.0133,  0.0752])\n",
      "Epoch 2166,Loss 2.944736\n",
      "\tParams: tensor([  5.2898, -16.8638])\n",
      "\tGrad:tensor([-0.0133,  0.0750])\n",
      "Epoch 2167,Loss 2.944678\n",
      "\tParams: tensor([  5.2900, -16.8646])\n",
      "\tGrad:tensor([-0.0132,  0.0749])\n",
      "Epoch 2168,Loss 2.944619\n",
      "\tParams: tensor([  5.2901, -16.8653])\n",
      "\tGrad:tensor([-0.0132,  0.0748])\n",
      "Epoch 2169,Loss 2.944562\n",
      "\tParams: tensor([  5.2902, -16.8661])\n",
      "\tGrad:tensor([-0.0132,  0.0747])\n",
      "Epoch 2170,Loss 2.944504\n",
      "\tParams: tensor([  5.2903, -16.8668])\n",
      "\tGrad:tensor([-0.0132,  0.0745])\n",
      "Epoch 2171,Loss 2.944447\n",
      "\tParams: tensor([  5.2905, -16.8676])\n",
      "\tGrad:tensor([-0.0132,  0.0744])\n",
      "Epoch 2172,Loss 2.944391\n",
      "\tParams: tensor([  5.2906, -16.8683])\n",
      "\tGrad:tensor([-0.0131,  0.0743])\n",
      "Epoch 2173,Loss 2.944332\n",
      "\tParams: tensor([  5.2907, -16.8690])\n",
      "\tGrad:tensor([-0.0131,  0.0742])\n",
      "Epoch 2174,Loss 2.944276\n",
      "\tParams: tensor([  5.2909, -16.8698])\n",
      "\tGrad:tensor([-0.0131,  0.0740])\n",
      "Epoch 2175,Loss 2.944220\n",
      "\tParams: tensor([  5.2910, -16.8705])\n",
      "\tGrad:tensor([-0.0131,  0.0739])\n",
      "Epoch 2176,Loss 2.944164\n",
      "\tParams: tensor([  5.2911, -16.8713])\n",
      "\tGrad:tensor([-0.0130,  0.0738])\n",
      "Epoch 2177,Loss 2.944108\n",
      "\tParams: tensor([  5.2913, -16.8720])\n",
      "\tGrad:tensor([-0.0130,  0.0736])\n",
      "Epoch 2178,Loss 2.944053\n",
      "\tParams: tensor([  5.2914, -16.8727])\n",
      "\tGrad:tensor([-0.0130,  0.0735])\n",
      "Epoch 2179,Loss 2.943996\n",
      "\tParams: tensor([  5.2915, -16.8735])\n",
      "\tGrad:tensor([-0.0130,  0.0734])\n",
      "Epoch 2180,Loss 2.943941\n",
      "\tParams: tensor([  5.2917, -16.8742])\n",
      "\tGrad:tensor([-0.0129,  0.0733])\n",
      "Epoch 2181,Loss 2.943887\n",
      "\tParams: tensor([  5.2918, -16.8749])\n",
      "\tGrad:tensor([-0.0129,  0.0731])\n",
      "Epoch 2182,Loss 2.943831\n",
      "\tParams: tensor([  5.2919, -16.8757])\n",
      "\tGrad:tensor([-0.0129,  0.0730])\n",
      "Epoch 2183,Loss 2.943776\n",
      "\tParams: tensor([  5.2920, -16.8764])\n",
      "\tGrad:tensor([-0.0129,  0.0729])\n",
      "Epoch 2184,Loss 2.943721\n",
      "\tParams: tensor([  5.2922, -16.8771])\n",
      "\tGrad:tensor([-0.0129,  0.0728])\n",
      "Epoch 2185,Loss 2.943666\n",
      "\tParams: tensor([  5.2923, -16.8778])\n",
      "\tGrad:tensor([-0.0128,  0.0727])\n",
      "Epoch 2186,Loss 2.943613\n",
      "\tParams: tensor([  5.2924, -16.8786])\n",
      "\tGrad:tensor([-0.0128,  0.0725])\n",
      "Epoch 2187,Loss 2.943558\n",
      "\tParams: tensor([  5.2926, -16.8793])\n",
      "\tGrad:tensor([-0.0128,  0.0724])\n",
      "Epoch 2188,Loss 2.943503\n",
      "\tParams: tensor([  5.2927, -16.8800])\n",
      "\tGrad:tensor([-0.0128,  0.0723])\n",
      "Epoch 2189,Loss 2.943451\n",
      "\tParams: tensor([  5.2928, -16.8807])\n",
      "\tGrad:tensor([-0.0127,  0.0722])\n",
      "Epoch 2190,Loss 2.943395\n",
      "\tParams: tensor([  5.2929, -16.8815])\n",
      "\tGrad:tensor([-0.0127,  0.0720])\n",
      "Epoch 2191,Loss 2.943343\n",
      "\tParams: tensor([  5.2931, -16.8822])\n",
      "\tGrad:tensor([-0.0127,  0.0719])\n",
      "Epoch 2192,Loss 2.943290\n",
      "\tParams: tensor([  5.2932, -16.8829])\n",
      "\tGrad:tensor([-0.0127,  0.0718])\n",
      "Epoch 2193,Loss 2.943235\n",
      "\tParams: tensor([  5.2933, -16.8836])\n",
      "\tGrad:tensor([-0.0127,  0.0717])\n",
      "Epoch 2194,Loss 2.943183\n",
      "\tParams: tensor([  5.2934, -16.8843])\n",
      "\tGrad:tensor([-0.0126,  0.0715])\n",
      "Epoch 2195,Loss 2.943130\n",
      "\tParams: tensor([  5.2936, -16.8850])\n",
      "\tGrad:tensor([-0.0126,  0.0714])\n",
      "Epoch 2196,Loss 2.943079\n",
      "\tParams: tensor([  5.2937, -16.8857])\n",
      "\tGrad:tensor([-0.0126,  0.0713])\n",
      "Epoch 2197,Loss 2.943027\n",
      "\tParams: tensor([  5.2938, -16.8865])\n",
      "\tGrad:tensor([-0.0126,  0.0712])\n",
      "Epoch 2198,Loss 2.942973\n",
      "\tParams: tensor([  5.2939, -16.8872])\n",
      "\tGrad:tensor([-0.0126,  0.0711])\n",
      "Epoch 2199,Loss 2.942922\n",
      "\tParams: tensor([  5.2941, -16.8879])\n",
      "\tGrad:tensor([-0.0125,  0.0709])\n",
      "Epoch 2200,Loss 2.942870\n",
      "\tParams: tensor([  5.2942, -16.8886])\n",
      "\tGrad:tensor([-0.0125,  0.0708])\n",
      "Epoch 2201,Loss 2.942818\n",
      "\tParams: tensor([  5.2943, -16.8893])\n",
      "\tGrad:tensor([-0.0125,  0.0707])\n",
      "Epoch 2202,Loss 2.942766\n",
      "\tParams: tensor([  5.2944, -16.8900])\n",
      "\tGrad:tensor([-0.0125,  0.0706])\n",
      "Epoch 2203,Loss 2.942714\n",
      "\tParams: tensor([  5.2946, -16.8907])\n",
      "\tGrad:tensor([-0.0124,  0.0705])\n",
      "Epoch 2204,Loss 2.942665\n",
      "\tParams: tensor([  5.2947, -16.8914])\n",
      "\tGrad:tensor([-0.0124,  0.0703])\n",
      "Epoch 2205,Loss 2.942612\n",
      "\tParams: tensor([  5.2948, -16.8921])\n",
      "\tGrad:tensor([-0.0124,  0.0702])\n",
      "Epoch 2206,Loss 2.942564\n",
      "\tParams: tensor([  5.2949, -16.8928])\n",
      "\tGrad:tensor([-0.0124,  0.0701])\n",
      "Epoch 2207,Loss 2.942510\n",
      "\tParams: tensor([  5.2951, -16.8935])\n",
      "\tGrad:tensor([-0.0124,  0.0700])\n",
      "Epoch 2208,Loss 2.942461\n",
      "\tParams: tensor([  5.2952, -16.8942])\n",
      "\tGrad:tensor([-0.0123,  0.0699])\n",
      "Epoch 2209,Loss 2.942411\n",
      "\tParams: tensor([  5.2953, -16.8949])\n",
      "\tGrad:tensor([-0.0123,  0.0697])\n",
      "Epoch 2210,Loss 2.942361\n",
      "\tParams: tensor([  5.2954, -16.8956])\n",
      "\tGrad:tensor([-0.0123,  0.0696])\n",
      "Epoch 2211,Loss 2.942310\n",
      "\tParams: tensor([  5.2956, -16.8963])\n",
      "\tGrad:tensor([-0.0123,  0.0695])\n",
      "Epoch 2212,Loss 2.942261\n",
      "\tParams: tensor([  5.2957, -16.8970])\n",
      "\tGrad:tensor([-0.0122,  0.0694])\n",
      "Epoch 2213,Loss 2.942211\n",
      "\tParams: tensor([  5.2958, -16.8977])\n",
      "\tGrad:tensor([-0.0122,  0.0693])\n",
      "Epoch 2214,Loss 2.942162\n",
      "\tParams: tensor([  5.2959, -16.8984])\n",
      "\tGrad:tensor([-0.0122,  0.0692])\n",
      "Epoch 2215,Loss 2.942112\n",
      "\tParams: tensor([  5.2960, -16.8991])\n",
      "\tGrad:tensor([-0.0122,  0.0690])\n",
      "Epoch 2216,Loss 2.942062\n",
      "\tParams: tensor([  5.2962, -16.8998])\n",
      "\tGrad:tensor([-0.0122,  0.0689])\n",
      "Epoch 2217,Loss 2.942014\n",
      "\tParams: tensor([  5.2963, -16.9004])\n",
      "\tGrad:tensor([-0.0122,  0.0688])\n",
      "Epoch 2218,Loss 2.941965\n",
      "\tParams: tensor([  5.2964, -16.9011])\n",
      "\tGrad:tensor([-0.0121,  0.0687])\n",
      "Epoch 2219,Loss 2.941918\n",
      "\tParams: tensor([  5.2965, -16.9018])\n",
      "\tGrad:tensor([-0.0121,  0.0686])\n",
      "Epoch 2220,Loss 2.941868\n",
      "\tParams: tensor([  5.2967, -16.9025])\n",
      "\tGrad:tensor([-0.0121,  0.0685])\n",
      "Epoch 2221,Loss 2.941821\n",
      "\tParams: tensor([  5.2968, -16.9032])\n",
      "\tGrad:tensor([-0.0121,  0.0683])\n",
      "Epoch 2222,Loss 2.941773\n",
      "\tParams: tensor([  5.2969, -16.9039])\n",
      "\tGrad:tensor([-0.0120,  0.0682])\n",
      "Epoch 2223,Loss 2.941724\n",
      "\tParams: tensor([  5.2970, -16.9046])\n",
      "\tGrad:tensor([-0.0120,  0.0681])\n",
      "Epoch 2224,Loss 2.941677\n",
      "\tParams: tensor([  5.2971, -16.9052])\n",
      "\tGrad:tensor([-0.0120,  0.0680])\n",
      "Epoch 2225,Loss 2.941629\n",
      "\tParams: tensor([  5.2973, -16.9059])\n",
      "\tGrad:tensor([-0.0120,  0.0679])\n",
      "Epoch 2226,Loss 2.941582\n",
      "\tParams: tensor([  5.2974, -16.9066])\n",
      "\tGrad:tensor([-0.0120,  0.0678])\n",
      "Epoch 2227,Loss 2.941534\n",
      "\tParams: tensor([  5.2975, -16.9073])\n",
      "\tGrad:tensor([-0.0119,  0.0676])\n",
      "Epoch 2228,Loss 2.941488\n",
      "\tParams: tensor([  5.2976, -16.9079])\n",
      "\tGrad:tensor([-0.0119,  0.0675])\n",
      "Epoch 2229,Loss 2.941440\n",
      "\tParams: tensor([  5.2977, -16.9086])\n",
      "\tGrad:tensor([-0.0119,  0.0674])\n",
      "Epoch 2230,Loss 2.941393\n",
      "\tParams: tensor([  5.2979, -16.9093])\n",
      "\tGrad:tensor([-0.0119,  0.0673])\n",
      "Epoch 2231,Loss 2.941346\n",
      "\tParams: tensor([  5.2980, -16.9100])\n",
      "\tGrad:tensor([-0.0119,  0.0672])\n",
      "Epoch 2232,Loss 2.941299\n",
      "\tParams: tensor([  5.2981, -16.9106])\n",
      "\tGrad:tensor([-0.0118,  0.0671])\n",
      "Epoch 2233,Loss 2.941253\n",
      "\tParams: tensor([  5.2982, -16.9113])\n",
      "\tGrad:tensor([-0.0118,  0.0670])\n",
      "Epoch 2234,Loss 2.941206\n",
      "\tParams: tensor([  5.2983, -16.9120])\n",
      "\tGrad:tensor([-0.0118,  0.0668])\n",
      "Epoch 2235,Loss 2.941163\n",
      "\tParams: tensor([  5.2984, -16.9126])\n",
      "\tGrad:tensor([-0.0118,  0.0667])\n",
      "Epoch 2236,Loss 2.941116\n",
      "\tParams: tensor([  5.2986, -16.9133])\n",
      "\tGrad:tensor([-0.0118,  0.0666])\n",
      "Epoch 2237,Loss 2.941070\n",
      "\tParams: tensor([  5.2987, -16.9140])\n",
      "\tGrad:tensor([-0.0117,  0.0665])\n",
      "Epoch 2238,Loss 2.941025\n",
      "\tParams: tensor([  5.2988, -16.9146])\n",
      "\tGrad:tensor([-0.0117,  0.0664])\n",
      "Epoch 2239,Loss 2.940979\n",
      "\tParams: tensor([  5.2989, -16.9153])\n",
      "\tGrad:tensor([-0.0117,  0.0663])\n",
      "Epoch 2240,Loss 2.940933\n",
      "\tParams: tensor([  5.2990, -16.9160])\n",
      "\tGrad:tensor([-0.0117,  0.0662])\n",
      "Epoch 2241,Loss 2.940890\n",
      "\tParams: tensor([  5.2991, -16.9166])\n",
      "\tGrad:tensor([-0.0117,  0.0661])\n",
      "Epoch 2242,Loss 2.940844\n",
      "\tParams: tensor([  5.2993, -16.9173])\n",
      "\tGrad:tensor([-0.0117,  0.0659])\n",
      "Epoch 2243,Loss 2.940798\n",
      "\tParams: tensor([  5.2994, -16.9179])\n",
      "\tGrad:tensor([-0.0116,  0.0658])\n",
      "Epoch 2244,Loss 2.940753\n",
      "\tParams: tensor([  5.2995, -16.9186])\n",
      "\tGrad:tensor([-0.0116,  0.0657])\n",
      "Epoch 2245,Loss 2.940711\n",
      "\tParams: tensor([  5.2996, -16.9192])\n",
      "\tGrad:tensor([-0.0116,  0.0656])\n",
      "Epoch 2246,Loss 2.940666\n",
      "\tParams: tensor([  5.2997, -16.9199])\n",
      "\tGrad:tensor([-0.0116,  0.0655])\n",
      "Epoch 2247,Loss 2.940621\n",
      "\tParams: tensor([  5.2998, -16.9206])\n",
      "\tGrad:tensor([-0.0115,  0.0654])\n",
      "Epoch 2248,Loss 2.940576\n",
      "\tParams: tensor([  5.3000, -16.9212])\n",
      "\tGrad:tensor([-0.0115,  0.0653])\n",
      "Epoch 2249,Loss 2.940533\n",
      "\tParams: tensor([  5.3001, -16.9219])\n",
      "\tGrad:tensor([-0.0115,  0.0652])\n",
      "Epoch 2250,Loss 2.940489\n",
      "\tParams: tensor([  5.3002, -16.9225])\n",
      "\tGrad:tensor([-0.0115,  0.0650])\n",
      "Epoch 2251,Loss 2.940446\n",
      "\tParams: tensor([  5.3003, -16.9232])\n",
      "\tGrad:tensor([-0.0115,  0.0649])\n",
      "Epoch 2252,Loss 2.940403\n",
      "\tParams: tensor([  5.3004, -16.9238])\n",
      "\tGrad:tensor([-0.0114,  0.0648])\n",
      "Epoch 2253,Loss 2.940358\n",
      "\tParams: tensor([  5.3005, -16.9245])\n",
      "\tGrad:tensor([-0.0114,  0.0647])\n",
      "Epoch 2254,Loss 2.940316\n",
      "\tParams: tensor([  5.3006, -16.9251])\n",
      "\tGrad:tensor([-0.0114,  0.0646])\n",
      "Epoch 2255,Loss 2.940274\n",
      "\tParams: tensor([  5.3008, -16.9257])\n",
      "\tGrad:tensor([-0.0114,  0.0645])\n",
      "Epoch 2256,Loss 2.940229\n",
      "\tParams: tensor([  5.3009, -16.9264])\n",
      "\tGrad:tensor([-0.0114,  0.0644])\n",
      "Epoch 2257,Loss 2.940188\n",
      "\tParams: tensor([  5.3010, -16.9270])\n",
      "\tGrad:tensor([-0.0114,  0.0643])\n",
      "Epoch 2258,Loss 2.940144\n",
      "\tParams: tensor([  5.3011, -16.9277])\n",
      "\tGrad:tensor([-0.0114,  0.0642])\n",
      "Epoch 2259,Loss 2.940102\n",
      "\tParams: tensor([  5.3012, -16.9283])\n",
      "\tGrad:tensor([-0.0113,  0.0641])\n",
      "Epoch 2260,Loss 2.940060\n",
      "\tParams: tensor([  5.3013, -16.9290])\n",
      "\tGrad:tensor([-0.0113,  0.0640])\n",
      "Epoch 2261,Loss 2.940018\n",
      "\tParams: tensor([  5.3014, -16.9296])\n",
      "\tGrad:tensor([-0.0113,  0.0638])\n",
      "Epoch 2262,Loss 2.939977\n",
      "\tParams: tensor([  5.3016, -16.9302])\n",
      "\tGrad:tensor([-0.0113,  0.0637])\n",
      "Epoch 2263,Loss 2.939934\n",
      "\tParams: tensor([  5.3017, -16.9309])\n",
      "\tGrad:tensor([-0.0112,  0.0636])\n",
      "Epoch 2264,Loss 2.939891\n",
      "\tParams: tensor([  5.3018, -16.9315])\n",
      "\tGrad:tensor([-0.0112,  0.0635])\n",
      "Epoch 2265,Loss 2.939851\n",
      "\tParams: tensor([  5.3019, -16.9321])\n",
      "\tGrad:tensor([-0.0112,  0.0634])\n",
      "Epoch 2266,Loss 2.939809\n",
      "\tParams: tensor([  5.3020, -16.9328])\n",
      "\tGrad:tensor([-0.0112,  0.0633])\n",
      "Epoch 2267,Loss 2.939770\n",
      "\tParams: tensor([  5.3021, -16.9334])\n",
      "\tGrad:tensor([-0.0112,  0.0632])\n",
      "Epoch 2268,Loss 2.939727\n",
      "\tParams: tensor([  5.3022, -16.9340])\n",
      "\tGrad:tensor([-0.0111,  0.0631])\n",
      "Epoch 2269,Loss 2.939686\n",
      "\tParams: tensor([  5.3023, -16.9347])\n",
      "\tGrad:tensor([-0.0111,  0.0630])\n",
      "Epoch 2270,Loss 2.939646\n",
      "\tParams: tensor([  5.3024, -16.9353])\n",
      "\tGrad:tensor([-0.0111,  0.0629])\n",
      "Epoch 2271,Loss 2.939605\n",
      "\tParams: tensor([  5.3026, -16.9359])\n",
      "\tGrad:tensor([-0.0111,  0.0628])\n",
      "Epoch 2272,Loss 2.939566\n",
      "\tParams: tensor([  5.3027, -16.9365])\n",
      "\tGrad:tensor([-0.0111,  0.0627])\n",
      "Epoch 2273,Loss 2.939522\n",
      "\tParams: tensor([  5.3028, -16.9372])\n",
      "\tGrad:tensor([-0.0111,  0.0626])\n",
      "Epoch 2274,Loss 2.939483\n",
      "\tParams: tensor([  5.3029, -16.9378])\n",
      "\tGrad:tensor([-0.0110,  0.0624])\n",
      "Epoch 2275,Loss 2.939443\n",
      "\tParams: tensor([  5.3030, -16.9384])\n",
      "\tGrad:tensor([-0.0110,  0.0623])\n",
      "Epoch 2276,Loss 2.939403\n",
      "\tParams: tensor([  5.3031, -16.9390])\n",
      "\tGrad:tensor([-0.0110,  0.0622])\n",
      "Epoch 2277,Loss 2.939361\n",
      "\tParams: tensor([  5.3032, -16.9397])\n",
      "\tGrad:tensor([-0.0110,  0.0621])\n",
      "Epoch 2278,Loss 2.939323\n",
      "\tParams: tensor([  5.3033, -16.9403])\n",
      "\tGrad:tensor([-0.0110,  0.0620])\n",
      "Epoch 2279,Loss 2.939282\n",
      "\tParams: tensor([  5.3034, -16.9409])\n",
      "\tGrad:tensor([-0.0109,  0.0619])\n",
      "Epoch 2280,Loss 2.939243\n",
      "\tParams: tensor([  5.3035, -16.9415])\n",
      "\tGrad:tensor([-0.0109,  0.0618])\n",
      "Epoch 2281,Loss 2.939205\n",
      "\tParams: tensor([  5.3037, -16.9421])\n",
      "\tGrad:tensor([-0.0109,  0.0617])\n",
      "Epoch 2282,Loss 2.939165\n",
      "\tParams: tensor([  5.3038, -16.9428])\n",
      "\tGrad:tensor([-0.0109,  0.0616])\n",
      "Epoch 2283,Loss 2.939127\n",
      "\tParams: tensor([  5.3039, -16.9434])\n",
      "\tGrad:tensor([-0.0109,  0.0615])\n",
      "Epoch 2284,Loss 2.939087\n",
      "\tParams: tensor([  5.3040, -16.9440])\n",
      "\tGrad:tensor([-0.0108,  0.0614])\n",
      "Epoch 2285,Loss 2.939049\n",
      "\tParams: tensor([  5.3041, -16.9446])\n",
      "\tGrad:tensor([-0.0108,  0.0613])\n",
      "Epoch 2286,Loss 2.939011\n",
      "\tParams: tensor([  5.3042, -16.9452])\n",
      "\tGrad:tensor([-0.0108,  0.0612])\n",
      "Epoch 2287,Loss 2.938971\n",
      "\tParams: tensor([  5.3043, -16.9458])\n",
      "\tGrad:tensor([-0.0108,  0.0611])\n",
      "Epoch 2288,Loss 2.938933\n",
      "\tParams: tensor([  5.3044, -16.9464])\n",
      "\tGrad:tensor([-0.0108,  0.0610])\n",
      "Epoch 2289,Loss 2.938893\n",
      "\tParams: tensor([  5.3045, -16.9470])\n",
      "\tGrad:tensor([-0.0108,  0.0609])\n",
      "Epoch 2290,Loss 2.938857\n",
      "\tParams: tensor([  5.3046, -16.9476])\n",
      "\tGrad:tensor([-0.0107,  0.0608])\n",
      "Epoch 2291,Loss 2.938820\n",
      "\tParams: tensor([  5.3047, -16.9482])\n",
      "\tGrad:tensor([-0.0107,  0.0607])\n",
      "Epoch 2292,Loss 2.938779\n",
      "\tParams: tensor([  5.3048, -16.9489])\n",
      "\tGrad:tensor([-0.0107,  0.0606])\n",
      "Epoch 2293,Loss 2.938743\n",
      "\tParams: tensor([  5.3049, -16.9495])\n",
      "\tGrad:tensor([-0.0107,  0.0605])\n",
      "Epoch 2294,Loss 2.938705\n",
      "\tParams: tensor([  5.3051, -16.9501])\n",
      "\tGrad:tensor([-0.0107,  0.0604])\n",
      "Epoch 2295,Loss 2.938667\n",
      "\tParams: tensor([  5.3052, -16.9507])\n",
      "\tGrad:tensor([-0.0106,  0.0603])\n",
      "Epoch 2296,Loss 2.938629\n",
      "\tParams: tensor([  5.3053, -16.9513])\n",
      "\tGrad:tensor([-0.0106,  0.0602])\n",
      "Epoch 2297,Loss 2.938593\n",
      "\tParams: tensor([  5.3054, -16.9519])\n",
      "\tGrad:tensor([-0.0106,  0.0601])\n",
      "Epoch 2298,Loss 2.938555\n",
      "\tParams: tensor([  5.3055, -16.9525])\n",
      "\tGrad:tensor([-0.0106,  0.0600])\n",
      "Epoch 2299,Loss 2.938519\n",
      "\tParams: tensor([  5.3056, -16.9531])\n",
      "\tGrad:tensor([-0.0106,  0.0598])\n",
      "Epoch 2300,Loss 2.938481\n",
      "\tParams: tensor([  5.3057, -16.9537])\n",
      "\tGrad:tensor([-0.0106,  0.0597])\n",
      "Epoch 2301,Loss 2.938444\n",
      "\tParams: tensor([  5.3058, -16.9543])\n",
      "\tGrad:tensor([-0.0105,  0.0596])\n",
      "Epoch 2302,Loss 2.938408\n",
      "\tParams: tensor([  5.3059, -16.9549])\n",
      "\tGrad:tensor([-0.0105,  0.0595])\n",
      "Epoch 2303,Loss 2.938371\n",
      "\tParams: tensor([  5.3060, -16.9554])\n",
      "\tGrad:tensor([-0.0105,  0.0594])\n",
      "Epoch 2304,Loss 2.938335\n",
      "\tParams: tensor([  5.3061, -16.9560])\n",
      "\tGrad:tensor([-0.0105,  0.0593])\n",
      "Epoch 2305,Loss 2.938299\n",
      "\tParams: tensor([  5.3062, -16.9566])\n",
      "\tGrad:tensor([-0.0105,  0.0592])\n",
      "Epoch 2306,Loss 2.938263\n",
      "\tParams: tensor([  5.3063, -16.9572])\n",
      "\tGrad:tensor([-0.0105,  0.0591])\n",
      "Epoch 2307,Loss 2.938227\n",
      "\tParams: tensor([  5.3064, -16.9578])\n",
      "\tGrad:tensor([-0.0104,  0.0590])\n",
      "Epoch 2308,Loss 2.938190\n",
      "\tParams: tensor([  5.3065, -16.9584])\n",
      "\tGrad:tensor([-0.0104,  0.0589])\n",
      "Epoch 2309,Loss 2.938155\n",
      "\tParams: tensor([  5.3066, -16.9590])\n",
      "\tGrad:tensor([-0.0104,  0.0588])\n",
      "Epoch 2310,Loss 2.938118\n",
      "\tParams: tensor([  5.3067, -16.9596])\n",
      "\tGrad:tensor([-0.0104,  0.0587])\n",
      "Epoch 2311,Loss 2.938084\n",
      "\tParams: tensor([  5.3068, -16.9602])\n",
      "\tGrad:tensor([-0.0104,  0.0586])\n",
      "Epoch 2312,Loss 2.938049\n",
      "\tParams: tensor([  5.3069, -16.9608])\n",
      "\tGrad:tensor([-0.0103,  0.0585])\n",
      "Epoch 2313,Loss 2.938014\n",
      "\tParams: tensor([  5.3070, -16.9613])\n",
      "\tGrad:tensor([-0.0103,  0.0584])\n",
      "Epoch 2314,Loss 2.937977\n",
      "\tParams: tensor([  5.3072, -16.9619])\n",
      "\tGrad:tensor([-0.0103,  0.0583])\n",
      "Epoch 2315,Loss 2.937943\n",
      "\tParams: tensor([  5.3073, -16.9625])\n",
      "\tGrad:tensor([-0.0103,  0.0582])\n",
      "Epoch 2316,Loss 2.937908\n",
      "\tParams: tensor([  5.3074, -16.9631])\n",
      "\tGrad:tensor([-0.0103,  0.0581])\n",
      "Epoch 2317,Loss 2.937872\n",
      "\tParams: tensor([  5.3075, -16.9637])\n",
      "\tGrad:tensor([-0.0103,  0.0580])\n",
      "Epoch 2318,Loss 2.937839\n",
      "\tParams: tensor([  5.3076, -16.9642])\n",
      "\tGrad:tensor([-0.0102,  0.0580])\n",
      "Epoch 2319,Loss 2.937804\n",
      "\tParams: tensor([  5.3077, -16.9648])\n",
      "\tGrad:tensor([-0.0102,  0.0578])\n",
      "Epoch 2320,Loss 2.937769\n",
      "\tParams: tensor([  5.3078, -16.9654])\n",
      "\tGrad:tensor([-0.0102,  0.0578])\n",
      "Epoch 2321,Loss 2.937734\n",
      "\tParams: tensor([  5.3079, -16.9660])\n",
      "\tGrad:tensor([-0.0102,  0.0577])\n",
      "Epoch 2322,Loss 2.937700\n",
      "\tParams: tensor([  5.3080, -16.9666])\n",
      "\tGrad:tensor([-0.0102,  0.0576])\n",
      "Epoch 2323,Loss 2.937665\n",
      "\tParams: tensor([  5.3081, -16.9671])\n",
      "\tGrad:tensor([-0.0102,  0.0575])\n",
      "Epoch 2324,Loss 2.937632\n",
      "\tParams: tensor([  5.3082, -16.9677])\n",
      "\tGrad:tensor([-0.0101,  0.0574])\n",
      "Epoch 2325,Loss 2.937598\n",
      "\tParams: tensor([  5.3083, -16.9683])\n",
      "\tGrad:tensor([-0.0101,  0.0573])\n",
      "Epoch 2326,Loss 2.937565\n",
      "\tParams: tensor([  5.3084, -16.9688])\n",
      "\tGrad:tensor([-0.0101,  0.0572])\n",
      "Epoch 2327,Loss 2.937531\n",
      "\tParams: tensor([  5.3085, -16.9694])\n",
      "\tGrad:tensor([-0.0101,  0.0571])\n",
      "Epoch 2328,Loss 2.937499\n",
      "\tParams: tensor([  5.3086, -16.9700])\n",
      "\tGrad:tensor([-0.0101,  0.0570])\n",
      "Epoch 2329,Loss 2.937465\n",
      "\tParams: tensor([  5.3087, -16.9706])\n",
      "\tGrad:tensor([-0.0101,  0.0569])\n",
      "Epoch 2330,Loss 2.937430\n",
      "\tParams: tensor([  5.3088, -16.9711])\n",
      "\tGrad:tensor([-0.0100,  0.0568])\n",
      "Epoch 2331,Loss 2.937398\n",
      "\tParams: tensor([  5.3089, -16.9717])\n",
      "\tGrad:tensor([-0.0100,  0.0567])\n",
      "Epoch 2332,Loss 2.937364\n",
      "\tParams: tensor([  5.3090, -16.9723])\n",
      "\tGrad:tensor([-0.0100,  0.0566])\n",
      "Epoch 2333,Loss 2.937332\n",
      "\tParams: tensor([  5.3091, -16.9728])\n",
      "\tGrad:tensor([-0.0100,  0.0565])\n",
      "Epoch 2334,Loss 2.937299\n",
      "\tParams: tensor([  5.3092, -16.9734])\n",
      "\tGrad:tensor([-0.0100,  0.0564])\n",
      "Epoch 2335,Loss 2.937265\n",
      "\tParams: tensor([  5.3093, -16.9739])\n",
      "\tGrad:tensor([-0.0100,  0.0563])\n",
      "Epoch 2336,Loss 2.937232\n",
      "\tParams: tensor([  5.3094, -16.9745])\n",
      "\tGrad:tensor([-0.0099,  0.0562])\n",
      "Epoch 2337,Loss 2.937201\n",
      "\tParams: tensor([  5.3095, -16.9751])\n",
      "\tGrad:tensor([-0.0099,  0.0561])\n",
      "Epoch 2338,Loss 2.937167\n",
      "\tParams: tensor([  5.3096, -16.9756])\n",
      "\tGrad:tensor([-0.0099,  0.0560])\n",
      "Epoch 2339,Loss 2.937134\n",
      "\tParams: tensor([  5.3097, -16.9762])\n",
      "\tGrad:tensor([-0.0099,  0.0559])\n",
      "Epoch 2340,Loss 2.937104\n",
      "\tParams: tensor([  5.3098, -16.9767])\n",
      "\tGrad:tensor([-0.0099,  0.0558])\n",
      "Epoch 2341,Loss 2.937071\n",
      "\tParams: tensor([  5.3099, -16.9773])\n",
      "\tGrad:tensor([-0.0098,  0.0557])\n",
      "Epoch 2342,Loss 2.937039\n",
      "\tParams: tensor([  5.3100, -16.9779])\n",
      "\tGrad:tensor([-0.0098,  0.0556])\n",
      "Epoch 2343,Loss 2.937008\n",
      "\tParams: tensor([  5.3101, -16.9784])\n",
      "\tGrad:tensor([-0.0098,  0.0555])\n",
      "Epoch 2344,Loss 2.936976\n",
      "\tParams: tensor([  5.3102, -16.9790])\n",
      "\tGrad:tensor([-0.0098,  0.0554])\n",
      "Epoch 2345,Loss 2.936945\n",
      "\tParams: tensor([  5.3103, -16.9795])\n",
      "\tGrad:tensor([-0.0098,  0.0553])\n",
      "Epoch 2346,Loss 2.936912\n",
      "\tParams: tensor([  5.3104, -16.9801])\n",
      "\tGrad:tensor([-0.0098,  0.0553])\n",
      "Epoch 2347,Loss 2.936883\n",
      "\tParams: tensor([  5.3105, -16.9806])\n",
      "\tGrad:tensor([-0.0097,  0.0552])\n",
      "Epoch 2348,Loss 2.936851\n",
      "\tParams: tensor([  5.3106, -16.9812])\n",
      "\tGrad:tensor([-0.0097,  0.0551])\n",
      "Epoch 2349,Loss 2.936819\n",
      "\tParams: tensor([  5.3107, -16.9817])\n",
      "\tGrad:tensor([-0.0097,  0.0550])\n",
      "Epoch 2350,Loss 2.936788\n",
      "\tParams: tensor([  5.3107, -16.9823])\n",
      "\tGrad:tensor([-0.0097,  0.0549])\n",
      "Epoch 2351,Loss 2.936757\n",
      "\tParams: tensor([  5.3108, -16.9828])\n",
      "\tGrad:tensor([-0.0097,  0.0548])\n",
      "Epoch 2352,Loss 2.936725\n",
      "\tParams: tensor([  5.3109, -16.9834])\n",
      "\tGrad:tensor([-0.0097,  0.0547])\n",
      "Epoch 2353,Loss 2.936694\n",
      "\tParams: tensor([  5.3110, -16.9839])\n",
      "\tGrad:tensor([-0.0096,  0.0546])\n",
      "Epoch 2354,Loss 2.936665\n",
      "\tParams: tensor([  5.3111, -16.9845])\n",
      "\tGrad:tensor([-0.0096,  0.0545])\n",
      "Epoch 2355,Loss 2.936633\n",
      "\tParams: tensor([  5.3112, -16.9850])\n",
      "\tGrad:tensor([-0.0096,  0.0544])\n",
      "Epoch 2356,Loss 2.936602\n",
      "\tParams: tensor([  5.3113, -16.9856])\n",
      "\tGrad:tensor([-0.0096,  0.0543])\n",
      "Epoch 2357,Loss 2.936572\n",
      "\tParams: tensor([  5.3114, -16.9861])\n",
      "\tGrad:tensor([-0.0096,  0.0542])\n",
      "Epoch 2358,Loss 2.936542\n",
      "\tParams: tensor([  5.3115, -16.9866])\n",
      "\tGrad:tensor([-0.0095,  0.0541])\n",
      "Epoch 2359,Loss 2.936511\n",
      "\tParams: tensor([  5.3116, -16.9872])\n",
      "\tGrad:tensor([-0.0096,  0.0540])\n",
      "Epoch 2360,Loss 2.936481\n",
      "\tParams: tensor([  5.3117, -16.9877])\n",
      "\tGrad:tensor([-0.0095,  0.0540])\n",
      "Epoch 2361,Loss 2.936451\n",
      "\tParams: tensor([  5.3118, -16.9883])\n",
      "\tGrad:tensor([-0.0095,  0.0539])\n",
      "Epoch 2362,Loss 2.936421\n",
      "\tParams: tensor([  5.3119, -16.9888])\n",
      "\tGrad:tensor([-0.0095,  0.0538])\n",
      "Epoch 2363,Loss 2.936392\n",
      "\tParams: tensor([  5.3120, -16.9893])\n",
      "\tGrad:tensor([-0.0095,  0.0537])\n",
      "Epoch 2364,Loss 2.936362\n",
      "\tParams: tensor([  5.3121, -16.9899])\n",
      "\tGrad:tensor([-0.0094,  0.0536])\n",
      "Epoch 2365,Loss 2.936332\n",
      "\tParams: tensor([  5.3122, -16.9904])\n",
      "\tGrad:tensor([-0.0094,  0.0535])\n",
      "Epoch 2366,Loss 2.936304\n",
      "\tParams: tensor([  5.3123, -16.9909])\n",
      "\tGrad:tensor([-0.0094,  0.0534])\n",
      "Epoch 2367,Loss 2.936274\n",
      "\tParams: tensor([  5.3124, -16.9915])\n",
      "\tGrad:tensor([-0.0094,  0.0533])\n",
      "Epoch 2368,Loss 2.936244\n",
      "\tParams: tensor([  5.3125, -16.9920])\n",
      "\tGrad:tensor([-0.0094,  0.0532])\n",
      "Epoch 2369,Loss 2.936216\n",
      "\tParams: tensor([  5.3126, -16.9925])\n",
      "\tGrad:tensor([-0.0094,  0.0531])\n",
      "Epoch 2370,Loss 2.936188\n",
      "\tParams: tensor([  5.3127, -16.9931])\n",
      "\tGrad:tensor([-0.0094,  0.0530])\n",
      "Epoch 2371,Loss 2.936156\n",
      "\tParams: tensor([  5.3127, -16.9936])\n",
      "\tGrad:tensor([-0.0094,  0.0530])\n",
      "Epoch 2372,Loss 2.936128\n",
      "\tParams: tensor([  5.3128, -16.9941])\n",
      "\tGrad:tensor([-0.0093,  0.0529])\n",
      "Epoch 2373,Loss 2.936100\n",
      "\tParams: tensor([  5.3129, -16.9946])\n",
      "\tGrad:tensor([-0.0093,  0.0528])\n",
      "Epoch 2374,Loss 2.936072\n",
      "\tParams: tensor([  5.3130, -16.9952])\n",
      "\tGrad:tensor([-0.0093,  0.0527])\n",
      "Epoch 2375,Loss 2.936042\n",
      "\tParams: tensor([  5.3131, -16.9957])\n",
      "\tGrad:tensor([-0.0093,  0.0526])\n",
      "Epoch 2376,Loss 2.936014\n",
      "\tParams: tensor([  5.3132, -16.9962])\n",
      "\tGrad:tensor([-0.0093,  0.0525])\n",
      "Epoch 2377,Loss 2.935986\n",
      "\tParams: tensor([  5.3133, -16.9967])\n",
      "\tGrad:tensor([-0.0093,  0.0524])\n",
      "Epoch 2378,Loss 2.935957\n",
      "\tParams: tensor([  5.3134, -16.9973])\n",
      "\tGrad:tensor([-0.0093,  0.0523])\n",
      "Epoch 2379,Loss 2.935928\n",
      "\tParams: tensor([  5.3135, -16.9978])\n",
      "\tGrad:tensor([-0.0092,  0.0522])\n",
      "Epoch 2380,Loss 2.935901\n",
      "\tParams: tensor([  5.3136, -16.9983])\n",
      "\tGrad:tensor([-0.0092,  0.0522])\n",
      "Epoch 2381,Loss 2.935873\n",
      "\tParams: tensor([  5.3137, -16.9988])\n",
      "\tGrad:tensor([-0.0092,  0.0521])\n",
      "Epoch 2382,Loss 2.935845\n",
      "\tParams: tensor([  5.3138, -16.9994])\n",
      "\tGrad:tensor([-0.0092,  0.0520])\n",
      "Epoch 2383,Loss 2.935817\n",
      "\tParams: tensor([  5.3139, -16.9999])\n",
      "\tGrad:tensor([-0.0092,  0.0519])\n",
      "Epoch 2384,Loss 2.935789\n",
      "\tParams: tensor([  5.3139, -17.0004])\n",
      "\tGrad:tensor([-0.0092,  0.0518])\n",
      "Epoch 2385,Loss 2.935762\n",
      "\tParams: tensor([  5.3140, -17.0009])\n",
      "\tGrad:tensor([-0.0092,  0.0517])\n",
      "Epoch 2386,Loss 2.935734\n",
      "\tParams: tensor([  5.3141, -17.0014])\n",
      "\tGrad:tensor([-0.0091,  0.0516])\n",
      "Epoch 2387,Loss 2.935707\n",
      "\tParams: tensor([  5.3142, -17.0019])\n",
      "\tGrad:tensor([-0.0091,  0.0515])\n",
      "Epoch 2388,Loss 2.935679\n",
      "\tParams: tensor([  5.3143, -17.0025])\n",
      "\tGrad:tensor([-0.0091,  0.0514])\n",
      "Epoch 2389,Loss 2.935650\n",
      "\tParams: tensor([  5.3144, -17.0030])\n",
      "\tGrad:tensor([-0.0091,  0.0514])\n",
      "Epoch 2390,Loss 2.935626\n",
      "\tParams: tensor([  5.3145, -17.0035])\n",
      "\tGrad:tensor([-0.0090,  0.0513])\n",
      "Epoch 2391,Loss 2.935596\n",
      "\tParams: tensor([  5.3146, -17.0040])\n",
      "\tGrad:tensor([-0.0090,  0.0512])\n",
      "Epoch 2392,Loss 2.935571\n",
      "\tParams: tensor([  5.3147, -17.0045])\n",
      "\tGrad:tensor([-0.0090,  0.0511])\n",
      "Epoch 2393,Loss 2.935544\n",
      "\tParams: tensor([  5.3148, -17.0050])\n",
      "\tGrad:tensor([-0.0090,  0.0510])\n",
      "Epoch 2394,Loss 2.935516\n",
      "\tParams: tensor([  5.3149, -17.0055])\n",
      "\tGrad:tensor([-0.0090,  0.0509])\n",
      "Epoch 2395,Loss 2.935489\n",
      "\tParams: tensor([  5.3149, -17.0060])\n",
      "\tGrad:tensor([-0.0090,  0.0508])\n",
      "Epoch 2396,Loss 2.935465\n",
      "\tParams: tensor([  5.3150, -17.0065])\n",
      "\tGrad:tensor([-0.0090,  0.0507])\n",
      "Epoch 2397,Loss 2.935436\n",
      "\tParams: tensor([  5.3151, -17.0070])\n",
      "\tGrad:tensor([-0.0090,  0.0507])\n",
      "Epoch 2398,Loss 2.935411\n",
      "\tParams: tensor([  5.3152, -17.0076])\n",
      "\tGrad:tensor([-0.0089,  0.0506])\n",
      "Epoch 2399,Loss 2.935385\n",
      "\tParams: tensor([  5.3153, -17.0081])\n",
      "\tGrad:tensor([-0.0089,  0.0505])\n",
      "Epoch 2400,Loss 2.935356\n",
      "\tParams: tensor([  5.3154, -17.0086])\n",
      "\tGrad:tensor([-0.0089,  0.0504])\n",
      "Epoch 2401,Loss 2.935332\n",
      "\tParams: tensor([  5.3155, -17.0091])\n",
      "\tGrad:tensor([-0.0089,  0.0503])\n",
      "Epoch 2402,Loss 2.935304\n",
      "\tParams: tensor([  5.3156, -17.0096])\n",
      "\tGrad:tensor([-0.0089,  0.0502])\n",
      "Epoch 2403,Loss 2.935281\n",
      "\tParams: tensor([  5.3157, -17.0101])\n",
      "\tGrad:tensor([-0.0088,  0.0502])\n",
      "Epoch 2404,Loss 2.935252\n",
      "\tParams: tensor([  5.3157, -17.0106])\n",
      "\tGrad:tensor([-0.0088,  0.0501])\n",
      "Epoch 2405,Loss 2.935228\n",
      "\tParams: tensor([  5.3158, -17.0111])\n",
      "\tGrad:tensor([-0.0088,  0.0500])\n",
      "Epoch 2406,Loss 2.935203\n",
      "\tParams: tensor([  5.3159, -17.0116])\n",
      "\tGrad:tensor([-0.0088,  0.0499])\n",
      "Epoch 2407,Loss 2.935177\n",
      "\tParams: tensor([  5.3160, -17.0121])\n",
      "\tGrad:tensor([-0.0088,  0.0498])\n",
      "Epoch 2408,Loss 2.935152\n",
      "\tParams: tensor([  5.3161, -17.0126])\n",
      "\tGrad:tensor([-0.0088,  0.0497])\n",
      "Epoch 2409,Loss 2.935126\n",
      "\tParams: tensor([  5.3162, -17.0131])\n",
      "\tGrad:tensor([-0.0088,  0.0496])\n",
      "Epoch 2410,Loss 2.935100\n",
      "\tParams: tensor([  5.3163, -17.0136])\n",
      "\tGrad:tensor([-0.0088,  0.0496])\n",
      "Epoch 2411,Loss 2.935075\n",
      "\tParams: tensor([  5.3164, -17.0140])\n",
      "\tGrad:tensor([-0.0087,  0.0495])\n",
      "Epoch 2412,Loss 2.935049\n",
      "\tParams: tensor([  5.3164, -17.0145])\n",
      "\tGrad:tensor([-0.0087,  0.0494])\n",
      "Epoch 2413,Loss 2.935024\n",
      "\tParams: tensor([  5.3165, -17.0150])\n",
      "\tGrad:tensor([-0.0087,  0.0493])\n",
      "Epoch 2414,Loss 2.935001\n",
      "\tParams: tensor([  5.3166, -17.0155])\n",
      "\tGrad:tensor([-0.0087,  0.0492])\n",
      "Epoch 2415,Loss 2.934973\n",
      "\tParams: tensor([  5.3167, -17.0160])\n",
      "\tGrad:tensor([-0.0087,  0.0491])\n",
      "Epoch 2416,Loss 2.934949\n",
      "\tParams: tensor([  5.3168, -17.0165])\n",
      "\tGrad:tensor([-0.0087,  0.0491])\n",
      "Epoch 2417,Loss 2.934925\n",
      "\tParams: tensor([  5.3169, -17.0170])\n",
      "\tGrad:tensor([-0.0086,  0.0490])\n",
      "Epoch 2418,Loss 2.934899\n",
      "\tParams: tensor([  5.3170, -17.0175])\n",
      "\tGrad:tensor([-0.0086,  0.0489])\n",
      "Epoch 2419,Loss 2.934876\n",
      "\tParams: tensor([  5.3171, -17.0180])\n",
      "\tGrad:tensor([-0.0086,  0.0488])\n",
      "Epoch 2420,Loss 2.934853\n",
      "\tParams: tensor([  5.3171, -17.0185])\n",
      "\tGrad:tensor([-0.0086,  0.0487])\n",
      "Epoch 2421,Loss 2.934826\n",
      "\tParams: tensor([  5.3172, -17.0189])\n",
      "\tGrad:tensor([-0.0086,  0.0486])\n",
      "Epoch 2422,Loss 2.934802\n",
      "\tParams: tensor([  5.3173, -17.0194])\n",
      "\tGrad:tensor([-0.0086,  0.0486])\n",
      "Epoch 2423,Loss 2.934777\n",
      "\tParams: tensor([  5.3174, -17.0199])\n",
      "\tGrad:tensor([-0.0086,  0.0485])\n",
      "Epoch 2424,Loss 2.934753\n",
      "\tParams: tensor([  5.3175, -17.0204])\n",
      "\tGrad:tensor([-0.0086,  0.0484])\n",
      "Epoch 2425,Loss 2.934730\n",
      "\tParams: tensor([  5.3176, -17.0209])\n",
      "\tGrad:tensor([-0.0086,  0.0483])\n",
      "Epoch 2426,Loss 2.934705\n",
      "\tParams: tensor([  5.3177, -17.0214])\n",
      "\tGrad:tensor([-0.0085,  0.0482])\n",
      "Epoch 2427,Loss 2.934681\n",
      "\tParams: tensor([  5.3177, -17.0219])\n",
      "\tGrad:tensor([-0.0085,  0.0481])\n",
      "Epoch 2428,Loss 2.934658\n",
      "\tParams: tensor([  5.3178, -17.0223])\n",
      "\tGrad:tensor([-0.0085,  0.0481])\n",
      "Epoch 2429,Loss 2.934635\n",
      "\tParams: tensor([  5.3179, -17.0228])\n",
      "\tGrad:tensor([-0.0085,  0.0480])\n",
      "Epoch 2430,Loss 2.934609\n",
      "\tParams: tensor([  5.3180, -17.0233])\n",
      "\tGrad:tensor([-0.0085,  0.0479])\n",
      "Epoch 2431,Loss 2.934585\n",
      "\tParams: tensor([  5.3181, -17.0238])\n",
      "\tGrad:tensor([-0.0084,  0.0478])\n",
      "Epoch 2432,Loss 2.934563\n",
      "\tParams: tensor([  5.3182, -17.0242])\n",
      "\tGrad:tensor([-0.0084,  0.0477])\n",
      "Epoch 2433,Loss 2.934541\n",
      "\tParams: tensor([  5.3182, -17.0247])\n",
      "\tGrad:tensor([-0.0084,  0.0477])\n",
      "Epoch 2434,Loss 2.934516\n",
      "\tParams: tensor([  5.3183, -17.0252])\n",
      "\tGrad:tensor([-0.0084,  0.0476])\n",
      "Epoch 2435,Loss 2.934493\n",
      "\tParams: tensor([  5.3184, -17.0257])\n",
      "\tGrad:tensor([-0.0084,  0.0475])\n",
      "Epoch 2436,Loss 2.934469\n",
      "\tParams: tensor([  5.3185, -17.0261])\n",
      "\tGrad:tensor([-0.0084,  0.0474])\n",
      "Epoch 2437,Loss 2.934446\n",
      "\tParams: tensor([  5.3186, -17.0266])\n",
      "\tGrad:tensor([-0.0084,  0.0473])\n",
      "Epoch 2438,Loss 2.934423\n",
      "\tParams: tensor([  5.3187, -17.0271])\n",
      "\tGrad:tensor([-0.0083,  0.0473])\n",
      "Epoch 2439,Loss 2.934400\n",
      "\tParams: tensor([  5.3187, -17.0276])\n",
      "\tGrad:tensor([-0.0083,  0.0472])\n",
      "Epoch 2440,Loss 2.934377\n",
      "\tParams: tensor([  5.3188, -17.0280])\n",
      "\tGrad:tensor([-0.0083,  0.0471])\n",
      "Epoch 2441,Loss 2.934355\n",
      "\tParams: tensor([  5.3189, -17.0285])\n",
      "\tGrad:tensor([-0.0083,  0.0470])\n",
      "Epoch 2442,Loss 2.934331\n",
      "\tParams: tensor([  5.3190, -17.0290])\n",
      "\tGrad:tensor([-0.0083,  0.0469])\n",
      "Epoch 2443,Loss 2.934309\n",
      "\tParams: tensor([  5.3191, -17.0294])\n",
      "\tGrad:tensor([-0.0083,  0.0469])\n",
      "Epoch 2444,Loss 2.934287\n",
      "\tParams: tensor([  5.3192, -17.0299])\n",
      "\tGrad:tensor([-0.0083,  0.0468])\n",
      "Epoch 2445,Loss 2.934264\n",
      "\tParams: tensor([  5.3192, -17.0304])\n",
      "\tGrad:tensor([-0.0083,  0.0467])\n",
      "Epoch 2446,Loss 2.934242\n",
      "\tParams: tensor([  5.3193, -17.0308])\n",
      "\tGrad:tensor([-0.0083,  0.0466])\n",
      "Epoch 2447,Loss 2.934219\n",
      "\tParams: tensor([  5.3194, -17.0313])\n",
      "\tGrad:tensor([-0.0082,  0.0465])\n",
      "Epoch 2448,Loss 2.934198\n",
      "\tParams: tensor([  5.3195, -17.0318])\n",
      "\tGrad:tensor([-0.0082,  0.0465])\n",
      "Epoch 2449,Loss 2.934175\n",
      "\tParams: tensor([  5.3196, -17.0322])\n",
      "\tGrad:tensor([-0.0082,  0.0464])\n",
      "Epoch 2450,Loss 2.934151\n",
      "\tParams: tensor([  5.3197, -17.0327])\n",
      "\tGrad:tensor([-0.0082,  0.0463])\n",
      "Epoch 2451,Loss 2.934129\n",
      "\tParams: tensor([  5.3197, -17.0332])\n",
      "\tGrad:tensor([-0.0082,  0.0462])\n",
      "Epoch 2452,Loss 2.934108\n",
      "\tParams: tensor([  5.3198, -17.0336])\n",
      "\tGrad:tensor([-0.0082,  0.0461])\n",
      "Epoch 2453,Loss 2.934084\n",
      "\tParams: tensor([  5.3199, -17.0341])\n",
      "\tGrad:tensor([-0.0081,  0.0461])\n",
      "Epoch 2454,Loss 2.934065\n",
      "\tParams: tensor([  5.3200, -17.0345])\n",
      "\tGrad:tensor([-0.0081,  0.0460])\n",
      "Epoch 2455,Loss 2.934043\n",
      "\tParams: tensor([  5.3201, -17.0350])\n",
      "\tGrad:tensor([-0.0081,  0.0459])\n",
      "Epoch 2456,Loss 2.934020\n",
      "\tParams: tensor([  5.3201, -17.0355])\n",
      "\tGrad:tensor([-0.0081,  0.0458])\n",
      "Epoch 2457,Loss 2.934000\n",
      "\tParams: tensor([  5.3202, -17.0359])\n",
      "\tGrad:tensor([-0.0081,  0.0457])\n",
      "Epoch 2458,Loss 2.933978\n",
      "\tParams: tensor([  5.3203, -17.0364])\n",
      "\tGrad:tensor([-0.0081,  0.0457])\n",
      "Epoch 2459,Loss 2.933956\n",
      "\tParams: tensor([  5.3204, -17.0368])\n",
      "\tGrad:tensor([-0.0080,  0.0456])\n",
      "Epoch 2460,Loss 2.933935\n",
      "\tParams: tensor([  5.3205, -17.0373])\n",
      "\tGrad:tensor([-0.0080,  0.0455])\n",
      "Epoch 2461,Loss 2.933914\n",
      "\tParams: tensor([  5.3205, -17.0377])\n",
      "\tGrad:tensor([-0.0080,  0.0454])\n",
      "Epoch 2462,Loss 2.933893\n",
      "\tParams: tensor([  5.3206, -17.0382])\n",
      "\tGrad:tensor([-0.0080,  0.0454])\n",
      "Epoch 2463,Loss 2.933871\n",
      "\tParams: tensor([  5.3207, -17.0386])\n",
      "\tGrad:tensor([-0.0080,  0.0453])\n",
      "Epoch 2464,Loss 2.933849\n",
      "\tParams: tensor([  5.3208, -17.0391])\n",
      "\tGrad:tensor([-0.0080,  0.0452])\n",
      "Epoch 2465,Loss 2.933828\n",
      "\tParams: tensor([  5.3209, -17.0396])\n",
      "\tGrad:tensor([-0.0080,  0.0451])\n",
      "Epoch 2466,Loss 2.933807\n",
      "\tParams: tensor([  5.3209, -17.0400])\n",
      "\tGrad:tensor([-0.0080,  0.0451])\n",
      "Epoch 2467,Loss 2.933787\n",
      "\tParams: tensor([  5.3210, -17.0405])\n",
      "\tGrad:tensor([-0.0079,  0.0450])\n",
      "Epoch 2468,Loss 2.933767\n",
      "\tParams: tensor([  5.3211, -17.0409])\n",
      "\tGrad:tensor([-0.0079,  0.0449])\n",
      "Epoch 2469,Loss 2.933746\n",
      "\tParams: tensor([  5.3212, -17.0413])\n",
      "\tGrad:tensor([-0.0079,  0.0448])\n",
      "Epoch 2470,Loss 2.933723\n",
      "\tParams: tensor([  5.3213, -17.0418])\n",
      "\tGrad:tensor([-0.0079,  0.0448])\n",
      "Epoch 2471,Loss 2.933704\n",
      "\tParams: tensor([  5.3213, -17.0422])\n",
      "\tGrad:tensor([-0.0079,  0.0447])\n",
      "Epoch 2472,Loss 2.933682\n",
      "\tParams: tensor([  5.3214, -17.0427])\n",
      "\tGrad:tensor([-0.0079,  0.0446])\n",
      "Epoch 2473,Loss 2.933662\n",
      "\tParams: tensor([  5.3215, -17.0431])\n",
      "\tGrad:tensor([-0.0079,  0.0445])\n",
      "Epoch 2474,Loss 2.933643\n",
      "\tParams: tensor([  5.3216, -17.0436])\n",
      "\tGrad:tensor([-0.0079,  0.0444])\n",
      "Epoch 2475,Loss 2.933622\n",
      "\tParams: tensor([  5.3217, -17.0440])\n",
      "\tGrad:tensor([-0.0078,  0.0444])\n",
      "Epoch 2476,Loss 2.933602\n",
      "\tParams: tensor([  5.3217, -17.0445])\n",
      "\tGrad:tensor([-0.0078,  0.0443])\n",
      "Epoch 2477,Loss 2.933583\n",
      "\tParams: tensor([  5.3218, -17.0449])\n",
      "\tGrad:tensor([-0.0078,  0.0442])\n",
      "Epoch 2478,Loss 2.933561\n",
      "\tParams: tensor([  5.3219, -17.0453])\n",
      "\tGrad:tensor([-0.0078,  0.0441])\n",
      "Epoch 2479,Loss 2.933541\n",
      "\tParams: tensor([  5.3220, -17.0458])\n",
      "\tGrad:tensor([-0.0078,  0.0441])\n",
      "Epoch 2480,Loss 2.933521\n",
      "\tParams: tensor([  5.3220, -17.0462])\n",
      "\tGrad:tensor([-0.0078,  0.0440])\n",
      "Epoch 2481,Loss 2.933501\n",
      "\tParams: tensor([  5.3221, -17.0467])\n",
      "\tGrad:tensor([-0.0078,  0.0439])\n",
      "Epoch 2482,Loss 2.933480\n",
      "\tParams: tensor([  5.3222, -17.0471])\n",
      "\tGrad:tensor([-0.0077,  0.0438])\n",
      "Epoch 2483,Loss 2.933463\n",
      "\tParams: tensor([  5.3223, -17.0475])\n",
      "\tGrad:tensor([-0.0077,  0.0438])\n",
      "Epoch 2484,Loss 2.933442\n",
      "\tParams: tensor([  5.3224, -17.0480])\n",
      "\tGrad:tensor([-0.0077,  0.0437])\n",
      "Epoch 2485,Loss 2.933422\n",
      "\tParams: tensor([  5.3224, -17.0484])\n",
      "\tGrad:tensor([-0.0077,  0.0436])\n",
      "Epoch 2486,Loss 2.933403\n",
      "\tParams: tensor([  5.3225, -17.0489])\n",
      "\tGrad:tensor([-0.0077,  0.0436])\n",
      "Epoch 2487,Loss 2.933382\n",
      "\tParams: tensor([  5.3226, -17.0493])\n",
      "\tGrad:tensor([-0.0077,  0.0435])\n",
      "Epoch 2488,Loss 2.933365\n",
      "\tParams: tensor([  5.3227, -17.0497])\n",
      "\tGrad:tensor([-0.0077,  0.0434])\n",
      "Epoch 2489,Loss 2.933345\n",
      "\tParams: tensor([  5.3227, -17.0502])\n",
      "\tGrad:tensor([-0.0077,  0.0433])\n",
      "Epoch 2490,Loss 2.933325\n",
      "\tParams: tensor([  5.3228, -17.0506])\n",
      "\tGrad:tensor([-0.0076,  0.0433])\n",
      "Epoch 2491,Loss 2.933306\n",
      "\tParams: tensor([  5.3229, -17.0510])\n",
      "\tGrad:tensor([-0.0076,  0.0432])\n",
      "Epoch 2492,Loss 2.933287\n",
      "\tParams: tensor([  5.3230, -17.0515])\n",
      "\tGrad:tensor([-0.0076,  0.0431])\n",
      "Epoch 2493,Loss 2.933266\n",
      "\tParams: tensor([  5.3230, -17.0519])\n",
      "\tGrad:tensor([-0.0076,  0.0430])\n",
      "Epoch 2494,Loss 2.933249\n",
      "\tParams: tensor([  5.3231, -17.0523])\n",
      "\tGrad:tensor([-0.0076,  0.0430])\n",
      "Epoch 2495,Loss 2.933229\n",
      "\tParams: tensor([  5.3232, -17.0527])\n",
      "\tGrad:tensor([-0.0076,  0.0429])\n",
      "Epoch 2496,Loss 2.933209\n",
      "\tParams: tensor([  5.3233, -17.0532])\n",
      "\tGrad:tensor([-0.0076,  0.0428])\n",
      "Epoch 2497,Loss 2.933190\n",
      "\tParams: tensor([  5.3233, -17.0536])\n",
      "\tGrad:tensor([-0.0075,  0.0427])\n",
      "Epoch 2498,Loss 2.933172\n",
      "\tParams: tensor([  5.3234, -17.0540])\n",
      "\tGrad:tensor([-0.0075,  0.0427])\n",
      "Epoch 2499,Loss 2.933154\n",
      "\tParams: tensor([  5.3235, -17.0544])\n",
      "\tGrad:tensor([-0.0075,  0.0426])\n",
      "Epoch 2500,Loss 2.933134\n",
      "\tParams: tensor([  5.3236, -17.0549])\n",
      "\tGrad:tensor([-0.0075,  0.0425])\n",
      "Epoch 2501,Loss 2.933116\n",
      "\tParams: tensor([  5.3236, -17.0553])\n",
      "\tGrad:tensor([-0.0075,  0.0425])\n",
      "Epoch 2502,Loss 2.933097\n",
      "\tParams: tensor([  5.3237, -17.0557])\n",
      "\tGrad:tensor([-0.0075,  0.0424])\n",
      "Epoch 2503,Loss 2.933079\n",
      "\tParams: tensor([  5.3238, -17.0561])\n",
      "\tGrad:tensor([-0.0075,  0.0423])\n",
      "Epoch 2504,Loss 2.933060\n",
      "\tParams: tensor([  5.3239, -17.0566])\n",
      "\tGrad:tensor([-0.0075,  0.0422])\n",
      "Epoch 2505,Loss 2.933043\n",
      "\tParams: tensor([  5.3239, -17.0570])\n",
      "\tGrad:tensor([-0.0074,  0.0422])\n",
      "Epoch 2506,Loss 2.933025\n",
      "\tParams: tensor([  5.3240, -17.0574])\n",
      "\tGrad:tensor([-0.0074,  0.0421])\n",
      "Epoch 2507,Loss 2.933007\n",
      "\tParams: tensor([  5.3241, -17.0578])\n",
      "\tGrad:tensor([-0.0074,  0.0420])\n",
      "Epoch 2508,Loss 2.932988\n",
      "\tParams: tensor([  5.3242, -17.0582])\n",
      "\tGrad:tensor([-0.0074,  0.0420])\n",
      "Epoch 2509,Loss 2.932970\n",
      "\tParams: tensor([  5.3242, -17.0587])\n",
      "\tGrad:tensor([-0.0074,  0.0419])\n",
      "Epoch 2510,Loss 2.932953\n",
      "\tParams: tensor([  5.3243, -17.0591])\n",
      "\tGrad:tensor([-0.0074,  0.0418])\n",
      "Epoch 2511,Loss 2.932932\n",
      "\tParams: tensor([  5.3244, -17.0595])\n",
      "\tGrad:tensor([-0.0074,  0.0417])\n",
      "Epoch 2512,Loss 2.932915\n",
      "\tParams: tensor([  5.3245, -17.0599])\n",
      "\tGrad:tensor([-0.0073,  0.0417])\n",
      "Epoch 2513,Loss 2.932898\n",
      "\tParams: tensor([  5.3245, -17.0603])\n",
      "\tGrad:tensor([-0.0073,  0.0416])\n",
      "Epoch 2514,Loss 2.932880\n",
      "\tParams: tensor([  5.3246, -17.0608])\n",
      "\tGrad:tensor([-0.0073,  0.0415])\n",
      "Epoch 2515,Loss 2.932862\n",
      "\tParams: tensor([  5.3247, -17.0612])\n",
      "\tGrad:tensor([-0.0073,  0.0415])\n",
      "Epoch 2516,Loss 2.932846\n",
      "\tParams: tensor([  5.3248, -17.0616])\n",
      "\tGrad:tensor([-0.0073,  0.0414])\n",
      "Epoch 2517,Loss 2.932826\n",
      "\tParams: tensor([  5.3248, -17.0620])\n",
      "\tGrad:tensor([-0.0073,  0.0413])\n",
      "Epoch 2518,Loss 2.932810\n",
      "\tParams: tensor([  5.3249, -17.0624])\n",
      "\tGrad:tensor([-0.0073,  0.0412])\n",
      "Epoch 2519,Loss 2.932790\n",
      "\tParams: tensor([  5.3250, -17.0628])\n",
      "\tGrad:tensor([-0.0073,  0.0412])\n",
      "Epoch 2520,Loss 2.932774\n",
      "\tParams: tensor([  5.3250, -17.0632])\n",
      "\tGrad:tensor([-0.0073,  0.0411])\n",
      "Epoch 2521,Loss 2.932758\n",
      "\tParams: tensor([  5.3251, -17.0636])\n",
      "\tGrad:tensor([-0.0073,  0.0410])\n",
      "Epoch 2522,Loss 2.932739\n",
      "\tParams: tensor([  5.3252, -17.0640])\n",
      "\tGrad:tensor([-0.0073,  0.0410])\n",
      "Epoch 2523,Loss 2.932723\n",
      "\tParams: tensor([  5.3253, -17.0645])\n",
      "\tGrad:tensor([-0.0072,  0.0409])\n",
      "Epoch 2524,Loss 2.932706\n",
      "\tParams: tensor([  5.3253, -17.0649])\n",
      "\tGrad:tensor([-0.0072,  0.0408])\n",
      "Epoch 2525,Loss 2.932689\n",
      "\tParams: tensor([  5.3254, -17.0653])\n",
      "\tGrad:tensor([-0.0072,  0.0408])\n",
      "Epoch 2526,Loss 2.932671\n",
      "\tParams: tensor([  5.3255, -17.0657])\n",
      "\tGrad:tensor([-0.0072,  0.0407])\n",
      "Epoch 2527,Loss 2.932654\n",
      "\tParams: tensor([  5.3256, -17.0661])\n",
      "\tGrad:tensor([-0.0072,  0.0406])\n",
      "Epoch 2528,Loss 2.932637\n",
      "\tParams: tensor([  5.3256, -17.0665])\n",
      "\tGrad:tensor([-0.0072,  0.0405])\n",
      "Epoch 2529,Loss 2.932619\n",
      "\tParams: tensor([  5.3257, -17.0669])\n",
      "\tGrad:tensor([-0.0072,  0.0405])\n",
      "Epoch 2530,Loss 2.932603\n",
      "\tParams: tensor([  5.3258, -17.0673])\n",
      "\tGrad:tensor([-0.0071,  0.0404])\n",
      "Epoch 2531,Loss 2.932585\n",
      "\tParams: tensor([  5.3258, -17.0677])\n",
      "\tGrad:tensor([-0.0071,  0.0403])\n",
      "Epoch 2532,Loss 2.932569\n",
      "\tParams: tensor([  5.3259, -17.0681])\n",
      "\tGrad:tensor([-0.0071,  0.0403])\n",
      "Epoch 2533,Loss 2.932553\n",
      "\tParams: tensor([  5.3260, -17.0685])\n",
      "\tGrad:tensor([-0.0071,  0.0402])\n",
      "Epoch 2534,Loss 2.932535\n",
      "\tParams: tensor([  5.3261, -17.0689])\n",
      "\tGrad:tensor([-0.0071,  0.0401])\n",
      "Epoch 2535,Loss 2.932520\n",
      "\tParams: tensor([  5.3261, -17.0693])\n",
      "\tGrad:tensor([-0.0071,  0.0401])\n",
      "Epoch 2536,Loss 2.932502\n",
      "\tParams: tensor([  5.3262, -17.0697])\n",
      "\tGrad:tensor([-0.0071,  0.0400])\n",
      "Epoch 2537,Loss 2.932487\n",
      "\tParams: tensor([  5.3263, -17.0701])\n",
      "\tGrad:tensor([-0.0071,  0.0399])\n",
      "Epoch 2538,Loss 2.932469\n",
      "\tParams: tensor([  5.3263, -17.0705])\n",
      "\tGrad:tensor([-0.0070,  0.0399])\n",
      "Epoch 2539,Loss 2.932455\n",
      "\tParams: tensor([  5.3264, -17.0709])\n",
      "\tGrad:tensor([-0.0070,  0.0398])\n",
      "Epoch 2540,Loss 2.932438\n",
      "\tParams: tensor([  5.3265, -17.0713])\n",
      "\tGrad:tensor([-0.0070,  0.0397])\n",
      "Epoch 2541,Loss 2.932421\n",
      "\tParams: tensor([  5.3265, -17.0717])\n",
      "\tGrad:tensor([-0.0070,  0.0397])\n",
      "Epoch 2542,Loss 2.932404\n",
      "\tParams: tensor([  5.3266, -17.0721])\n",
      "\tGrad:tensor([-0.0070,  0.0396])\n",
      "Epoch 2543,Loss 2.932387\n",
      "\tParams: tensor([  5.3267, -17.0725])\n",
      "\tGrad:tensor([-0.0070,  0.0395])\n",
      "Epoch 2544,Loss 2.932371\n",
      "\tParams: tensor([  5.3268, -17.0729])\n",
      "\tGrad:tensor([-0.0070,  0.0395])\n",
      "Epoch 2545,Loss 2.932358\n",
      "\tParams: tensor([  5.3268, -17.0733])\n",
      "\tGrad:tensor([-0.0070,  0.0394])\n",
      "Epoch 2546,Loss 2.932340\n",
      "\tParams: tensor([  5.3269, -17.0737])\n",
      "\tGrad:tensor([-0.0069,  0.0393])\n",
      "Epoch 2547,Loss 2.932324\n",
      "\tParams: tensor([  5.3270, -17.0741])\n",
      "\tGrad:tensor([-0.0069,  0.0393])\n",
      "Epoch 2548,Loss 2.932310\n",
      "\tParams: tensor([  5.3270, -17.0745])\n",
      "\tGrad:tensor([-0.0069,  0.0392])\n",
      "Epoch 2549,Loss 2.932293\n",
      "\tParams: tensor([  5.3271, -17.0749])\n",
      "\tGrad:tensor([-0.0069,  0.0391])\n",
      "Epoch 2550,Loss 2.932277\n",
      "\tParams: tensor([  5.3272, -17.0752])\n",
      "\tGrad:tensor([-0.0069,  0.0391])\n",
      "Epoch 2551,Loss 2.932261\n",
      "\tParams: tensor([  5.3272, -17.0756])\n",
      "\tGrad:tensor([-0.0069,  0.0390])\n",
      "Epoch 2552,Loss 2.932246\n",
      "\tParams: tensor([  5.3273, -17.0760])\n",
      "\tGrad:tensor([-0.0069,  0.0389])\n",
      "Epoch 2553,Loss 2.932229\n",
      "\tParams: tensor([  5.3274, -17.0764])\n",
      "\tGrad:tensor([-0.0069,  0.0389])\n",
      "Epoch 2554,Loss 2.932215\n",
      "\tParams: tensor([  5.3274, -17.0768])\n",
      "\tGrad:tensor([-0.0069,  0.0388])\n",
      "Epoch 2555,Loss 2.932198\n",
      "\tParams: tensor([  5.3275, -17.0772])\n",
      "\tGrad:tensor([-0.0068,  0.0387])\n",
      "Epoch 2556,Loss 2.932183\n",
      "\tParams: tensor([  5.3276, -17.0776])\n",
      "\tGrad:tensor([-0.0068,  0.0387])\n",
      "Epoch 2557,Loss 2.932167\n",
      "\tParams: tensor([  5.3276, -17.0780])\n",
      "\tGrad:tensor([-0.0068,  0.0386])\n",
      "Epoch 2558,Loss 2.932153\n",
      "\tParams: tensor([  5.3277, -17.0783])\n",
      "\tGrad:tensor([-0.0068,  0.0385])\n",
      "Epoch 2559,Loss 2.932137\n",
      "\tParams: tensor([  5.3278, -17.0787])\n",
      "\tGrad:tensor([-0.0068,  0.0385])\n",
      "Epoch 2560,Loss 2.932122\n",
      "\tParams: tensor([  5.3279, -17.0791])\n",
      "\tGrad:tensor([-0.0068,  0.0384])\n",
      "Epoch 2561,Loss 2.932107\n",
      "\tParams: tensor([  5.3279, -17.0795])\n",
      "\tGrad:tensor([-0.0068,  0.0383])\n",
      "Epoch 2562,Loss 2.932092\n",
      "\tParams: tensor([  5.3280, -17.0799])\n",
      "\tGrad:tensor([-0.0068,  0.0383])\n",
      "Epoch 2563,Loss 2.932076\n",
      "\tParams: tensor([  5.3281, -17.0803])\n",
      "\tGrad:tensor([-0.0067,  0.0382])\n",
      "Epoch 2564,Loss 2.932061\n",
      "\tParams: tensor([  5.3281, -17.0806])\n",
      "\tGrad:tensor([-0.0067,  0.0381])\n",
      "Epoch 2565,Loss 2.932047\n",
      "\tParams: tensor([  5.3282, -17.0810])\n",
      "\tGrad:tensor([-0.0067,  0.0381])\n",
      "Epoch 2566,Loss 2.932031\n",
      "\tParams: tensor([  5.3283, -17.0814])\n",
      "\tGrad:tensor([-0.0067,  0.0380])\n",
      "Epoch 2567,Loss 2.932017\n",
      "\tParams: tensor([  5.3283, -17.0818])\n",
      "\tGrad:tensor([-0.0067,  0.0379])\n",
      "Epoch 2568,Loss 2.932002\n",
      "\tParams: tensor([  5.3284, -17.0822])\n",
      "\tGrad:tensor([-0.0067,  0.0379])\n",
      "Epoch 2569,Loss 2.931986\n",
      "\tParams: tensor([  5.3285, -17.0825])\n",
      "\tGrad:tensor([-0.0067,  0.0378])\n",
      "Epoch 2570,Loss 2.931972\n",
      "\tParams: tensor([  5.3285, -17.0829])\n",
      "\tGrad:tensor([-0.0067,  0.0378])\n",
      "Epoch 2571,Loss 2.931957\n",
      "\tParams: tensor([  5.3286, -17.0833])\n",
      "\tGrad:tensor([-0.0067,  0.0377])\n",
      "Epoch 2572,Loss 2.931941\n",
      "\tParams: tensor([  5.3287, -17.0837])\n",
      "\tGrad:tensor([-0.0067,  0.0376])\n",
      "Epoch 2573,Loss 2.931929\n",
      "\tParams: tensor([  5.3287, -17.0840])\n",
      "\tGrad:tensor([-0.0066,  0.0376])\n",
      "Epoch 2574,Loss 2.931914\n",
      "\tParams: tensor([  5.3288, -17.0844])\n",
      "\tGrad:tensor([-0.0066,  0.0375])\n",
      "Epoch 2575,Loss 2.931900\n",
      "\tParams: tensor([  5.3289, -17.0848])\n",
      "\tGrad:tensor([-0.0066,  0.0374])\n",
      "Epoch 2576,Loss 2.931885\n",
      "\tParams: tensor([  5.3289, -17.0852])\n",
      "\tGrad:tensor([-0.0066,  0.0374])\n",
      "Epoch 2577,Loss 2.931870\n",
      "\tParams: tensor([  5.3290, -17.0855])\n",
      "\tGrad:tensor([-0.0066,  0.0373])\n",
      "Epoch 2578,Loss 2.931855\n",
      "\tParams: tensor([  5.3291, -17.0859])\n",
      "\tGrad:tensor([-0.0066,  0.0372])\n",
      "Epoch 2579,Loss 2.931842\n",
      "\tParams: tensor([  5.3291, -17.0863])\n",
      "\tGrad:tensor([-0.0066,  0.0372])\n",
      "Epoch 2580,Loss 2.931828\n",
      "\tParams: tensor([  5.3292, -17.0867])\n",
      "\tGrad:tensor([-0.0066,  0.0371])\n",
      "Epoch 2581,Loss 2.931813\n",
      "\tParams: tensor([  5.3293, -17.0870])\n",
      "\tGrad:tensor([-0.0065,  0.0371])\n",
      "Epoch 2582,Loss 2.931799\n",
      "\tParams: tensor([  5.3293, -17.0874])\n",
      "\tGrad:tensor([-0.0065,  0.0370])\n",
      "Epoch 2583,Loss 2.931786\n",
      "\tParams: tensor([  5.3294, -17.0878])\n",
      "\tGrad:tensor([-0.0065,  0.0369])\n",
      "Epoch 2584,Loss 2.931771\n",
      "\tParams: tensor([  5.3294, -17.0881])\n",
      "\tGrad:tensor([-0.0065,  0.0369])\n",
      "Epoch 2585,Loss 2.931759\n",
      "\tParams: tensor([  5.3295, -17.0885])\n",
      "\tGrad:tensor([-0.0065,  0.0368])\n",
      "Epoch 2586,Loss 2.931742\n",
      "\tParams: tensor([  5.3296, -17.0889])\n",
      "\tGrad:tensor([-0.0065,  0.0367])\n",
      "Epoch 2587,Loss 2.931729\n",
      "\tParams: tensor([  5.3296, -17.0892])\n",
      "\tGrad:tensor([-0.0065,  0.0367])\n",
      "Epoch 2588,Loss 2.931717\n",
      "\tParams: tensor([  5.3297, -17.0896])\n",
      "\tGrad:tensor([-0.0065,  0.0366])\n",
      "Epoch 2589,Loss 2.931701\n",
      "\tParams: tensor([  5.3298, -17.0900])\n",
      "\tGrad:tensor([-0.0065,  0.0366])\n",
      "Epoch 2590,Loss 2.931687\n",
      "\tParams: tensor([  5.3298, -17.0903])\n",
      "\tGrad:tensor([-0.0065,  0.0365])\n",
      "Epoch 2591,Loss 2.931674\n",
      "\tParams: tensor([  5.3299, -17.0907])\n",
      "\tGrad:tensor([-0.0064,  0.0364])\n",
      "Epoch 2592,Loss 2.931660\n",
      "\tParams: tensor([  5.3300, -17.0911])\n",
      "\tGrad:tensor([-0.0064,  0.0364])\n",
      "Epoch 2593,Loss 2.931648\n",
      "\tParams: tensor([  5.3300, -17.0914])\n",
      "\tGrad:tensor([-0.0064,  0.0363])\n",
      "Epoch 2594,Loss 2.931632\n",
      "\tParams: tensor([  5.3301, -17.0918])\n",
      "\tGrad:tensor([-0.0064,  0.0362])\n",
      "Epoch 2595,Loss 2.931619\n",
      "\tParams: tensor([  5.3302, -17.0921])\n",
      "\tGrad:tensor([-0.0064,  0.0362])\n",
      "Epoch 2596,Loss 2.931606\n",
      "\tParams: tensor([  5.3302, -17.0925])\n",
      "\tGrad:tensor([-0.0064,  0.0361])\n",
      "Epoch 2597,Loss 2.931593\n",
      "\tParams: tensor([  5.3303, -17.0929])\n",
      "\tGrad:tensor([-0.0064,  0.0361])\n",
      "Epoch 2598,Loss 2.931580\n",
      "\tParams: tensor([  5.3303, -17.0932])\n",
      "\tGrad:tensor([-0.0064,  0.0360])\n",
      "Epoch 2599,Loss 2.931566\n",
      "\tParams: tensor([  5.3304, -17.0936])\n",
      "\tGrad:tensor([-0.0064,  0.0359])\n",
      "Epoch 2600,Loss 2.931554\n",
      "\tParams: tensor([  5.3305, -17.0939])\n",
      "\tGrad:tensor([-0.0064,  0.0359])\n",
      "Epoch 2601,Loss 2.931538\n",
      "\tParams: tensor([  5.3305, -17.0943])\n",
      "\tGrad:tensor([-0.0063,  0.0358])\n",
      "Epoch 2602,Loss 2.931526\n",
      "\tParams: tensor([  5.3306, -17.0947])\n",
      "\tGrad:tensor([-0.0063,  0.0358])\n",
      "Epoch 2603,Loss 2.931512\n",
      "\tParams: tensor([  5.3307, -17.0950])\n",
      "\tGrad:tensor([-0.0063,  0.0357])\n",
      "Epoch 2604,Loss 2.931499\n",
      "\tParams: tensor([  5.3307, -17.0954])\n",
      "\tGrad:tensor([-0.0063,  0.0356])\n",
      "Epoch 2605,Loss 2.931488\n",
      "\tParams: tensor([  5.3308, -17.0957])\n",
      "\tGrad:tensor([-0.0063,  0.0356])\n",
      "Epoch 2606,Loss 2.931474\n",
      "\tParams: tensor([  5.3309, -17.0961])\n",
      "\tGrad:tensor([-0.0063,  0.0355])\n",
      "Epoch 2607,Loss 2.931462\n",
      "\tParams: tensor([  5.3309, -17.0964])\n",
      "\tGrad:tensor([-0.0062,  0.0355])\n",
      "Epoch 2608,Loss 2.931448\n",
      "\tParams: tensor([  5.3310, -17.0968])\n",
      "\tGrad:tensor([-0.0062,  0.0354])\n",
      "Epoch 2609,Loss 2.931436\n",
      "\tParams: tensor([  5.3310, -17.0971])\n",
      "\tGrad:tensor([-0.0062,  0.0353])\n",
      "Epoch 2610,Loss 2.931423\n",
      "\tParams: tensor([  5.3311, -17.0975])\n",
      "\tGrad:tensor([-0.0062,  0.0353])\n",
      "Epoch 2611,Loss 2.931411\n",
      "\tParams: tensor([  5.3312, -17.0979])\n",
      "\tGrad:tensor([-0.0062,  0.0352])\n",
      "Epoch 2612,Loss 2.931397\n",
      "\tParams: tensor([  5.3312, -17.0982])\n",
      "\tGrad:tensor([-0.0062,  0.0352])\n",
      "Epoch 2613,Loss 2.931384\n",
      "\tParams: tensor([  5.3313, -17.0986])\n",
      "\tGrad:tensor([-0.0062,  0.0351])\n",
      "Epoch 2614,Loss 2.931371\n",
      "\tParams: tensor([  5.3313, -17.0989])\n",
      "\tGrad:tensor([-0.0062,  0.0350])\n",
      "Epoch 2615,Loss 2.931358\n",
      "\tParams: tensor([  5.3314, -17.0993])\n",
      "\tGrad:tensor([-0.0062,  0.0350])\n",
      "Epoch 2616,Loss 2.931346\n",
      "\tParams: tensor([  5.3315, -17.0996])\n",
      "\tGrad:tensor([-0.0062,  0.0349])\n",
      "Epoch 2617,Loss 2.931335\n",
      "\tParams: tensor([  5.3315, -17.1000])\n",
      "\tGrad:tensor([-0.0062,  0.0349])\n",
      "Epoch 2618,Loss 2.931322\n",
      "\tParams: tensor([  5.3316, -17.1003])\n",
      "\tGrad:tensor([-0.0062,  0.0348])\n",
      "Epoch 2619,Loss 2.931308\n",
      "\tParams: tensor([  5.3317, -17.1006])\n",
      "\tGrad:tensor([-0.0061,  0.0347])\n",
      "Epoch 2620,Loss 2.931296\n",
      "\tParams: tensor([  5.3317, -17.1010])\n",
      "\tGrad:tensor([-0.0061,  0.0347])\n",
      "Epoch 2621,Loss 2.931282\n",
      "\tParams: tensor([  5.3318, -17.1013])\n",
      "\tGrad:tensor([-0.0061,  0.0346])\n",
      "Epoch 2622,Loss 2.931272\n",
      "\tParams: tensor([  5.3318, -17.1017])\n",
      "\tGrad:tensor([-0.0061,  0.0346])\n",
      "Epoch 2623,Loss 2.931258\n",
      "\tParams: tensor([  5.3319, -17.1020])\n",
      "\tGrad:tensor([-0.0061,  0.0345])\n",
      "Epoch 2624,Loss 2.931245\n",
      "\tParams: tensor([  5.3320, -17.1024])\n",
      "\tGrad:tensor([-0.0061,  0.0344])\n",
      "Epoch 2625,Loss 2.931234\n",
      "\tParams: tensor([  5.3320, -17.1027])\n",
      "\tGrad:tensor([-0.0061,  0.0344])\n",
      "Epoch 2626,Loss 2.931222\n",
      "\tParams: tensor([  5.3321, -17.1031])\n",
      "\tGrad:tensor([-0.0061,  0.0343])\n",
      "Epoch 2627,Loss 2.931211\n",
      "\tParams: tensor([  5.3321, -17.1034])\n",
      "\tGrad:tensor([-0.0060,  0.0343])\n",
      "Epoch 2628,Loss 2.931196\n",
      "\tParams: tensor([  5.3322, -17.1038])\n",
      "\tGrad:tensor([-0.0060,  0.0342])\n",
      "Epoch 2629,Loss 2.931185\n",
      "\tParams: tensor([  5.3323, -17.1041])\n",
      "\tGrad:tensor([-0.0060,  0.0342])\n",
      "Epoch 2630,Loss 2.931173\n",
      "\tParams: tensor([  5.3323, -17.1044])\n",
      "\tGrad:tensor([-0.0060,  0.0341])\n",
      "Epoch 2631,Loss 2.931162\n",
      "\tParams: tensor([  5.3324, -17.1048])\n",
      "\tGrad:tensor([-0.0060,  0.0340])\n",
      "Epoch 2632,Loss 2.931149\n",
      "\tParams: tensor([  5.3324, -17.1051])\n",
      "\tGrad:tensor([-0.0060,  0.0340])\n",
      "Epoch 2633,Loss 2.931138\n",
      "\tParams: tensor([  5.3325, -17.1055])\n",
      "\tGrad:tensor([-0.0060,  0.0339])\n",
      "Epoch 2634,Loss 2.931126\n",
      "\tParams: tensor([  5.3326, -17.1058])\n",
      "\tGrad:tensor([-0.0060,  0.0339])\n",
      "Epoch 2635,Loss 2.931114\n",
      "\tParams: tensor([  5.3326, -17.1061])\n",
      "\tGrad:tensor([-0.0060,  0.0338])\n",
      "Epoch 2636,Loss 2.931101\n",
      "\tParams: tensor([  5.3327, -17.1065])\n",
      "\tGrad:tensor([-0.0060,  0.0337])\n",
      "Epoch 2637,Loss 2.931090\n",
      "\tParams: tensor([  5.3327, -17.1068])\n",
      "\tGrad:tensor([-0.0059,  0.0337])\n",
      "Epoch 2638,Loss 2.931079\n",
      "\tParams: tensor([  5.3328, -17.1071])\n",
      "\tGrad:tensor([-0.0059,  0.0336])\n",
      "Epoch 2639,Loss 2.931067\n",
      "\tParams: tensor([  5.3329, -17.1075])\n",
      "\tGrad:tensor([-0.0059,  0.0336])\n",
      "Epoch 2640,Loss 2.931054\n",
      "\tParams: tensor([  5.3329, -17.1078])\n",
      "\tGrad:tensor([-0.0059,  0.0335])\n",
      "Epoch 2641,Loss 2.931044\n",
      "\tParams: tensor([  5.3330, -17.1081])\n",
      "\tGrad:tensor([-0.0059,  0.0335])\n",
      "Epoch 2642,Loss 2.931034\n",
      "\tParams: tensor([  5.3330, -17.1085])\n",
      "\tGrad:tensor([-0.0059,  0.0334])\n",
      "Epoch 2643,Loss 2.931021\n",
      "\tParams: tensor([  5.3331, -17.1088])\n",
      "\tGrad:tensor([-0.0059,  0.0333])\n",
      "Epoch 2644,Loss 2.931010\n",
      "\tParams: tensor([  5.3332, -17.1091])\n",
      "\tGrad:tensor([-0.0059,  0.0333])\n",
      "Epoch 2645,Loss 2.930999\n",
      "\tParams: tensor([  5.3332, -17.1095])\n",
      "\tGrad:tensor([-0.0059,  0.0332])\n",
      "Epoch 2646,Loss 2.930987\n",
      "\tParams: tensor([  5.3333, -17.1098])\n",
      "\tGrad:tensor([-0.0059,  0.0332])\n",
      "Epoch 2647,Loss 2.930976\n",
      "\tParams: tensor([  5.3333, -17.1101])\n",
      "\tGrad:tensor([-0.0059,  0.0331])\n",
      "Epoch 2648,Loss 2.930964\n",
      "\tParams: tensor([  5.3334, -17.1105])\n",
      "\tGrad:tensor([-0.0059,  0.0331])\n",
      "Epoch 2649,Loss 2.930953\n",
      "\tParams: tensor([  5.3335, -17.1108])\n",
      "\tGrad:tensor([-0.0058,  0.0330])\n",
      "Epoch 2650,Loss 2.930941\n",
      "\tParams: tensor([  5.3335, -17.1111])\n",
      "\tGrad:tensor([-0.0058,  0.0330])\n",
      "Epoch 2651,Loss 2.930932\n",
      "\tParams: tensor([  5.3336, -17.1115])\n",
      "\tGrad:tensor([-0.0058,  0.0329])\n",
      "Epoch 2652,Loss 2.930921\n",
      "\tParams: tensor([  5.3336, -17.1118])\n",
      "\tGrad:tensor([-0.0058,  0.0328])\n",
      "Epoch 2653,Loss 2.930908\n",
      "\tParams: tensor([  5.3337, -17.1121])\n",
      "\tGrad:tensor([-0.0058,  0.0328])\n",
      "Epoch 2654,Loss 2.930899\n",
      "\tParams: tensor([  5.3337, -17.1124])\n",
      "\tGrad:tensor([-0.0058,  0.0327])\n",
      "Epoch 2655,Loss 2.930885\n",
      "\tParams: tensor([  5.3338, -17.1128])\n",
      "\tGrad:tensor([-0.0058,  0.0327])\n",
      "Epoch 2656,Loss 2.930876\n",
      "\tParams: tensor([  5.3339, -17.1131])\n",
      "\tGrad:tensor([-0.0058,  0.0326])\n",
      "Epoch 2657,Loss 2.930863\n",
      "\tParams: tensor([  5.3339, -17.1134])\n",
      "\tGrad:tensor([-0.0057,  0.0326])\n",
      "Epoch 2658,Loss 2.930854\n",
      "\tParams: tensor([  5.3340, -17.1137])\n",
      "\tGrad:tensor([-0.0057,  0.0325])\n",
      "Epoch 2659,Loss 2.930841\n",
      "\tParams: tensor([  5.3340, -17.1141])\n",
      "\tGrad:tensor([-0.0057,  0.0325])\n",
      "Epoch 2660,Loss 2.930833\n",
      "\tParams: tensor([  5.3341, -17.1144])\n",
      "\tGrad:tensor([-0.0057,  0.0324])\n",
      "Epoch 2661,Loss 2.930821\n",
      "\tParams: tensor([  5.3341, -17.1147])\n",
      "\tGrad:tensor([-0.0057,  0.0323])\n",
      "Epoch 2662,Loss 2.930811\n",
      "\tParams: tensor([  5.3342, -17.1150])\n",
      "\tGrad:tensor([-0.0057,  0.0323])\n",
      "Epoch 2663,Loss 2.930801\n",
      "\tParams: tensor([  5.3343, -17.1154])\n",
      "\tGrad:tensor([-0.0057,  0.0322])\n",
      "Epoch 2664,Loss 2.930788\n",
      "\tParams: tensor([  5.3343, -17.1157])\n",
      "\tGrad:tensor([-0.0057,  0.0322])\n",
      "Epoch 2665,Loss 2.930778\n",
      "\tParams: tensor([  5.3344, -17.1160])\n",
      "\tGrad:tensor([-0.0057,  0.0321])\n",
      "Epoch 2666,Loss 2.930767\n",
      "\tParams: tensor([  5.3344, -17.1163])\n",
      "\tGrad:tensor([-0.0057,  0.0321])\n",
      "Epoch 2667,Loss 2.930757\n",
      "\tParams: tensor([  5.3345, -17.1166])\n",
      "\tGrad:tensor([-0.0057,  0.0320])\n",
      "Epoch 2668,Loss 2.930746\n",
      "\tParams: tensor([  5.3345, -17.1170])\n",
      "\tGrad:tensor([-0.0056,  0.0320])\n",
      "Epoch 2669,Loss 2.930736\n",
      "\tParams: tensor([  5.3346, -17.1173])\n",
      "\tGrad:tensor([-0.0056,  0.0319])\n",
      "Epoch 2670,Loss 2.930724\n",
      "\tParams: tensor([  5.3347, -17.1176])\n",
      "\tGrad:tensor([-0.0056,  0.0319])\n",
      "Epoch 2671,Loss 2.930715\n",
      "\tParams: tensor([  5.3347, -17.1179])\n",
      "\tGrad:tensor([-0.0056,  0.0318])\n",
      "Epoch 2672,Loss 2.930704\n",
      "\tParams: tensor([  5.3348, -17.1182])\n",
      "\tGrad:tensor([-0.0056,  0.0317])\n",
      "Epoch 2673,Loss 2.930694\n",
      "\tParams: tensor([  5.3348, -17.1186])\n",
      "\tGrad:tensor([-0.0056,  0.0317])\n",
      "Epoch 2674,Loss 2.930685\n",
      "\tParams: tensor([  5.3349, -17.1189])\n",
      "\tGrad:tensor([-0.0056,  0.0316])\n",
      "Epoch 2675,Loss 2.930674\n",
      "\tParams: tensor([  5.3349, -17.1192])\n",
      "\tGrad:tensor([-0.0056,  0.0316])\n",
      "Epoch 2676,Loss 2.930663\n",
      "\tParams: tensor([  5.3350, -17.1195])\n",
      "\tGrad:tensor([-0.0056,  0.0315])\n",
      "Epoch 2677,Loss 2.930654\n",
      "\tParams: tensor([  5.3350, -17.1198])\n",
      "\tGrad:tensor([-0.0056,  0.0315])\n",
      "Epoch 2678,Loss 2.930644\n",
      "\tParams: tensor([  5.3351, -17.1201])\n",
      "\tGrad:tensor([-0.0055,  0.0314])\n",
      "Epoch 2679,Loss 2.930631\n",
      "\tParams: tensor([  5.3352, -17.1204])\n",
      "\tGrad:tensor([-0.0055,  0.0314])\n",
      "Epoch 2680,Loss 2.930621\n",
      "\tParams: tensor([  5.3352, -17.1208])\n",
      "\tGrad:tensor([-0.0055,  0.0313])\n",
      "Epoch 2681,Loss 2.930613\n",
      "\tParams: tensor([  5.3353, -17.1211])\n",
      "\tGrad:tensor([-0.0055,  0.0313])\n",
      "Epoch 2682,Loss 2.930603\n",
      "\tParams: tensor([  5.3353, -17.1214])\n",
      "\tGrad:tensor([-0.0055,  0.0312])\n",
      "Epoch 2683,Loss 2.930593\n",
      "\tParams: tensor([  5.3354, -17.1217])\n",
      "\tGrad:tensor([-0.0055,  0.0312])\n",
      "Epoch 2684,Loss 2.930582\n",
      "\tParams: tensor([  5.3354, -17.1220])\n",
      "\tGrad:tensor([-0.0055,  0.0311])\n",
      "Epoch 2685,Loss 2.930571\n",
      "\tParams: tensor([  5.3355, -17.1223])\n",
      "\tGrad:tensor([-0.0055,  0.0310])\n",
      "Epoch 2686,Loss 2.930562\n",
      "\tParams: tensor([  5.3355, -17.1226])\n",
      "\tGrad:tensor([-0.0055,  0.0310])\n",
      "Epoch 2687,Loss 2.930552\n",
      "\tParams: tensor([  5.3356, -17.1229])\n",
      "\tGrad:tensor([-0.0055,  0.0309])\n",
      "Epoch 2688,Loss 2.930543\n",
      "\tParams: tensor([  5.3356, -17.1232])\n",
      "\tGrad:tensor([-0.0055,  0.0309])\n",
      "Epoch 2689,Loss 2.930534\n",
      "\tParams: tensor([  5.3357, -17.1236])\n",
      "\tGrad:tensor([-0.0055,  0.0308])\n",
      "Epoch 2690,Loss 2.930523\n",
      "\tParams: tensor([  5.3358, -17.1239])\n",
      "\tGrad:tensor([-0.0054,  0.0308])\n",
      "Epoch 2691,Loss 2.930514\n",
      "\tParams: tensor([  5.3358, -17.1242])\n",
      "\tGrad:tensor([-0.0054,  0.0307])\n",
      "Epoch 2692,Loss 2.930502\n",
      "\tParams: tensor([  5.3359, -17.1245])\n",
      "\tGrad:tensor([-0.0054,  0.0307])\n",
      "Epoch 2693,Loss 2.930493\n",
      "\tParams: tensor([  5.3359, -17.1248])\n",
      "\tGrad:tensor([-0.0054,  0.0306])\n",
      "Epoch 2694,Loss 2.930482\n",
      "\tParams: tensor([  5.3360, -17.1251])\n",
      "\tGrad:tensor([-0.0054,  0.0306])\n",
      "Epoch 2695,Loss 2.930474\n",
      "\tParams: tensor([  5.3360, -17.1254])\n",
      "\tGrad:tensor([-0.0054,  0.0305])\n",
      "Epoch 2696,Loss 2.930464\n",
      "\tParams: tensor([  5.3361, -17.1257])\n",
      "\tGrad:tensor([-0.0054,  0.0305])\n",
      "Epoch 2697,Loss 2.930454\n",
      "\tParams: tensor([  5.3361, -17.1260])\n",
      "\tGrad:tensor([-0.0054,  0.0304])\n",
      "Epoch 2698,Loss 2.930445\n",
      "\tParams: tensor([  5.3362, -17.1263])\n",
      "\tGrad:tensor([-0.0054,  0.0304])\n",
      "Epoch 2699,Loss 2.930436\n",
      "\tParams: tensor([  5.3362, -17.1266])\n",
      "\tGrad:tensor([-0.0054,  0.0303])\n",
      "Epoch 2700,Loss 2.930426\n",
      "\tParams: tensor([  5.3363, -17.1269])\n",
      "\tGrad:tensor([-0.0054,  0.0303])\n",
      "Epoch 2701,Loss 2.930416\n",
      "\tParams: tensor([  5.3364, -17.1272])\n",
      "\tGrad:tensor([-0.0054,  0.0302])\n",
      "Epoch 2702,Loss 2.930408\n",
      "\tParams: tensor([  5.3364, -17.1275])\n",
      "\tGrad:tensor([-0.0053,  0.0302])\n",
      "Epoch 2703,Loss 2.930398\n",
      "\tParams: tensor([  5.3365, -17.1278])\n",
      "\tGrad:tensor([-0.0053,  0.0301])\n",
      "Epoch 2704,Loss 2.930388\n",
      "\tParams: tensor([  5.3365, -17.1281])\n",
      "\tGrad:tensor([-0.0053,  0.0301])\n",
      "Epoch 2705,Loss 2.930380\n",
      "\tParams: tensor([  5.3366, -17.1284])\n",
      "\tGrad:tensor([-0.0053,  0.0300])\n",
      "Epoch 2706,Loss 2.930370\n",
      "\tParams: tensor([  5.3366, -17.1287])\n",
      "\tGrad:tensor([-0.0053,  0.0300])\n",
      "Epoch 2707,Loss 2.930360\n",
      "\tParams: tensor([  5.3367, -17.1290])\n",
      "\tGrad:tensor([-0.0053,  0.0299])\n",
      "Epoch 2708,Loss 2.930353\n",
      "\tParams: tensor([  5.3367, -17.1293])\n",
      "\tGrad:tensor([-0.0053,  0.0299])\n",
      "Epoch 2709,Loss 2.930342\n",
      "\tParams: tensor([  5.3368, -17.1296])\n",
      "\tGrad:tensor([-0.0053,  0.0298])\n",
      "Epoch 2710,Loss 2.930335\n",
      "\tParams: tensor([  5.3368, -17.1299])\n",
      "\tGrad:tensor([-0.0053,  0.0298])\n",
      "Epoch 2711,Loss 2.930325\n",
      "\tParams: tensor([  5.3369, -17.1302])\n",
      "\tGrad:tensor([-0.0053,  0.0297])\n",
      "Epoch 2712,Loss 2.930315\n",
      "\tParams: tensor([  5.3369, -17.1305])\n",
      "\tGrad:tensor([-0.0053,  0.0297])\n",
      "Epoch 2713,Loss 2.930306\n",
      "\tParams: tensor([  5.3370, -17.1308])\n",
      "\tGrad:tensor([-0.0052,  0.0296])\n",
      "Epoch 2714,Loss 2.930298\n",
      "\tParams: tensor([  5.3370, -17.1311])\n",
      "\tGrad:tensor([-0.0052,  0.0296])\n",
      "Epoch 2715,Loss 2.930288\n",
      "\tParams: tensor([  5.3371, -17.1314])\n",
      "\tGrad:tensor([-0.0052,  0.0295])\n",
      "Epoch 2716,Loss 2.930279\n",
      "\tParams: tensor([  5.3371, -17.1317])\n",
      "\tGrad:tensor([-0.0052,  0.0295])\n",
      "Epoch 2717,Loss 2.930270\n",
      "\tParams: tensor([  5.3372, -17.1320])\n",
      "\tGrad:tensor([-0.0052,  0.0294])\n",
      "Epoch 2718,Loss 2.930262\n",
      "\tParams: tensor([  5.3372, -17.1323])\n",
      "\tGrad:tensor([-0.0052,  0.0294])\n",
      "Epoch 2719,Loss 2.930254\n",
      "\tParams: tensor([  5.3373, -17.1326])\n",
      "\tGrad:tensor([-0.0052,  0.0293])\n",
      "Epoch 2720,Loss 2.930244\n",
      "\tParams: tensor([  5.3373, -17.1329])\n",
      "\tGrad:tensor([-0.0052,  0.0293])\n",
      "Epoch 2721,Loss 2.930235\n",
      "\tParams: tensor([  5.3374, -17.1332])\n",
      "\tGrad:tensor([-0.0052,  0.0292])\n",
      "Epoch 2722,Loss 2.930226\n",
      "\tParams: tensor([  5.3375, -17.1334])\n",
      "\tGrad:tensor([-0.0052,  0.0292])\n",
      "Epoch 2723,Loss 2.930218\n",
      "\tParams: tensor([  5.3375, -17.1337])\n",
      "\tGrad:tensor([-0.0051,  0.0291])\n",
      "Epoch 2724,Loss 2.930209\n",
      "\tParams: tensor([  5.3376, -17.1340])\n",
      "\tGrad:tensor([-0.0051,  0.0291])\n",
      "Epoch 2725,Loss 2.930201\n",
      "\tParams: tensor([  5.3376, -17.1343])\n",
      "\tGrad:tensor([-0.0051,  0.0290])\n",
      "Epoch 2726,Loss 2.930190\n",
      "\tParams: tensor([  5.3377, -17.1346])\n",
      "\tGrad:tensor([-0.0051,  0.0290])\n",
      "Epoch 2727,Loss 2.930183\n",
      "\tParams: tensor([  5.3377, -17.1349])\n",
      "\tGrad:tensor([-0.0051,  0.0289])\n",
      "Epoch 2728,Loss 2.930173\n",
      "\tParams: tensor([  5.3378, -17.1352])\n",
      "\tGrad:tensor([-0.0051,  0.0289])\n",
      "Epoch 2729,Loss 2.930166\n",
      "\tParams: tensor([  5.3378, -17.1355])\n",
      "\tGrad:tensor([-0.0051,  0.0288])\n",
      "Epoch 2730,Loss 2.930156\n",
      "\tParams: tensor([  5.3379, -17.1358])\n",
      "\tGrad:tensor([-0.0051,  0.0288])\n",
      "Epoch 2731,Loss 2.930149\n",
      "\tParams: tensor([  5.3379, -17.1360])\n",
      "\tGrad:tensor([-0.0051,  0.0287])\n",
      "Epoch 2732,Loss 2.930139\n",
      "\tParams: tensor([  5.3380, -17.1363])\n",
      "\tGrad:tensor([-0.0051,  0.0287])\n",
      "Epoch 2733,Loss 2.930131\n",
      "\tParams: tensor([  5.3380, -17.1366])\n",
      "\tGrad:tensor([-0.0050,  0.0286])\n",
      "Epoch 2734,Loss 2.930123\n",
      "\tParams: tensor([  5.3381, -17.1369])\n",
      "\tGrad:tensor([-0.0050,  0.0286])\n",
      "Epoch 2735,Loss 2.930113\n",
      "\tParams: tensor([  5.3381, -17.1372])\n",
      "\tGrad:tensor([-0.0050,  0.0285])\n",
      "Epoch 2736,Loss 2.930107\n",
      "\tParams: tensor([  5.3382, -17.1375])\n",
      "\tGrad:tensor([-0.0051,  0.0285])\n",
      "Epoch 2737,Loss 2.930099\n",
      "\tParams: tensor([  5.3382, -17.1378])\n",
      "\tGrad:tensor([-0.0050,  0.0284])\n",
      "Epoch 2738,Loss 2.930090\n",
      "\tParams: tensor([  5.3383, -17.1380])\n",
      "\tGrad:tensor([-0.0050,  0.0284])\n",
      "Epoch 2739,Loss 2.930081\n",
      "\tParams: tensor([  5.3383, -17.1383])\n",
      "\tGrad:tensor([-0.0050,  0.0283])\n",
      "Epoch 2740,Loss 2.930073\n",
      "\tParams: tensor([  5.3384, -17.1386])\n",
      "\tGrad:tensor([-0.0050,  0.0283])\n",
      "Epoch 2741,Loss 2.930064\n",
      "\tParams: tensor([  5.3384, -17.1389])\n",
      "\tGrad:tensor([-0.0050,  0.0282])\n",
      "Epoch 2742,Loss 2.930056\n",
      "\tParams: tensor([  5.3385, -17.1392])\n",
      "\tGrad:tensor([-0.0050,  0.0282])\n",
      "Epoch 2743,Loss 2.930048\n",
      "\tParams: tensor([  5.3385, -17.1395])\n",
      "\tGrad:tensor([-0.0050,  0.0281])\n",
      "Epoch 2744,Loss 2.930041\n",
      "\tParams: tensor([  5.3386, -17.1397])\n",
      "\tGrad:tensor([-0.0050,  0.0281])\n",
      "Epoch 2745,Loss 2.930032\n",
      "\tParams: tensor([  5.3386, -17.1400])\n",
      "\tGrad:tensor([-0.0050,  0.0280])\n",
      "Epoch 2746,Loss 2.930022\n",
      "\tParams: tensor([  5.3387, -17.1403])\n",
      "\tGrad:tensor([-0.0050,  0.0280])\n",
      "Epoch 2747,Loss 2.930016\n",
      "\tParams: tensor([  5.3387, -17.1406])\n",
      "\tGrad:tensor([-0.0049,  0.0279])\n",
      "Epoch 2748,Loss 2.930008\n",
      "\tParams: tensor([  5.3388, -17.1409])\n",
      "\tGrad:tensor([-0.0049,  0.0279])\n",
      "Epoch 2749,Loss 2.930000\n",
      "\tParams: tensor([  5.3388, -17.1411])\n",
      "\tGrad:tensor([-0.0049,  0.0279])\n",
      "Epoch 2750,Loss 2.929992\n",
      "\tParams: tensor([  5.3389, -17.1414])\n",
      "\tGrad:tensor([-0.0049,  0.0278])\n",
      "Epoch 2751,Loss 2.929983\n",
      "\tParams: tensor([  5.3389, -17.1417])\n",
      "\tGrad:tensor([-0.0049,  0.0278])\n",
      "Epoch 2752,Loss 2.929975\n",
      "\tParams: tensor([  5.3390, -17.1420])\n",
      "\tGrad:tensor([-0.0049,  0.0277])\n",
      "Epoch 2753,Loss 2.929968\n",
      "\tParams: tensor([  5.3390, -17.1422])\n",
      "\tGrad:tensor([-0.0049,  0.0277])\n",
      "Epoch 2754,Loss 2.929960\n",
      "\tParams: tensor([  5.3391, -17.1425])\n",
      "\tGrad:tensor([-0.0049,  0.0276])\n",
      "Epoch 2755,Loss 2.929953\n",
      "\tParams: tensor([  5.3391, -17.1428])\n",
      "\tGrad:tensor([-0.0049,  0.0276])\n",
      "Epoch 2756,Loss 2.929945\n",
      "\tParams: tensor([  5.3392, -17.1431])\n",
      "\tGrad:tensor([-0.0049,  0.0275])\n",
      "Epoch 2757,Loss 2.929936\n",
      "\tParams: tensor([  5.3392, -17.1433])\n",
      "\tGrad:tensor([-0.0049,  0.0275])\n",
      "Epoch 2758,Loss 2.929929\n",
      "\tParams: tensor([  5.3392, -17.1436])\n",
      "\tGrad:tensor([-0.0049,  0.0274])\n",
      "Epoch 2759,Loss 2.929921\n",
      "\tParams: tensor([  5.3393, -17.1439])\n",
      "\tGrad:tensor([-0.0048,  0.0274])\n",
      "Epoch 2760,Loss 2.929914\n",
      "\tParams: tensor([  5.3393, -17.1442])\n",
      "\tGrad:tensor([-0.0049,  0.0273])\n",
      "Epoch 2761,Loss 2.929905\n",
      "\tParams: tensor([  5.3394, -17.1444])\n",
      "\tGrad:tensor([-0.0048,  0.0273])\n",
      "Epoch 2762,Loss 2.929896\n",
      "\tParams: tensor([  5.3394, -17.1447])\n",
      "\tGrad:tensor([-0.0048,  0.0272])\n",
      "Epoch 2763,Loss 2.929891\n",
      "\tParams: tensor([  5.3395, -17.1450])\n",
      "\tGrad:tensor([-0.0048,  0.0272])\n",
      "Epoch 2764,Loss 2.929882\n",
      "\tParams: tensor([  5.3395, -17.1453])\n",
      "\tGrad:tensor([-0.0048,  0.0271])\n",
      "Epoch 2765,Loss 2.929875\n",
      "\tParams: tensor([  5.3396, -17.1455])\n",
      "\tGrad:tensor([-0.0048,  0.0271])\n",
      "Epoch 2766,Loss 2.929868\n",
      "\tParams: tensor([  5.3396, -17.1458])\n",
      "\tGrad:tensor([-0.0048,  0.0271])\n",
      "Epoch 2767,Loss 2.929859\n",
      "\tParams: tensor([  5.3397, -17.1461])\n",
      "\tGrad:tensor([-0.0048,  0.0270])\n",
      "Epoch 2768,Loss 2.929852\n",
      "\tParams: tensor([  5.3397, -17.1463])\n",
      "\tGrad:tensor([-0.0048,  0.0270])\n",
      "Epoch 2769,Loss 2.929845\n",
      "\tParams: tensor([  5.3398, -17.1466])\n",
      "\tGrad:tensor([-0.0048,  0.0269])\n",
      "Epoch 2770,Loss 2.929838\n",
      "\tParams: tensor([  5.3398, -17.1469])\n",
      "\tGrad:tensor([-0.0047,  0.0269])\n",
      "Epoch 2771,Loss 2.929830\n",
      "\tParams: tensor([  5.3399, -17.1471])\n",
      "\tGrad:tensor([-0.0047,  0.0268])\n",
      "Epoch 2772,Loss 2.929822\n",
      "\tParams: tensor([  5.3399, -17.1474])\n",
      "\tGrad:tensor([-0.0047,  0.0268])\n",
      "Epoch 2773,Loss 2.929816\n",
      "\tParams: tensor([  5.3400, -17.1477])\n",
      "\tGrad:tensor([-0.0047,  0.0267])\n",
      "Epoch 2774,Loss 2.929807\n",
      "\tParams: tensor([  5.3400, -17.1479])\n",
      "\tGrad:tensor([-0.0047,  0.0267])\n",
      "Epoch 2775,Loss 2.929800\n",
      "\tParams: tensor([  5.3401, -17.1482])\n",
      "\tGrad:tensor([-0.0047,  0.0266])\n",
      "Epoch 2776,Loss 2.929794\n",
      "\tParams: tensor([  5.3401, -17.1485])\n",
      "\tGrad:tensor([-0.0047,  0.0266])\n",
      "Epoch 2777,Loss 2.929786\n",
      "\tParams: tensor([  5.3402, -17.1487])\n",
      "\tGrad:tensor([-0.0047,  0.0266])\n",
      "Epoch 2778,Loss 2.929778\n",
      "\tParams: tensor([  5.3402, -17.1490])\n",
      "\tGrad:tensor([-0.0047,  0.0265])\n",
      "Epoch 2779,Loss 2.929771\n",
      "\tParams: tensor([  5.3402, -17.1493])\n",
      "\tGrad:tensor([-0.0047,  0.0265])\n",
      "Epoch 2780,Loss 2.929765\n",
      "\tParams: tensor([  5.3403, -17.1495])\n",
      "\tGrad:tensor([-0.0047,  0.0264])\n",
      "Epoch 2781,Loss 2.929757\n",
      "\tParams: tensor([  5.3403, -17.1498])\n",
      "\tGrad:tensor([-0.0047,  0.0264])\n",
      "Epoch 2782,Loss 2.929750\n",
      "\tParams: tensor([  5.3404, -17.1501])\n",
      "\tGrad:tensor([-0.0046,  0.0263])\n",
      "Epoch 2783,Loss 2.929743\n",
      "\tParams: tensor([  5.3404, -17.1503])\n",
      "\tGrad:tensor([-0.0046,  0.0263])\n",
      "Epoch 2784,Loss 2.929735\n",
      "\tParams: tensor([  5.3405, -17.1506])\n",
      "\tGrad:tensor([-0.0046,  0.0262])\n",
      "Epoch 2785,Loss 2.929729\n",
      "\tParams: tensor([  5.3405, -17.1508])\n",
      "\tGrad:tensor([-0.0047,  0.0262])\n",
      "Epoch 2786,Loss 2.929722\n",
      "\tParams: tensor([  5.3406, -17.1511])\n",
      "\tGrad:tensor([-0.0046,  0.0262])\n",
      "Epoch 2787,Loss 2.929714\n",
      "\tParams: tensor([  5.3406, -17.1514])\n",
      "\tGrad:tensor([-0.0046,  0.0261])\n",
      "Epoch 2788,Loss 2.929707\n",
      "\tParams: tensor([  5.3407, -17.1516])\n",
      "\tGrad:tensor([-0.0046,  0.0261])\n",
      "Epoch 2789,Loss 2.929701\n",
      "\tParams: tensor([  5.3407, -17.1519])\n",
      "\tGrad:tensor([-0.0046,  0.0260])\n",
      "Epoch 2790,Loss 2.929692\n",
      "\tParams: tensor([  5.3408, -17.1522])\n",
      "\tGrad:tensor([-0.0046,  0.0260])\n",
      "Epoch 2791,Loss 2.929685\n",
      "\tParams: tensor([  5.3408, -17.1524])\n",
      "\tGrad:tensor([-0.0046,  0.0259])\n",
      "Epoch 2792,Loss 2.929681\n",
      "\tParams: tensor([  5.3408, -17.1527])\n",
      "\tGrad:tensor([-0.0046,  0.0259])\n",
      "Epoch 2793,Loss 2.929672\n",
      "\tParams: tensor([  5.3409, -17.1529])\n",
      "\tGrad:tensor([-0.0046,  0.0258])\n",
      "Epoch 2794,Loss 2.929666\n",
      "\tParams: tensor([  5.3409, -17.1532])\n",
      "\tGrad:tensor([-0.0046,  0.0258])\n",
      "Epoch 2795,Loss 2.929659\n",
      "\tParams: tensor([  5.3410, -17.1534])\n",
      "\tGrad:tensor([-0.0045,  0.0258])\n",
      "Epoch 2796,Loss 2.929653\n",
      "\tParams: tensor([  5.3410, -17.1537])\n",
      "\tGrad:tensor([-0.0045,  0.0257])\n",
      "Epoch 2797,Loss 2.929646\n",
      "\tParams: tensor([  5.3411, -17.1540])\n",
      "\tGrad:tensor([-0.0045,  0.0257])\n",
      "Epoch 2798,Loss 2.929638\n",
      "\tParams: tensor([  5.3411, -17.1542])\n",
      "\tGrad:tensor([-0.0045,  0.0256])\n",
      "Epoch 2799,Loss 2.929632\n",
      "\tParams: tensor([  5.3412, -17.1545])\n",
      "\tGrad:tensor([-0.0045,  0.0256])\n",
      "Epoch 2800,Loss 2.929626\n",
      "\tParams: tensor([  5.3412, -17.1547])\n",
      "\tGrad:tensor([-0.0045,  0.0255])\n",
      "Epoch 2801,Loss 2.929620\n",
      "\tParams: tensor([  5.3413, -17.1550])\n",
      "\tGrad:tensor([-0.0045,  0.0255])\n",
      "Epoch 2802,Loss 2.929611\n",
      "\tParams: tensor([  5.3413, -17.1552])\n",
      "\tGrad:tensor([-0.0045,  0.0254])\n",
      "Epoch 2803,Loss 2.929605\n",
      "\tParams: tensor([  5.3413, -17.1555])\n",
      "\tGrad:tensor([-0.0045,  0.0254])\n",
      "Epoch 2804,Loss 2.929600\n",
      "\tParams: tensor([  5.3414, -17.1557])\n",
      "\tGrad:tensor([-0.0045,  0.0254])\n",
      "Epoch 2805,Loss 2.929592\n",
      "\tParams: tensor([  5.3414, -17.1560])\n",
      "\tGrad:tensor([-0.0045,  0.0253])\n",
      "Epoch 2806,Loss 2.929586\n",
      "\tParams: tensor([  5.3415, -17.1562])\n",
      "\tGrad:tensor([-0.0045,  0.0253])\n",
      "Epoch 2807,Loss 2.929579\n",
      "\tParams: tensor([  5.3415, -17.1565])\n",
      "\tGrad:tensor([-0.0045,  0.0252])\n",
      "Epoch 2808,Loss 2.929572\n",
      "\tParams: tensor([  5.3416, -17.1568])\n",
      "\tGrad:tensor([-0.0044,  0.0252])\n",
      "Epoch 2809,Loss 2.929566\n",
      "\tParams: tensor([  5.3416, -17.1570])\n",
      "\tGrad:tensor([-0.0044,  0.0251])\n",
      "Epoch 2810,Loss 2.929559\n",
      "\tParams: tensor([  5.3417, -17.1573])\n",
      "\tGrad:tensor([-0.0044,  0.0251])\n",
      "Epoch 2811,Loss 2.929551\n",
      "\tParams: tensor([  5.3417, -17.1575])\n",
      "\tGrad:tensor([-0.0044,  0.0251])\n",
      "Epoch 2812,Loss 2.929545\n",
      "\tParams: tensor([  5.3417, -17.1578])\n",
      "\tGrad:tensor([-0.0044,  0.0250])\n",
      "Epoch 2813,Loss 2.929540\n",
      "\tParams: tensor([  5.3418, -17.1580])\n",
      "\tGrad:tensor([-0.0044,  0.0250])\n",
      "Epoch 2814,Loss 2.929533\n",
      "\tParams: tensor([  5.3418, -17.1583])\n",
      "\tGrad:tensor([-0.0044,  0.0249])\n",
      "Epoch 2815,Loss 2.929528\n",
      "\tParams: tensor([  5.3419, -17.1585])\n",
      "\tGrad:tensor([-0.0044,  0.0249])\n",
      "Epoch 2816,Loss 2.929521\n",
      "\tParams: tensor([  5.3419, -17.1588])\n",
      "\tGrad:tensor([-0.0044,  0.0249])\n",
      "Epoch 2817,Loss 2.929513\n",
      "\tParams: tensor([  5.3420, -17.1590])\n",
      "\tGrad:tensor([-0.0044,  0.0248])\n",
      "Epoch 2818,Loss 2.929507\n",
      "\tParams: tensor([  5.3420, -17.1592])\n",
      "\tGrad:tensor([-0.0043,  0.0248])\n",
      "Epoch 2819,Loss 2.929501\n",
      "\tParams: tensor([  5.3421, -17.1595])\n",
      "\tGrad:tensor([-0.0044,  0.0247])\n",
      "Epoch 2820,Loss 2.929496\n",
      "\tParams: tensor([  5.3421, -17.1597])\n",
      "\tGrad:tensor([-0.0044,  0.0247])\n",
      "Epoch 2821,Loss 2.929489\n",
      "\tParams: tensor([  5.3421, -17.1600])\n",
      "\tGrad:tensor([-0.0044,  0.0246])\n",
      "Epoch 2822,Loss 2.929482\n",
      "\tParams: tensor([  5.3422, -17.1602])\n",
      "\tGrad:tensor([-0.0043,  0.0246])\n",
      "Epoch 2823,Loss 2.929476\n",
      "\tParams: tensor([  5.3422, -17.1605])\n",
      "\tGrad:tensor([-0.0043,  0.0246])\n",
      "Epoch 2824,Loss 2.929471\n",
      "\tParams: tensor([  5.3423, -17.1607])\n",
      "\tGrad:tensor([-0.0043,  0.0245])\n",
      "Epoch 2825,Loss 2.929463\n",
      "\tParams: tensor([  5.3423, -17.1610])\n",
      "\tGrad:tensor([-0.0043,  0.0245])\n",
      "Epoch 2826,Loss 2.929458\n",
      "\tParams: tensor([  5.3424, -17.1612])\n",
      "\tGrad:tensor([-0.0043,  0.0244])\n",
      "Epoch 2827,Loss 2.929452\n",
      "\tParams: tensor([  5.3424, -17.1615])\n",
      "\tGrad:tensor([-0.0043,  0.0244])\n",
      "Epoch 2828,Loss 2.929445\n",
      "\tParams: tensor([  5.3424, -17.1617])\n",
      "\tGrad:tensor([-0.0043,  0.0243])\n",
      "Epoch 2829,Loss 2.929439\n",
      "\tParams: tensor([  5.3425, -17.1619])\n",
      "\tGrad:tensor([-0.0043,  0.0243])\n",
      "Epoch 2830,Loss 2.929433\n",
      "\tParams: tensor([  5.3425, -17.1622])\n",
      "\tGrad:tensor([-0.0043,  0.0243])\n",
      "Epoch 2831,Loss 2.929427\n",
      "\tParams: tensor([  5.3426, -17.1624])\n",
      "\tGrad:tensor([-0.0043,  0.0242])\n",
      "Epoch 2832,Loss 2.929421\n",
      "\tParams: tensor([  5.3426, -17.1627])\n",
      "\tGrad:tensor([-0.0043,  0.0242])\n",
      "Epoch 2833,Loss 2.929415\n",
      "\tParams: tensor([  5.3427, -17.1629])\n",
      "\tGrad:tensor([-0.0043,  0.0241])\n",
      "Epoch 2834,Loss 2.929409\n",
      "\tParams: tensor([  5.3427, -17.1632])\n",
      "\tGrad:tensor([-0.0043,  0.0241])\n",
      "Epoch 2835,Loss 2.929404\n",
      "\tParams: tensor([  5.3427, -17.1634])\n",
      "\tGrad:tensor([-0.0043,  0.0241])\n",
      "Epoch 2836,Loss 2.929396\n",
      "\tParams: tensor([  5.3428, -17.1636])\n",
      "\tGrad:tensor([-0.0042,  0.0240])\n",
      "Epoch 2837,Loss 2.929391\n",
      "\tParams: tensor([  5.3428, -17.1639])\n",
      "\tGrad:tensor([-0.0042,  0.0240])\n",
      "Epoch 2838,Loss 2.929383\n",
      "\tParams: tensor([  5.3429, -17.1641])\n",
      "\tGrad:tensor([-0.0042,  0.0239])\n",
      "Epoch 2839,Loss 2.929380\n",
      "\tParams: tensor([  5.3429, -17.1644])\n",
      "\tGrad:tensor([-0.0042,  0.0239])\n",
      "Epoch 2840,Loss 2.929373\n",
      "\tParams: tensor([  5.3430, -17.1646])\n",
      "\tGrad:tensor([-0.0042,  0.0239])\n",
      "Epoch 2841,Loss 2.929368\n",
      "\tParams: tensor([  5.3430, -17.1648])\n",
      "\tGrad:tensor([-0.0042,  0.0238])\n",
      "Epoch 2842,Loss 2.929361\n",
      "\tParams: tensor([  5.3430, -17.1651])\n",
      "\tGrad:tensor([-0.0042,  0.0238])\n",
      "Epoch 2843,Loss 2.929356\n",
      "\tParams: tensor([  5.3431, -17.1653])\n",
      "\tGrad:tensor([-0.0042,  0.0237])\n",
      "Epoch 2844,Loss 2.929351\n",
      "\tParams: tensor([  5.3431, -17.1655])\n",
      "\tGrad:tensor([-0.0042,  0.0237])\n",
      "Epoch 2845,Loss 2.929344\n",
      "\tParams: tensor([  5.3432, -17.1658])\n",
      "\tGrad:tensor([-0.0042,  0.0237])\n",
      "Epoch 2846,Loss 2.929338\n",
      "\tParams: tensor([  5.3432, -17.1660])\n",
      "\tGrad:tensor([-0.0042,  0.0236])\n",
      "Epoch 2847,Loss 2.929332\n",
      "\tParams: tensor([  5.3432, -17.1662])\n",
      "\tGrad:tensor([-0.0042,  0.0236])\n",
      "Epoch 2848,Loss 2.929328\n",
      "\tParams: tensor([  5.3433, -17.1665])\n",
      "\tGrad:tensor([-0.0042,  0.0235])\n",
      "Epoch 2849,Loss 2.929321\n",
      "\tParams: tensor([  5.3433, -17.1667])\n",
      "\tGrad:tensor([-0.0041,  0.0235])\n",
      "Epoch 2850,Loss 2.929316\n",
      "\tParams: tensor([  5.3434, -17.1670])\n",
      "\tGrad:tensor([-0.0041,  0.0235])\n",
      "Epoch 2851,Loss 2.929309\n",
      "\tParams: tensor([  5.3434, -17.1672])\n",
      "\tGrad:tensor([-0.0041,  0.0234])\n",
      "Epoch 2852,Loss 2.929304\n",
      "\tParams: tensor([  5.3435, -17.1674])\n",
      "\tGrad:tensor([-0.0041,  0.0234])\n",
      "Epoch 2853,Loss 2.929300\n",
      "\tParams: tensor([  5.3435, -17.1677])\n",
      "\tGrad:tensor([-0.0041,  0.0233])\n",
      "Epoch 2854,Loss 2.929293\n",
      "\tParams: tensor([  5.3435, -17.1679])\n",
      "\tGrad:tensor([-0.0041,  0.0233])\n",
      "Epoch 2855,Loss 2.929288\n",
      "\tParams: tensor([  5.3436, -17.1681])\n",
      "\tGrad:tensor([-0.0041,  0.0233])\n",
      "Epoch 2856,Loss 2.929282\n",
      "\tParams: tensor([  5.3436, -17.1684])\n",
      "\tGrad:tensor([-0.0041,  0.0232])\n",
      "Epoch 2857,Loss 2.929277\n",
      "\tParams: tensor([  5.3437, -17.1686])\n",
      "\tGrad:tensor([-0.0041,  0.0232])\n",
      "Epoch 2858,Loss 2.929271\n",
      "\tParams: tensor([  5.3437, -17.1688])\n",
      "\tGrad:tensor([-0.0041,  0.0231])\n",
      "Epoch 2859,Loss 2.929266\n",
      "\tParams: tensor([  5.3437, -17.1690])\n",
      "\tGrad:tensor([-0.0041,  0.0231])\n",
      "Epoch 2860,Loss 2.929260\n",
      "\tParams: tensor([  5.3438, -17.1693])\n",
      "\tGrad:tensor([-0.0041,  0.0231])\n",
      "Epoch 2861,Loss 2.929255\n",
      "\tParams: tensor([  5.3438, -17.1695])\n",
      "\tGrad:tensor([-0.0041,  0.0230])\n",
      "Epoch 2862,Loss 2.929250\n",
      "\tParams: tensor([  5.3439, -17.1697])\n",
      "\tGrad:tensor([-0.0041,  0.0230])\n",
      "Epoch 2863,Loss 2.929244\n",
      "\tParams: tensor([  5.3439, -17.1700])\n",
      "\tGrad:tensor([-0.0040,  0.0229])\n",
      "Epoch 2864,Loss 2.929238\n",
      "\tParams: tensor([  5.3439, -17.1702])\n",
      "\tGrad:tensor([-0.0040,  0.0229])\n",
      "Epoch 2865,Loss 2.929234\n",
      "\tParams: tensor([  5.3440, -17.1704])\n",
      "\tGrad:tensor([-0.0040,  0.0229])\n",
      "Epoch 2866,Loss 2.929228\n",
      "\tParams: tensor([  5.3440, -17.1707])\n",
      "\tGrad:tensor([-0.0040,  0.0228])\n",
      "Epoch 2867,Loss 2.929222\n",
      "\tParams: tensor([  5.3441, -17.1709])\n",
      "\tGrad:tensor([-0.0040,  0.0228])\n",
      "Epoch 2868,Loss 2.929217\n",
      "\tParams: tensor([  5.3441, -17.1711])\n",
      "\tGrad:tensor([-0.0040,  0.0227])\n",
      "Epoch 2869,Loss 2.929211\n",
      "\tParams: tensor([  5.3441, -17.1713])\n",
      "\tGrad:tensor([-0.0040,  0.0227])\n",
      "Epoch 2870,Loss 2.929208\n",
      "\tParams: tensor([  5.3442, -17.1716])\n",
      "\tGrad:tensor([-0.0040,  0.0227])\n",
      "Epoch 2871,Loss 2.929201\n",
      "\tParams: tensor([  5.3442, -17.1718])\n",
      "\tGrad:tensor([-0.0040,  0.0226])\n",
      "Epoch 2872,Loss 2.929195\n",
      "\tParams: tensor([  5.3443, -17.1720])\n",
      "\tGrad:tensor([-0.0040,  0.0226])\n",
      "Epoch 2873,Loss 2.929191\n",
      "\tParams: tensor([  5.3443, -17.1722])\n",
      "\tGrad:tensor([-0.0040,  0.0226])\n",
      "Epoch 2874,Loss 2.929185\n",
      "\tParams: tensor([  5.3443, -17.1725])\n",
      "\tGrad:tensor([-0.0040,  0.0225])\n",
      "Epoch 2875,Loss 2.929180\n",
      "\tParams: tensor([  5.3444, -17.1727])\n",
      "\tGrad:tensor([-0.0040,  0.0225])\n",
      "Epoch 2876,Loss 2.929175\n",
      "\tParams: tensor([  5.3444, -17.1729])\n",
      "\tGrad:tensor([-0.0040,  0.0224])\n",
      "Epoch 2877,Loss 2.929170\n",
      "\tParams: tensor([  5.3445, -17.1731])\n",
      "\tGrad:tensor([-0.0040,  0.0224])\n",
      "Epoch 2878,Loss 2.929165\n",
      "\tParams: tensor([  5.3445, -17.1734])\n",
      "\tGrad:tensor([-0.0040,  0.0224])\n",
      "Epoch 2879,Loss 2.929160\n",
      "\tParams: tensor([  5.3445, -17.1736])\n",
      "\tGrad:tensor([-0.0039,  0.0223])\n",
      "Epoch 2880,Loss 2.929155\n",
      "\tParams: tensor([  5.3446, -17.1738])\n",
      "\tGrad:tensor([-0.0039,  0.0223])\n",
      "Epoch 2881,Loss 2.929149\n",
      "\tParams: tensor([  5.3446, -17.1740])\n",
      "\tGrad:tensor([-0.0039,  0.0223])\n",
      "Epoch 2882,Loss 2.929143\n",
      "\tParams: tensor([  5.3447, -17.1742])\n",
      "\tGrad:tensor([-0.0039,  0.0222])\n",
      "Epoch 2883,Loss 2.929139\n",
      "\tParams: tensor([  5.3447, -17.1745])\n",
      "\tGrad:tensor([-0.0039,  0.0222])\n",
      "Epoch 2884,Loss 2.929133\n",
      "\tParams: tensor([  5.3447, -17.1747])\n",
      "\tGrad:tensor([-0.0039,  0.0221])\n",
      "Epoch 2885,Loss 2.929128\n",
      "\tParams: tensor([  5.3448, -17.1749])\n",
      "\tGrad:tensor([-0.0039,  0.0221])\n",
      "Epoch 2886,Loss 2.929122\n",
      "\tParams: tensor([  5.3448, -17.1751])\n",
      "\tGrad:tensor([-0.0039,  0.0221])\n",
      "Epoch 2887,Loss 2.929119\n",
      "\tParams: tensor([  5.3449, -17.1754])\n",
      "\tGrad:tensor([-0.0039,  0.0220])\n",
      "Epoch 2888,Loss 2.929113\n",
      "\tParams: tensor([  5.3449, -17.1756])\n",
      "\tGrad:tensor([-0.0039,  0.0220])\n",
      "Epoch 2889,Loss 2.929108\n",
      "\tParams: tensor([  5.3449, -17.1758])\n",
      "\tGrad:tensor([-0.0039,  0.0220])\n",
      "Epoch 2890,Loss 2.929104\n",
      "\tParams: tensor([  5.3450, -17.1760])\n",
      "\tGrad:tensor([-0.0039,  0.0219])\n",
      "Epoch 2891,Loss 2.929099\n",
      "\tParams: tensor([  5.3450, -17.1762])\n",
      "\tGrad:tensor([-0.0039,  0.0219])\n",
      "Epoch 2892,Loss 2.929093\n",
      "\tParams: tensor([  5.3450, -17.1764])\n",
      "\tGrad:tensor([-0.0039,  0.0218])\n",
      "Epoch 2893,Loss 2.929088\n",
      "\tParams: tensor([  5.3451, -17.1767])\n",
      "\tGrad:tensor([-0.0039,  0.0218])\n",
      "Epoch 2894,Loss 2.929083\n",
      "\tParams: tensor([  5.3451, -17.1769])\n",
      "\tGrad:tensor([-0.0038,  0.0218])\n",
      "Epoch 2895,Loss 2.929079\n",
      "\tParams: tensor([  5.3452, -17.1771])\n",
      "\tGrad:tensor([-0.0038,  0.0217])\n",
      "Epoch 2896,Loss 2.929074\n",
      "\tParams: tensor([  5.3452, -17.1773])\n",
      "\tGrad:tensor([-0.0038,  0.0217])\n",
      "Epoch 2897,Loss 2.929069\n",
      "\tParams: tensor([  5.3452, -17.1775])\n",
      "\tGrad:tensor([-0.0038,  0.0217])\n",
      "Epoch 2898,Loss 2.929065\n",
      "\tParams: tensor([  5.3453, -17.1777])\n",
      "\tGrad:tensor([-0.0038,  0.0216])\n",
      "Epoch 2899,Loss 2.929058\n",
      "\tParams: tensor([  5.3453, -17.1780])\n",
      "\tGrad:tensor([-0.0038,  0.0216])\n",
      "Epoch 2900,Loss 2.929054\n",
      "\tParams: tensor([  5.3454, -17.1782])\n",
      "\tGrad:tensor([-0.0038,  0.0215])\n",
      "Epoch 2901,Loss 2.929050\n",
      "\tParams: tensor([  5.3454, -17.1784])\n",
      "\tGrad:tensor([-0.0038,  0.0215])\n",
      "Epoch 2902,Loss 2.929044\n",
      "\tParams: tensor([  5.3454, -17.1786])\n",
      "\tGrad:tensor([-0.0038,  0.0215])\n",
      "Epoch 2903,Loss 2.929041\n",
      "\tParams: tensor([  5.3455, -17.1788])\n",
      "\tGrad:tensor([-0.0038,  0.0214])\n",
      "Epoch 2904,Loss 2.929036\n",
      "\tParams: tensor([  5.3455, -17.1790])\n",
      "\tGrad:tensor([-0.0038,  0.0214])\n",
      "Epoch 2905,Loss 2.929031\n",
      "\tParams: tensor([  5.3455, -17.1793])\n",
      "\tGrad:tensor([-0.0038,  0.0214])\n",
      "Epoch 2906,Loss 2.929025\n",
      "\tParams: tensor([  5.3456, -17.1795])\n",
      "\tGrad:tensor([-0.0038,  0.0213])\n",
      "Epoch 2907,Loss 2.929021\n",
      "\tParams: tensor([  5.3456, -17.1797])\n",
      "\tGrad:tensor([-0.0038,  0.0213])\n",
      "Epoch 2908,Loss 2.929017\n",
      "\tParams: tensor([  5.3457, -17.1799])\n",
      "\tGrad:tensor([-0.0037,  0.0213])\n",
      "Epoch 2909,Loss 2.929012\n",
      "\tParams: tensor([  5.3457, -17.1801])\n",
      "\tGrad:tensor([-0.0037,  0.0212])\n",
      "Epoch 2910,Loss 2.929007\n",
      "\tParams: tensor([  5.3457, -17.1803])\n",
      "\tGrad:tensor([-0.0037,  0.0212])\n",
      "Epoch 2911,Loss 2.929003\n",
      "\tParams: tensor([  5.3458, -17.1805])\n",
      "\tGrad:tensor([-0.0037,  0.0211])\n",
      "Epoch 2912,Loss 2.928999\n",
      "\tParams: tensor([  5.3458, -17.1807])\n",
      "\tGrad:tensor([-0.0037,  0.0211])\n",
      "Epoch 2913,Loss 2.928993\n",
      "\tParams: tensor([  5.3458, -17.1809])\n",
      "\tGrad:tensor([-0.0037,  0.0211])\n",
      "Epoch 2914,Loss 2.928989\n",
      "\tParams: tensor([  5.3459, -17.1812])\n",
      "\tGrad:tensor([-0.0037,  0.0210])\n",
      "Epoch 2915,Loss 2.928985\n",
      "\tParams: tensor([  5.3459, -17.1814])\n",
      "\tGrad:tensor([-0.0037,  0.0210])\n",
      "Epoch 2916,Loss 2.928980\n",
      "\tParams: tensor([  5.3460, -17.1816])\n",
      "\tGrad:tensor([-0.0037,  0.0210])\n",
      "Epoch 2917,Loss 2.928976\n",
      "\tParams: tensor([  5.3460, -17.1818])\n",
      "\tGrad:tensor([-0.0037,  0.0209])\n",
      "Epoch 2918,Loss 2.928971\n",
      "\tParams: tensor([  5.3460, -17.1820])\n",
      "\tGrad:tensor([-0.0037,  0.0209])\n",
      "Epoch 2919,Loss 2.928967\n",
      "\tParams: tensor([  5.3461, -17.1822])\n",
      "\tGrad:tensor([-0.0037,  0.0209])\n",
      "Epoch 2920,Loss 2.928962\n",
      "\tParams: tensor([  5.3461, -17.1824])\n",
      "\tGrad:tensor([-0.0037,  0.0208])\n",
      "Epoch 2921,Loss 2.928958\n",
      "\tParams: tensor([  5.3461, -17.1826])\n",
      "\tGrad:tensor([-0.0037,  0.0208])\n",
      "Epoch 2922,Loss 2.928953\n",
      "\tParams: tensor([  5.3462, -17.1828])\n",
      "\tGrad:tensor([-0.0037,  0.0208])\n",
      "Epoch 2923,Loss 2.928947\n",
      "\tParams: tensor([  5.3462, -17.1830])\n",
      "\tGrad:tensor([-0.0036,  0.0207])\n",
      "Epoch 2924,Loss 2.928943\n",
      "\tParams: tensor([  5.3462, -17.1832])\n",
      "\tGrad:tensor([-0.0037,  0.0207])\n",
      "Epoch 2925,Loss 2.928940\n",
      "\tParams: tensor([  5.3463, -17.1834])\n",
      "\tGrad:tensor([-0.0036,  0.0206])\n",
      "Epoch 2926,Loss 2.928935\n",
      "\tParams: tensor([  5.3463, -17.1837])\n",
      "\tGrad:tensor([-0.0036,  0.0206])\n",
      "Epoch 2927,Loss 2.928932\n",
      "\tParams: tensor([  5.3464, -17.1839])\n",
      "\tGrad:tensor([-0.0036,  0.0206])\n",
      "Epoch 2928,Loss 2.928926\n",
      "\tParams: tensor([  5.3464, -17.1841])\n",
      "\tGrad:tensor([-0.0036,  0.0205])\n",
      "Epoch 2929,Loss 2.928923\n",
      "\tParams: tensor([  5.3464, -17.1843])\n",
      "\tGrad:tensor([-0.0036,  0.0205])\n",
      "Epoch 2930,Loss 2.928919\n",
      "\tParams: tensor([  5.3465, -17.1845])\n",
      "\tGrad:tensor([-0.0036,  0.0205])\n",
      "Epoch 2931,Loss 2.928913\n",
      "\tParams: tensor([  5.3465, -17.1847])\n",
      "\tGrad:tensor([-0.0036,  0.0204])\n",
      "Epoch 2932,Loss 2.928909\n",
      "\tParams: tensor([  5.3465, -17.1849])\n",
      "\tGrad:tensor([-0.0036,  0.0204])\n",
      "Epoch 2933,Loss 2.928904\n",
      "\tParams: tensor([  5.3466, -17.1851])\n",
      "\tGrad:tensor([-0.0036,  0.0204])\n",
      "Epoch 2934,Loss 2.928902\n",
      "\tParams: tensor([  5.3466, -17.1853])\n",
      "\tGrad:tensor([-0.0036,  0.0203])\n",
      "Epoch 2935,Loss 2.928897\n",
      "\tParams: tensor([  5.3466, -17.1855])\n",
      "\tGrad:tensor([-0.0036,  0.0203])\n",
      "Epoch 2936,Loss 2.928893\n",
      "\tParams: tensor([  5.3467, -17.1857])\n",
      "\tGrad:tensor([-0.0036,  0.0203])\n",
      "Epoch 2937,Loss 2.928887\n",
      "\tParams: tensor([  5.3467, -17.1859])\n",
      "\tGrad:tensor([-0.0036,  0.0202])\n",
      "Epoch 2938,Loss 2.928883\n",
      "\tParams: tensor([  5.3468, -17.1861])\n",
      "\tGrad:tensor([-0.0035,  0.0202])\n",
      "Epoch 2939,Loss 2.928880\n",
      "\tParams: tensor([  5.3468, -17.1863])\n",
      "\tGrad:tensor([-0.0036,  0.0202])\n",
      "Epoch 2940,Loss 2.928878\n",
      "\tParams: tensor([  5.3468, -17.1865])\n",
      "\tGrad:tensor([-0.0036,  0.0201])\n",
      "Epoch 2941,Loss 2.928871\n",
      "\tParams: tensor([  5.3469, -17.1867])\n",
      "\tGrad:tensor([-0.0035,  0.0201])\n",
      "Epoch 2942,Loss 2.928867\n",
      "\tParams: tensor([  5.3469, -17.1869])\n",
      "\tGrad:tensor([-0.0035,  0.0201])\n",
      "Epoch 2943,Loss 2.928864\n",
      "\tParams: tensor([  5.3469, -17.1871])\n",
      "\tGrad:tensor([-0.0035,  0.0200])\n",
      "Epoch 2944,Loss 2.928860\n",
      "\tParams: tensor([  5.3470, -17.1873])\n",
      "\tGrad:tensor([-0.0035,  0.0200])\n",
      "Epoch 2945,Loss 2.928855\n",
      "\tParams: tensor([  5.3470, -17.1875])\n",
      "\tGrad:tensor([-0.0035,  0.0200])\n",
      "Epoch 2946,Loss 2.928850\n",
      "\tParams: tensor([  5.3470, -17.1877])\n",
      "\tGrad:tensor([-0.0035,  0.0199])\n",
      "Epoch 2947,Loss 2.928845\n",
      "\tParams: tensor([  5.3471, -17.1879])\n",
      "\tGrad:tensor([-0.0035,  0.0199])\n",
      "Epoch 2948,Loss 2.928843\n",
      "\tParams: tensor([  5.3471, -17.1881])\n",
      "\tGrad:tensor([-0.0035,  0.0199])\n",
      "Epoch 2949,Loss 2.928838\n",
      "\tParams: tensor([  5.3471, -17.1883])\n",
      "\tGrad:tensor([-0.0035,  0.0198])\n",
      "Epoch 2950,Loss 2.928833\n",
      "\tParams: tensor([  5.3472, -17.1885])\n",
      "\tGrad:tensor([-0.0035,  0.0198])\n",
      "Epoch 2951,Loss 2.928830\n",
      "\tParams: tensor([  5.3472, -17.1887])\n",
      "\tGrad:tensor([-0.0035,  0.0198])\n",
      "Epoch 2952,Loss 2.928826\n",
      "\tParams: tensor([  5.3472, -17.1889])\n",
      "\tGrad:tensor([-0.0035,  0.0197])\n",
      "Epoch 2953,Loss 2.928823\n",
      "\tParams: tensor([  5.3473, -17.1891])\n",
      "\tGrad:tensor([-0.0035,  0.0197])\n",
      "Epoch 2954,Loss 2.928818\n",
      "\tParams: tensor([  5.3473, -17.1893])\n",
      "\tGrad:tensor([-0.0035,  0.0197])\n",
      "Epoch 2955,Loss 2.928816\n",
      "\tParams: tensor([  5.3474, -17.1895])\n",
      "\tGrad:tensor([-0.0035,  0.0196])\n",
      "Epoch 2956,Loss 2.928811\n",
      "\tParams: tensor([  5.3474, -17.1897])\n",
      "\tGrad:tensor([-0.0035,  0.0196])\n",
      "Epoch 2957,Loss 2.928805\n",
      "\tParams: tensor([  5.3474, -17.1899])\n",
      "\tGrad:tensor([-0.0034,  0.0196])\n",
      "Epoch 2958,Loss 2.928802\n",
      "\tParams: tensor([  5.3475, -17.1901])\n",
      "\tGrad:tensor([-0.0035,  0.0195])\n",
      "Epoch 2959,Loss 2.928799\n",
      "\tParams: tensor([  5.3475, -17.1903])\n",
      "\tGrad:tensor([-0.0034,  0.0195])\n",
      "Epoch 2960,Loss 2.928795\n",
      "\tParams: tensor([  5.3475, -17.1905])\n",
      "\tGrad:tensor([-0.0034,  0.0195])\n",
      "Epoch 2961,Loss 2.928789\n",
      "\tParams: tensor([  5.3476, -17.1907])\n",
      "\tGrad:tensor([-0.0034,  0.0194])\n",
      "Epoch 2962,Loss 2.928789\n",
      "\tParams: tensor([  5.3476, -17.1908])\n",
      "\tGrad:tensor([-0.0034,  0.0194])\n",
      "Epoch 2963,Loss 2.928783\n",
      "\tParams: tensor([  5.3476, -17.1910])\n",
      "\tGrad:tensor([-0.0034,  0.0194])\n",
      "Epoch 2964,Loss 2.928779\n",
      "\tParams: tensor([  5.3477, -17.1912])\n",
      "\tGrad:tensor([-0.0034,  0.0193])\n",
      "Epoch 2965,Loss 2.928775\n",
      "\tParams: tensor([  5.3477, -17.1914])\n",
      "\tGrad:tensor([-0.0034,  0.0193])\n",
      "Epoch 2966,Loss 2.928771\n",
      "\tParams: tensor([  5.3477, -17.1916])\n",
      "\tGrad:tensor([-0.0034,  0.0193])\n",
      "Epoch 2967,Loss 2.928767\n",
      "\tParams: tensor([  5.3478, -17.1918])\n",
      "\tGrad:tensor([-0.0034,  0.0192])\n",
      "Epoch 2968,Loss 2.928765\n",
      "\tParams: tensor([  5.3478, -17.1920])\n",
      "\tGrad:tensor([-0.0034,  0.0192])\n",
      "Epoch 2969,Loss 2.928761\n",
      "\tParams: tensor([  5.3478, -17.1922])\n",
      "\tGrad:tensor([-0.0034,  0.0192])\n",
      "Epoch 2970,Loss 2.928758\n",
      "\tParams: tensor([  5.3479, -17.1924])\n",
      "\tGrad:tensor([-0.0034,  0.0191])\n",
      "Epoch 2971,Loss 2.928752\n",
      "\tParams: tensor([  5.3479, -17.1926])\n",
      "\tGrad:tensor([-0.0034,  0.0191])\n",
      "Epoch 2972,Loss 2.928750\n",
      "\tParams: tensor([  5.3479, -17.1928])\n",
      "\tGrad:tensor([-0.0034,  0.0191])\n",
      "Epoch 2973,Loss 2.928745\n",
      "\tParams: tensor([  5.3480, -17.1930])\n",
      "\tGrad:tensor([-0.0034,  0.0190])\n",
      "Epoch 2974,Loss 2.928741\n",
      "\tParams: tensor([  5.3480, -17.1931])\n",
      "\tGrad:tensor([-0.0034,  0.0190])\n",
      "Epoch 2975,Loss 2.928737\n",
      "\tParams: tensor([  5.3480, -17.1933])\n",
      "\tGrad:tensor([-0.0034,  0.0190])\n",
      "Epoch 2976,Loss 2.928735\n",
      "\tParams: tensor([  5.3481, -17.1935])\n",
      "\tGrad:tensor([-0.0033,  0.0189])\n",
      "Epoch 2977,Loss 2.928730\n",
      "\tParams: tensor([  5.3481, -17.1937])\n",
      "\tGrad:tensor([-0.0033,  0.0189])\n",
      "Epoch 2978,Loss 2.928727\n",
      "\tParams: tensor([  5.3481, -17.1939])\n",
      "\tGrad:tensor([-0.0033,  0.0189])\n",
      "Epoch 2979,Loss 2.928723\n",
      "\tParams: tensor([  5.3482, -17.1941])\n",
      "\tGrad:tensor([-0.0033,  0.0188])\n",
      "Epoch 2980,Loss 2.928719\n",
      "\tParams: tensor([  5.3482, -17.1943])\n",
      "\tGrad:tensor([-0.0033,  0.0188])\n",
      "Epoch 2981,Loss 2.928716\n",
      "\tParams: tensor([  5.3482, -17.1945])\n",
      "\tGrad:tensor([-0.0033,  0.0188])\n",
      "Epoch 2982,Loss 2.928712\n",
      "\tParams: tensor([  5.3483, -17.1947])\n",
      "\tGrad:tensor([-0.0033,  0.0187])\n",
      "Epoch 2983,Loss 2.928708\n",
      "\tParams: tensor([  5.3483, -17.1948])\n",
      "\tGrad:tensor([-0.0033,  0.0187])\n",
      "Epoch 2984,Loss 2.928705\n",
      "\tParams: tensor([  5.3483, -17.1950])\n",
      "\tGrad:tensor([-0.0033,  0.0187])\n",
      "Epoch 2985,Loss 2.928700\n",
      "\tParams: tensor([  5.3484, -17.1952])\n",
      "\tGrad:tensor([-0.0033,  0.0186])\n",
      "Epoch 2986,Loss 2.928698\n",
      "\tParams: tensor([  5.3484, -17.1954])\n",
      "\tGrad:tensor([-0.0033,  0.0186])\n",
      "Epoch 2987,Loss 2.928695\n",
      "\tParams: tensor([  5.3484, -17.1956])\n",
      "\tGrad:tensor([-0.0033,  0.0186])\n",
      "Epoch 2988,Loss 2.928690\n",
      "\tParams: tensor([  5.3485, -17.1958])\n",
      "\tGrad:tensor([-0.0033,  0.0186])\n",
      "Epoch 2989,Loss 2.928687\n",
      "\tParams: tensor([  5.3485, -17.1960])\n",
      "\tGrad:tensor([-0.0033,  0.0185])\n",
      "Epoch 2990,Loss 2.928684\n",
      "\tParams: tensor([  5.3485, -17.1961])\n",
      "\tGrad:tensor([-0.0033,  0.0185])\n",
      "Epoch 2991,Loss 2.928679\n",
      "\tParams: tensor([  5.3486, -17.1963])\n",
      "\tGrad:tensor([-0.0032,  0.0185])\n",
      "Epoch 2992,Loss 2.928677\n",
      "\tParams: tensor([  5.3486, -17.1965])\n",
      "\tGrad:tensor([-0.0033,  0.0184])\n",
      "Epoch 2993,Loss 2.928673\n",
      "\tParams: tensor([  5.3486, -17.1967])\n",
      "\tGrad:tensor([-0.0033,  0.0184])\n",
      "Epoch 2994,Loss 2.928669\n",
      "\tParams: tensor([  5.3487, -17.1969])\n",
      "\tGrad:tensor([-0.0033,  0.0184])\n",
      "Epoch 2995,Loss 2.928666\n",
      "\tParams: tensor([  5.3487, -17.1971])\n",
      "\tGrad:tensor([-0.0032,  0.0183])\n",
      "Epoch 2996,Loss 2.928662\n",
      "\tParams: tensor([  5.3487, -17.1972])\n",
      "\tGrad:tensor([-0.0032,  0.0183])\n",
      "Epoch 2997,Loss 2.928660\n",
      "\tParams: tensor([  5.3488, -17.1974])\n",
      "\tGrad:tensor([-0.0032,  0.0183])\n",
      "Epoch 2998,Loss 2.928656\n",
      "\tParams: tensor([  5.3488, -17.1976])\n",
      "\tGrad:tensor([-0.0032,  0.0182])\n",
      "Epoch 2999,Loss 2.928651\n",
      "\tParams: tensor([  5.3488, -17.1978])\n",
      "\tGrad:tensor([-0.0032,  0.0182])\n",
      "Epoch 3000,Loss 2.928648\n",
      "\tParams: tensor([  5.3489, -17.1980])\n",
      "\tGrad:tensor([-0.0032,  0.0182])\n",
      "Epoch 3001,Loss 2.928646\n",
      "\tParams: tensor([  5.3489, -17.1982])\n",
      "\tGrad:tensor([-0.0032,  0.0181])\n",
      "Epoch 3002,Loss 2.928643\n",
      "\tParams: tensor([  5.3489, -17.1983])\n",
      "\tGrad:tensor([-0.0032,  0.0181])\n",
      "Epoch 3003,Loss 2.928638\n",
      "\tParams: tensor([  5.3489, -17.1985])\n",
      "\tGrad:tensor([-0.0032,  0.0181])\n",
      "Epoch 3004,Loss 2.928635\n",
      "\tParams: tensor([  5.3490, -17.1987])\n",
      "\tGrad:tensor([-0.0032,  0.0181])\n",
      "Epoch 3005,Loss 2.928632\n",
      "\tParams: tensor([  5.3490, -17.1989])\n",
      "\tGrad:tensor([-0.0032,  0.0180])\n",
      "Epoch 3006,Loss 2.928629\n",
      "\tParams: tensor([  5.3490, -17.1991])\n",
      "\tGrad:tensor([-0.0032,  0.0180])\n",
      "Epoch 3007,Loss 2.928625\n",
      "\tParams: tensor([  5.3491, -17.1992])\n",
      "\tGrad:tensor([-0.0032,  0.0180])\n",
      "Epoch 3008,Loss 2.928621\n",
      "\tParams: tensor([  5.3491, -17.1994])\n",
      "\tGrad:tensor([-0.0032,  0.0179])\n",
      "Epoch 3009,Loss 2.928617\n",
      "\tParams: tensor([  5.3491, -17.1996])\n",
      "\tGrad:tensor([-0.0032,  0.0179])\n",
      "Epoch 3010,Loss 2.928616\n",
      "\tParams: tensor([  5.3492, -17.1998])\n",
      "\tGrad:tensor([-0.0032,  0.0179])\n",
      "Epoch 3011,Loss 2.928612\n",
      "\tParams: tensor([  5.3492, -17.2000])\n",
      "\tGrad:tensor([-0.0032,  0.0178])\n",
      "Epoch 3012,Loss 2.928608\n",
      "\tParams: tensor([  5.3492, -17.2001])\n",
      "\tGrad:tensor([-0.0032,  0.0178])\n",
      "Epoch 3013,Loss 2.928604\n",
      "\tParams: tensor([  5.3493, -17.2003])\n",
      "\tGrad:tensor([-0.0031,  0.0178])\n",
      "Epoch 3014,Loss 2.928601\n",
      "\tParams: tensor([  5.3493, -17.2005])\n",
      "\tGrad:tensor([-0.0031,  0.0177])\n",
      "Epoch 3015,Loss 2.928599\n",
      "\tParams: tensor([  5.3493, -17.2007])\n",
      "\tGrad:tensor([-0.0031,  0.0177])\n",
      "Epoch 3016,Loss 2.928596\n",
      "\tParams: tensor([  5.3494, -17.2008])\n",
      "\tGrad:tensor([-0.0031,  0.0177])\n",
      "Epoch 3017,Loss 2.928592\n",
      "\tParams: tensor([  5.3494, -17.2010])\n",
      "\tGrad:tensor([-0.0031,  0.0177])\n",
      "Epoch 3018,Loss 2.928588\n",
      "\tParams: tensor([  5.3494, -17.2012])\n",
      "\tGrad:tensor([-0.0031,  0.0176])\n",
      "Epoch 3019,Loss 2.928586\n",
      "\tParams: tensor([  5.3495, -17.2014])\n",
      "\tGrad:tensor([-0.0031,  0.0176])\n",
      "Epoch 3020,Loss 2.928583\n",
      "\tParams: tensor([  5.3495, -17.2015])\n",
      "\tGrad:tensor([-0.0031,  0.0176])\n",
      "Epoch 3021,Loss 2.928580\n",
      "\tParams: tensor([  5.3495, -17.2017])\n",
      "\tGrad:tensor([-0.0031,  0.0175])\n",
      "Epoch 3022,Loss 2.928576\n",
      "\tParams: tensor([  5.3495, -17.2019])\n",
      "\tGrad:tensor([-0.0031,  0.0175])\n",
      "Epoch 3023,Loss 2.928574\n",
      "\tParams: tensor([  5.3496, -17.2021])\n",
      "\tGrad:tensor([-0.0031,  0.0175])\n",
      "Epoch 3024,Loss 2.928570\n",
      "\tParams: tensor([  5.3496, -17.2022])\n",
      "\tGrad:tensor([-0.0031,  0.0175])\n",
      "Epoch 3025,Loss 2.928567\n",
      "\tParams: tensor([  5.3496, -17.2024])\n",
      "\tGrad:tensor([-0.0031,  0.0174])\n",
      "Epoch 3026,Loss 2.928564\n",
      "\tParams: tensor([  5.3497, -17.2026])\n",
      "\tGrad:tensor([-0.0031,  0.0174])\n",
      "Epoch 3027,Loss 2.928561\n",
      "\tParams: tensor([  5.3497, -17.2028])\n",
      "\tGrad:tensor([-0.0030,  0.0174])\n",
      "Epoch 3028,Loss 2.928557\n",
      "\tParams: tensor([  5.3497, -17.2029])\n",
      "\tGrad:tensor([-0.0031,  0.0173])\n",
      "Epoch 3029,Loss 2.928555\n",
      "\tParams: tensor([  5.3498, -17.2031])\n",
      "\tGrad:tensor([-0.0031,  0.0173])\n",
      "Epoch 3030,Loss 2.928551\n",
      "\tParams: tensor([  5.3498, -17.2033])\n",
      "\tGrad:tensor([-0.0031,  0.0173])\n",
      "Epoch 3031,Loss 2.928548\n",
      "\tParams: tensor([  5.3498, -17.2035])\n",
      "\tGrad:tensor([-0.0031,  0.0172])\n",
      "Epoch 3032,Loss 2.928545\n",
      "\tParams: tensor([  5.3498, -17.2036])\n",
      "\tGrad:tensor([-0.0030,  0.0172])\n",
      "Epoch 3033,Loss 2.928543\n",
      "\tParams: tensor([  5.3499, -17.2038])\n",
      "\tGrad:tensor([-0.0030,  0.0172])\n",
      "Epoch 3034,Loss 2.928539\n",
      "\tParams: tensor([  5.3499, -17.2040])\n",
      "\tGrad:tensor([-0.0030,  0.0172])\n",
      "Epoch 3035,Loss 2.928536\n",
      "\tParams: tensor([  5.3499, -17.2041])\n",
      "\tGrad:tensor([-0.0030,  0.0171])\n",
      "Epoch 3036,Loss 2.928532\n",
      "\tParams: tensor([  5.3500, -17.2043])\n",
      "\tGrad:tensor([-0.0030,  0.0171])\n",
      "Epoch 3037,Loss 2.928531\n",
      "\tParams: tensor([  5.3500, -17.2045])\n",
      "\tGrad:tensor([-0.0030,  0.0171])\n",
      "Epoch 3038,Loss 2.928528\n",
      "\tParams: tensor([  5.3500, -17.2047])\n",
      "\tGrad:tensor([-0.0030,  0.0170])\n",
      "Epoch 3039,Loss 2.928524\n",
      "\tParams: tensor([  5.3501, -17.2048])\n",
      "\tGrad:tensor([-0.0030,  0.0170])\n",
      "Epoch 3040,Loss 2.928521\n",
      "\tParams: tensor([  5.3501, -17.2050])\n",
      "\tGrad:tensor([-0.0030,  0.0170])\n",
      "Epoch 3041,Loss 2.928519\n",
      "\tParams: tensor([  5.3501, -17.2052])\n",
      "\tGrad:tensor([-0.0030,  0.0170])\n",
      "Epoch 3042,Loss 2.928514\n",
      "\tParams: tensor([  5.3502, -17.2053])\n",
      "\tGrad:tensor([-0.0030,  0.0169])\n",
      "Epoch 3043,Loss 2.928512\n",
      "\tParams: tensor([  5.3502, -17.2055])\n",
      "\tGrad:tensor([-0.0030,  0.0169])\n",
      "Epoch 3044,Loss 2.928509\n",
      "\tParams: tensor([  5.3502, -17.2057])\n",
      "\tGrad:tensor([-0.0030,  0.0169])\n",
      "Epoch 3045,Loss 2.928505\n",
      "\tParams: tensor([  5.3502, -17.2058])\n",
      "\tGrad:tensor([-0.0030,  0.0168])\n",
      "Epoch 3046,Loss 2.928503\n",
      "\tParams: tensor([  5.3503, -17.2060])\n",
      "\tGrad:tensor([-0.0030,  0.0168])\n",
      "Epoch 3047,Loss 2.928500\n",
      "\tParams: tensor([  5.3503, -17.2062])\n",
      "\tGrad:tensor([-0.0030,  0.0168])\n",
      "Epoch 3048,Loss 2.928498\n",
      "\tParams: tensor([  5.3503, -17.2063])\n",
      "\tGrad:tensor([-0.0030,  0.0168])\n",
      "Epoch 3049,Loss 2.928495\n",
      "\tParams: tensor([  5.3504, -17.2065])\n",
      "\tGrad:tensor([-0.0030,  0.0167])\n",
      "Epoch 3050,Loss 2.928491\n",
      "\tParams: tensor([  5.3504, -17.2067])\n",
      "\tGrad:tensor([-0.0030,  0.0167])\n",
      "Epoch 3051,Loss 2.928489\n",
      "\tParams: tensor([  5.3504, -17.2068])\n",
      "\tGrad:tensor([-0.0030,  0.0167])\n",
      "Epoch 3052,Loss 2.928486\n",
      "\tParams: tensor([  5.3504, -17.2070])\n",
      "\tGrad:tensor([-0.0029,  0.0166])\n",
      "Epoch 3053,Loss 2.928484\n",
      "\tParams: tensor([  5.3505, -17.2072])\n",
      "\tGrad:tensor([-0.0029,  0.0166])\n",
      "Epoch 3054,Loss 2.928481\n",
      "\tParams: tensor([  5.3505, -17.2073])\n",
      "\tGrad:tensor([-0.0029,  0.0166])\n",
      "Epoch 3055,Loss 2.928477\n",
      "\tParams: tensor([  5.3505, -17.2075])\n",
      "\tGrad:tensor([-0.0029,  0.0165])\n",
      "Epoch 3056,Loss 2.928474\n",
      "\tParams: tensor([  5.3506, -17.2077])\n",
      "\tGrad:tensor([-0.0029,  0.0165])\n",
      "Epoch 3057,Loss 2.928472\n",
      "\tParams: tensor([  5.3506, -17.2078])\n",
      "\tGrad:tensor([-0.0029,  0.0165])\n",
      "Epoch 3058,Loss 2.928469\n",
      "\tParams: tensor([  5.3506, -17.2080])\n",
      "\tGrad:tensor([-0.0029,  0.0165])\n",
      "Epoch 3059,Loss 2.928468\n",
      "\tParams: tensor([  5.3507, -17.2082])\n",
      "\tGrad:tensor([-0.0029,  0.0164])\n",
      "Epoch 3060,Loss 2.928463\n",
      "\tParams: tensor([  5.3507, -17.2083])\n",
      "\tGrad:tensor([-0.0029,  0.0164])\n",
      "Epoch 3061,Loss 2.928460\n",
      "\tParams: tensor([  5.3507, -17.2085])\n",
      "\tGrad:tensor([-0.0029,  0.0164])\n",
      "Epoch 3062,Loss 2.928458\n",
      "\tParams: tensor([  5.3507, -17.2087])\n",
      "\tGrad:tensor([-0.0029,  0.0164])\n",
      "Epoch 3063,Loss 2.928456\n",
      "\tParams: tensor([  5.3508, -17.2088])\n",
      "\tGrad:tensor([-0.0029,  0.0163])\n",
      "Epoch 3064,Loss 2.928452\n",
      "\tParams: tensor([  5.3508, -17.2090])\n",
      "\tGrad:tensor([-0.0029,  0.0163])\n",
      "Epoch 3065,Loss 2.928449\n",
      "\tParams: tensor([  5.3508, -17.2091])\n",
      "\tGrad:tensor([-0.0029,  0.0163])\n",
      "Epoch 3066,Loss 2.928447\n",
      "\tParams: tensor([  5.3509, -17.2093])\n",
      "\tGrad:tensor([-0.0029,  0.0162])\n",
      "Epoch 3067,Loss 2.928443\n",
      "\tParams: tensor([  5.3509, -17.2095])\n",
      "\tGrad:tensor([-0.0029,  0.0162])\n",
      "Epoch 3068,Loss 2.928444\n",
      "\tParams: tensor([  5.3509, -17.2096])\n",
      "\tGrad:tensor([-0.0029,  0.0162])\n",
      "Epoch 3069,Loss 2.928440\n",
      "\tParams: tensor([  5.3509, -17.2098])\n",
      "\tGrad:tensor([-0.0029,  0.0162])\n",
      "Epoch 3070,Loss 2.928435\n",
      "\tParams: tensor([  5.3510, -17.2100])\n",
      "\tGrad:tensor([-0.0029,  0.0161])\n",
      "Epoch 3071,Loss 2.928435\n",
      "\tParams: tensor([  5.3510, -17.2101])\n",
      "\tGrad:tensor([-0.0029,  0.0161])\n",
      "Epoch 3072,Loss 2.928430\n",
      "\tParams: tensor([  5.3510, -17.2103])\n",
      "\tGrad:tensor([-0.0028,  0.0161])\n",
      "Epoch 3073,Loss 2.928428\n",
      "\tParams: tensor([  5.3511, -17.2104])\n",
      "\tGrad:tensor([-0.0028,  0.0161])\n",
      "Epoch 3074,Loss 2.928426\n",
      "\tParams: tensor([  5.3511, -17.2106])\n",
      "\tGrad:tensor([-0.0028,  0.0160])\n",
      "Epoch 3075,Loss 2.928423\n",
      "\tParams: tensor([  5.3511, -17.2108])\n",
      "\tGrad:tensor([-0.0028,  0.0160])\n",
      "Epoch 3076,Loss 2.928421\n",
      "\tParams: tensor([  5.3511, -17.2109])\n",
      "\tGrad:tensor([-0.0028,  0.0160])\n",
      "Epoch 3077,Loss 2.928417\n",
      "\tParams: tensor([  5.3512, -17.2111])\n",
      "\tGrad:tensor([-0.0028,  0.0159])\n",
      "Epoch 3078,Loss 2.928416\n",
      "\tParams: tensor([  5.3512, -17.2112])\n",
      "\tGrad:tensor([-0.0028,  0.0159])\n",
      "Epoch 3079,Loss 2.928411\n",
      "\tParams: tensor([  5.3512, -17.2114])\n",
      "\tGrad:tensor([-0.0028,  0.0159])\n",
      "Epoch 3080,Loss 2.928410\n",
      "\tParams: tensor([  5.3512, -17.2116])\n",
      "\tGrad:tensor([-0.0028,  0.0159])\n",
      "Epoch 3081,Loss 2.928407\n",
      "\tParams: tensor([  5.3513, -17.2117])\n",
      "\tGrad:tensor([-0.0028,  0.0158])\n",
      "Epoch 3082,Loss 2.928404\n",
      "\tParams: tensor([  5.3513, -17.2119])\n",
      "\tGrad:tensor([-0.0028,  0.0158])\n",
      "Epoch 3083,Loss 2.928402\n",
      "\tParams: tensor([  5.3513, -17.2120])\n",
      "\tGrad:tensor([-0.0028,  0.0158])\n",
      "Epoch 3084,Loss 2.928399\n",
      "\tParams: tensor([  5.3514, -17.2122])\n",
      "\tGrad:tensor([-0.0028,  0.0158])\n",
      "Epoch 3085,Loss 2.928396\n",
      "\tParams: tensor([  5.3514, -17.2123])\n",
      "\tGrad:tensor([-0.0028,  0.0157])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3086,Loss 2.928395\n",
      "\tParams: tensor([  5.3514, -17.2125])\n",
      "\tGrad:tensor([-0.0028,  0.0157])\n",
      "Epoch 3087,Loss 2.928392\n",
      "\tParams: tensor([  5.3514, -17.2127])\n",
      "\tGrad:tensor([-0.0027,  0.0157])\n",
      "Epoch 3088,Loss 2.928389\n",
      "\tParams: tensor([  5.3515, -17.2128])\n",
      "\tGrad:tensor([-0.0027,  0.0157])\n",
      "Epoch 3089,Loss 2.928386\n",
      "\tParams: tensor([  5.3515, -17.2130])\n",
      "\tGrad:tensor([-0.0027,  0.0156])\n",
      "Epoch 3090,Loss 2.928383\n",
      "\tParams: tensor([  5.3515, -17.2131])\n",
      "\tGrad:tensor([-0.0028,  0.0156])\n",
      "Epoch 3091,Loss 2.928382\n",
      "\tParams: tensor([  5.3516, -17.2133])\n",
      "\tGrad:tensor([-0.0028,  0.0156])\n",
      "Epoch 3092,Loss 2.928379\n",
      "\tParams: tensor([  5.3516, -17.2134])\n",
      "\tGrad:tensor([-0.0027,  0.0155])\n",
      "Epoch 3093,Loss 2.928378\n",
      "\tParams: tensor([  5.3516, -17.2136])\n",
      "\tGrad:tensor([-0.0027,  0.0155])\n",
      "Epoch 3094,Loss 2.928375\n",
      "\tParams: tensor([  5.3516, -17.2137])\n",
      "\tGrad:tensor([-0.0027,  0.0155])\n",
      "Epoch 3095,Loss 2.928372\n",
      "\tParams: tensor([  5.3517, -17.2139])\n",
      "\tGrad:tensor([-0.0027,  0.0155])\n",
      "Epoch 3096,Loss 2.928370\n",
      "\tParams: tensor([  5.3517, -17.2141])\n",
      "\tGrad:tensor([-0.0027,  0.0154])\n",
      "Epoch 3097,Loss 2.928368\n",
      "\tParams: tensor([  5.3517, -17.2142])\n",
      "\tGrad:tensor([-0.0027,  0.0154])\n",
      "Epoch 3098,Loss 2.928364\n",
      "\tParams: tensor([  5.3517, -17.2144])\n",
      "\tGrad:tensor([-0.0027,  0.0154])\n",
      "Epoch 3099,Loss 2.928362\n",
      "\tParams: tensor([  5.3518, -17.2145])\n",
      "\tGrad:tensor([-0.0027,  0.0154])\n",
      "Epoch 3100,Loss 2.928361\n",
      "\tParams: tensor([  5.3518, -17.2147])\n",
      "\tGrad:tensor([-0.0027,  0.0153])\n",
      "Epoch 3101,Loss 2.928356\n",
      "\tParams: tensor([  5.3518, -17.2148])\n",
      "\tGrad:tensor([-0.0027,  0.0153])\n",
      "Epoch 3102,Loss 2.928355\n",
      "\tParams: tensor([  5.3519, -17.2150])\n",
      "\tGrad:tensor([-0.0027,  0.0153])\n",
      "Epoch 3103,Loss 2.928353\n",
      "\tParams: tensor([  5.3519, -17.2151])\n",
      "\tGrad:tensor([-0.0027,  0.0153])\n",
      "Epoch 3104,Loss 2.928349\n",
      "\tParams: tensor([  5.3519, -17.2153])\n",
      "\tGrad:tensor([-0.0027,  0.0152])\n",
      "Epoch 3105,Loss 2.928348\n",
      "\tParams: tensor([  5.3519, -17.2154])\n",
      "\tGrad:tensor([-0.0027,  0.0152])\n",
      "Epoch 3106,Loss 2.928345\n",
      "\tParams: tensor([  5.3520, -17.2156])\n",
      "\tGrad:tensor([-0.0027,  0.0152])\n",
      "Epoch 3107,Loss 2.928343\n",
      "\tParams: tensor([  5.3520, -17.2157])\n",
      "\tGrad:tensor([-0.0027,  0.0152])\n",
      "Epoch 3108,Loss 2.928340\n",
      "\tParams: tensor([  5.3520, -17.2159])\n",
      "\tGrad:tensor([-0.0027,  0.0151])\n",
      "Epoch 3109,Loss 2.928339\n",
      "\tParams: tensor([  5.3520, -17.2160])\n",
      "\tGrad:tensor([-0.0027,  0.0151])\n",
      "Epoch 3110,Loss 2.928337\n",
      "\tParams: tensor([  5.3521, -17.2162])\n",
      "\tGrad:tensor([-0.0027,  0.0151])\n",
      "Epoch 3111,Loss 2.928333\n",
      "\tParams: tensor([  5.3521, -17.2163])\n",
      "\tGrad:tensor([-0.0027,  0.0151])\n",
      "Epoch 3112,Loss 2.928332\n",
      "\tParams: tensor([  5.3521, -17.2165])\n",
      "\tGrad:tensor([-0.0027,  0.0150])\n",
      "Epoch 3113,Loss 2.928328\n",
      "\tParams: tensor([  5.3521, -17.2166])\n",
      "\tGrad:tensor([-0.0026,  0.0150])\n",
      "Epoch 3114,Loss 2.928329\n",
      "\tParams: tensor([  5.3522, -17.2168])\n",
      "\tGrad:tensor([-0.0027,  0.0150])\n",
      "Epoch 3115,Loss 2.928324\n",
      "\tParams: tensor([  5.3522, -17.2169])\n",
      "\tGrad:tensor([-0.0026,  0.0149])\n",
      "Epoch 3116,Loss 2.928323\n",
      "\tParams: tensor([  5.3522, -17.2171])\n",
      "\tGrad:tensor([-0.0026,  0.0149])\n",
      "Epoch 3117,Loss 2.928320\n",
      "\tParams: tensor([  5.3523, -17.2172])\n",
      "\tGrad:tensor([-0.0026,  0.0149])\n",
      "Epoch 3118,Loss 2.928319\n",
      "\tParams: tensor([  5.3523, -17.2174])\n",
      "\tGrad:tensor([-0.0026,  0.0149])\n",
      "Epoch 3119,Loss 2.928315\n",
      "\tParams: tensor([  5.3523, -17.2175])\n",
      "\tGrad:tensor([-0.0026,  0.0148])\n",
      "Epoch 3120,Loss 2.928313\n",
      "\tParams: tensor([  5.3523, -17.2177])\n",
      "\tGrad:tensor([-0.0026,  0.0148])\n",
      "Epoch 3121,Loss 2.928310\n",
      "\tParams: tensor([  5.3524, -17.2178])\n",
      "\tGrad:tensor([-0.0026,  0.0148])\n",
      "Epoch 3122,Loss 2.928308\n",
      "\tParams: tensor([  5.3524, -17.2180])\n",
      "\tGrad:tensor([-0.0026,  0.0148])\n",
      "Epoch 3123,Loss 2.928306\n",
      "\tParams: tensor([  5.3524, -17.2181])\n",
      "\tGrad:tensor([-0.0026,  0.0147])\n",
      "Epoch 3124,Loss 2.928304\n",
      "\tParams: tensor([  5.3524, -17.2183])\n",
      "\tGrad:tensor([-0.0026,  0.0147])\n",
      "Epoch 3125,Loss 2.928303\n",
      "\tParams: tensor([  5.3525, -17.2184])\n",
      "\tGrad:tensor([-0.0026,  0.0147])\n",
      "Epoch 3126,Loss 2.928299\n",
      "\tParams: tensor([  5.3525, -17.2186])\n",
      "\tGrad:tensor([-0.0026,  0.0147])\n",
      "Epoch 3127,Loss 2.928296\n",
      "\tParams: tensor([  5.3525, -17.2187])\n",
      "\tGrad:tensor([-0.0026,  0.0146])\n",
      "Epoch 3128,Loss 2.928295\n",
      "\tParams: tensor([  5.3525, -17.2189])\n",
      "\tGrad:tensor([-0.0026,  0.0146])\n",
      "Epoch 3129,Loss 2.928293\n",
      "\tParams: tensor([  5.3526, -17.2190])\n",
      "\tGrad:tensor([-0.0026,  0.0146])\n",
      "Epoch 3130,Loss 2.928291\n",
      "\tParams: tensor([  5.3526, -17.2192])\n",
      "\tGrad:tensor([-0.0026,  0.0146])\n",
      "Epoch 3131,Loss 2.928288\n",
      "\tParams: tensor([  5.3526, -17.2193])\n",
      "\tGrad:tensor([-0.0026,  0.0145])\n",
      "Epoch 3132,Loss 2.928287\n",
      "\tParams: tensor([  5.3526, -17.2194])\n",
      "\tGrad:tensor([-0.0026,  0.0145])\n",
      "Epoch 3133,Loss 2.928285\n",
      "\tParams: tensor([  5.3527, -17.2196])\n",
      "\tGrad:tensor([-0.0025,  0.0145])\n",
      "Epoch 3134,Loss 2.928282\n",
      "\tParams: tensor([  5.3527, -17.2197])\n",
      "\tGrad:tensor([-0.0026,  0.0145])\n",
      "Epoch 3135,Loss 2.928280\n",
      "\tParams: tensor([  5.3527, -17.2199])\n",
      "\tGrad:tensor([-0.0026,  0.0144])\n",
      "Epoch 3136,Loss 2.928276\n",
      "\tParams: tensor([  5.3527, -17.2200])\n",
      "\tGrad:tensor([-0.0025,  0.0144])\n",
      "Epoch 3137,Loss 2.928275\n",
      "\tParams: tensor([  5.3528, -17.2202])\n",
      "\tGrad:tensor([-0.0026,  0.0144])\n",
      "Epoch 3138,Loss 2.928273\n",
      "\tParams: tensor([  5.3528, -17.2203])\n",
      "\tGrad:tensor([-0.0025,  0.0144])\n",
      "Epoch 3139,Loss 2.928271\n",
      "\tParams: tensor([  5.3528, -17.2205])\n",
      "\tGrad:tensor([-0.0025,  0.0144])\n",
      "Epoch 3140,Loss 2.928268\n",
      "\tParams: tensor([  5.3528, -17.2206])\n",
      "\tGrad:tensor([-0.0025,  0.0143])\n",
      "Epoch 3141,Loss 2.928267\n",
      "\tParams: tensor([  5.3529, -17.2207])\n",
      "\tGrad:tensor([-0.0025,  0.0143])\n",
      "Epoch 3142,Loss 2.928264\n",
      "\tParams: tensor([  5.3529, -17.2209])\n",
      "\tGrad:tensor([-0.0025,  0.0143])\n",
      "Epoch 3143,Loss 2.928263\n",
      "\tParams: tensor([  5.3529, -17.2210])\n",
      "\tGrad:tensor([-0.0025,  0.0143])\n",
      "Epoch 3144,Loss 2.928260\n",
      "\tParams: tensor([  5.3529, -17.2212])\n",
      "\tGrad:tensor([-0.0025,  0.0142])\n",
      "Epoch 3145,Loss 2.928259\n",
      "\tParams: tensor([  5.3530, -17.2213])\n",
      "\tGrad:tensor([-0.0025,  0.0142])\n",
      "Epoch 3146,Loss 2.928256\n",
      "\tParams: tensor([  5.3530, -17.2214])\n",
      "\tGrad:tensor([-0.0025,  0.0142])\n",
      "Epoch 3147,Loss 2.928255\n",
      "\tParams: tensor([  5.3530, -17.2216])\n",
      "\tGrad:tensor([-0.0025,  0.0142])\n",
      "Epoch 3148,Loss 2.928252\n",
      "\tParams: tensor([  5.3530, -17.2217])\n",
      "\tGrad:tensor([-0.0025,  0.0141])\n",
      "Epoch 3149,Loss 2.928250\n",
      "\tParams: tensor([  5.3531, -17.2219])\n",
      "\tGrad:tensor([-0.0025,  0.0141])\n",
      "Epoch 3150,Loss 2.928249\n",
      "\tParams: tensor([  5.3531, -17.2220])\n",
      "\tGrad:tensor([-0.0025,  0.0141])\n",
      "Epoch 3151,Loss 2.928246\n",
      "\tParams: tensor([  5.3531, -17.2222])\n",
      "\tGrad:tensor([-0.0025,  0.0141])\n",
      "Epoch 3152,Loss 2.928245\n",
      "\tParams: tensor([  5.3531, -17.2223])\n",
      "\tGrad:tensor([-0.0025,  0.0140])\n",
      "Epoch 3153,Loss 2.928242\n",
      "\tParams: tensor([  5.3532, -17.2224])\n",
      "\tGrad:tensor([-0.0025,  0.0140])\n",
      "Epoch 3154,Loss 2.928239\n",
      "\tParams: tensor([  5.3532, -17.2226])\n",
      "\tGrad:tensor([-0.0025,  0.0140])\n",
      "Epoch 3155,Loss 2.928236\n",
      "\tParams: tensor([  5.3532, -17.2227])\n",
      "\tGrad:tensor([-0.0025,  0.0140])\n",
      "Epoch 3156,Loss 2.928236\n",
      "\tParams: tensor([  5.3532, -17.2229])\n",
      "\tGrad:tensor([-0.0024,  0.0139])\n",
      "Epoch 3157,Loss 2.928233\n",
      "\tParams: tensor([  5.3533, -17.2230])\n",
      "\tGrad:tensor([-0.0025,  0.0139])\n",
      "Epoch 3158,Loss 2.928231\n",
      "\tParams: tensor([  5.3533, -17.2231])\n",
      "\tGrad:tensor([-0.0024,  0.0139])\n",
      "Epoch 3159,Loss 2.928230\n",
      "\tParams: tensor([  5.3533, -17.2233])\n",
      "\tGrad:tensor([-0.0025,  0.0139])\n",
      "Epoch 3160,Loss 2.928227\n",
      "\tParams: tensor([  5.3533, -17.2234])\n",
      "\tGrad:tensor([-0.0024,  0.0138])\n",
      "Epoch 3161,Loss 2.928226\n",
      "\tParams: tensor([  5.3534, -17.2235])\n",
      "\tGrad:tensor([-0.0025,  0.0138])\n",
      "Epoch 3162,Loss 2.928225\n",
      "\tParams: tensor([  5.3534, -17.2237])\n",
      "\tGrad:tensor([-0.0024,  0.0138])\n",
      "Epoch 3163,Loss 2.928222\n",
      "\tParams: tensor([  5.3534, -17.2238])\n",
      "\tGrad:tensor([-0.0024,  0.0138])\n",
      "Epoch 3164,Loss 2.928219\n",
      "\tParams: tensor([  5.3534, -17.2240])\n",
      "\tGrad:tensor([-0.0024,  0.0138])\n",
      "Epoch 3165,Loss 2.928218\n",
      "\tParams: tensor([  5.3535, -17.2241])\n",
      "\tGrad:tensor([-0.0024,  0.0137])\n",
      "Epoch 3166,Loss 2.928216\n",
      "\tParams: tensor([  5.3535, -17.2242])\n",
      "\tGrad:tensor([-0.0024,  0.0137])\n",
      "Epoch 3167,Loss 2.928215\n",
      "\tParams: tensor([  5.3535, -17.2244])\n",
      "\tGrad:tensor([-0.0024,  0.0137])\n",
      "Epoch 3168,Loss 2.928212\n",
      "\tParams: tensor([  5.3535, -17.2245])\n",
      "\tGrad:tensor([-0.0024,  0.0137])\n",
      "Epoch 3169,Loss 2.928211\n",
      "\tParams: tensor([  5.3536, -17.2246])\n",
      "\tGrad:tensor([-0.0024,  0.0136])\n",
      "Epoch 3170,Loss 2.928209\n",
      "\tParams: tensor([  5.3536, -17.2248])\n",
      "\tGrad:tensor([-0.0024,  0.0136])\n",
      "Epoch 3171,Loss 2.928206\n",
      "\tParams: tensor([  5.3536, -17.2249])\n",
      "\tGrad:tensor([-0.0024,  0.0136])\n",
      "Epoch 3172,Loss 2.928205\n",
      "\tParams: tensor([  5.3536, -17.2250])\n",
      "\tGrad:tensor([-0.0024,  0.0136])\n",
      "Epoch 3173,Loss 2.928204\n",
      "\tParams: tensor([  5.3537, -17.2252])\n",
      "\tGrad:tensor([-0.0024,  0.0135])\n",
      "Epoch 3174,Loss 2.928202\n",
      "\tParams: tensor([  5.3537, -17.2253])\n",
      "\tGrad:tensor([-0.0024,  0.0135])\n",
      "Epoch 3175,Loss 2.928200\n",
      "\tParams: tensor([  5.3537, -17.2255])\n",
      "\tGrad:tensor([-0.0024,  0.0135])\n",
      "Epoch 3176,Loss 2.928196\n",
      "\tParams: tensor([  5.3537, -17.2256])\n",
      "\tGrad:tensor([-0.0024,  0.0135])\n",
      "Epoch 3177,Loss 2.928195\n",
      "\tParams: tensor([  5.3538, -17.2257])\n",
      "\tGrad:tensor([-0.0024,  0.0134])\n",
      "Epoch 3178,Loss 2.928195\n",
      "\tParams: tensor([  5.3538, -17.2259])\n",
      "\tGrad:tensor([-0.0024,  0.0134])\n",
      "Epoch 3179,Loss 2.928191\n",
      "\tParams: tensor([  5.3538, -17.2260])\n",
      "\tGrad:tensor([-0.0024,  0.0134])\n",
      "Epoch 3180,Loss 2.928190\n",
      "\tParams: tensor([  5.3538, -17.2261])\n",
      "\tGrad:tensor([-0.0024,  0.0134])\n",
      "Epoch 3181,Loss 2.928188\n",
      "\tParams: tensor([  5.3538, -17.2263])\n",
      "\tGrad:tensor([-0.0023,  0.0134])\n",
      "Epoch 3182,Loss 2.928186\n",
      "\tParams: tensor([  5.3539, -17.2264])\n",
      "\tGrad:tensor([-0.0023,  0.0133])\n",
      "Epoch 3183,Loss 2.928185\n",
      "\tParams: tensor([  5.3539, -17.2265])\n",
      "\tGrad:tensor([-0.0024,  0.0133])\n",
      "Epoch 3184,Loss 2.928184\n",
      "\tParams: tensor([  5.3539, -17.2267])\n",
      "\tGrad:tensor([-0.0023,  0.0133])\n",
      "Epoch 3185,Loss 2.928182\n",
      "\tParams: tensor([  5.3539, -17.2268])\n",
      "\tGrad:tensor([-0.0024,  0.0133])\n",
      "Epoch 3186,Loss 2.928180\n",
      "\tParams: tensor([  5.3540, -17.2269])\n",
      "\tGrad:tensor([-0.0024,  0.0132])\n",
      "Epoch 3187,Loss 2.928178\n",
      "\tParams: tensor([  5.3540, -17.2271])\n",
      "\tGrad:tensor([-0.0023,  0.0132])\n",
      "Epoch 3188,Loss 2.928175\n",
      "\tParams: tensor([  5.3540, -17.2272])\n",
      "\tGrad:tensor([-0.0023,  0.0132])\n",
      "Epoch 3189,Loss 2.928172\n",
      "\tParams: tensor([  5.3540, -17.2273])\n",
      "\tGrad:tensor([-0.0023,  0.0132])\n",
      "Epoch 3190,Loss 2.928171\n",
      "\tParams: tensor([  5.3541, -17.2275])\n",
      "\tGrad:tensor([-0.0023,  0.0132])\n",
      "Epoch 3191,Loss 2.928170\n",
      "\tParams: tensor([  5.3541, -17.2276])\n",
      "\tGrad:tensor([-0.0023,  0.0131])\n",
      "Epoch 3192,Loss 2.928169\n",
      "\tParams: tensor([  5.3541, -17.2277])\n",
      "\tGrad:tensor([-0.0023,  0.0131])\n",
      "Epoch 3193,Loss 2.928167\n",
      "\tParams: tensor([  5.3541, -17.2278])\n",
      "\tGrad:tensor([-0.0023,  0.0131])\n",
      "Epoch 3194,Loss 2.928164\n",
      "\tParams: tensor([  5.3542, -17.2280])\n",
      "\tGrad:tensor([-0.0023,  0.0131])\n",
      "Epoch 3195,Loss 2.928163\n",
      "\tParams: tensor([  5.3542, -17.2281])\n",
      "\tGrad:tensor([-0.0023,  0.0130])\n",
      "Epoch 3196,Loss 2.928162\n",
      "\tParams: tensor([  5.3542, -17.2282])\n",
      "\tGrad:tensor([-0.0023,  0.0130])\n",
      "Epoch 3197,Loss 2.928160\n",
      "\tParams: tensor([  5.3542, -17.2284])\n",
      "\tGrad:tensor([-0.0023,  0.0130])\n",
      "Epoch 3198,Loss 2.928158\n",
      "\tParams: tensor([  5.3542, -17.2285])\n",
      "\tGrad:tensor([-0.0023,  0.0130])\n",
      "Epoch 3199,Loss 2.928157\n",
      "\tParams: tensor([  5.3543, -17.2286])\n",
      "\tGrad:tensor([-0.0023,  0.0130])\n",
      "Epoch 3200,Loss 2.928154\n",
      "\tParams: tensor([  5.3543, -17.2288])\n",
      "\tGrad:tensor([-0.0023,  0.0129])\n",
      "Epoch 3201,Loss 2.928152\n",
      "\tParams: tensor([  5.3543, -17.2289])\n",
      "\tGrad:tensor([-0.0023,  0.0129])\n",
      "Epoch 3202,Loss 2.928149\n",
      "\tParams: tensor([  5.3543, -17.2290])\n",
      "\tGrad:tensor([-0.0023,  0.0129])\n",
      "Epoch 3203,Loss 2.928150\n",
      "\tParams: tensor([  5.3544, -17.2291])\n",
      "\tGrad:tensor([-0.0023,  0.0129])\n",
      "Epoch 3204,Loss 2.928147\n",
      "\tParams: tensor([  5.3544, -17.2293])\n",
      "\tGrad:tensor([-0.0022,  0.0129])\n",
      "Epoch 3205,Loss 2.928146\n",
      "\tParams: tensor([  5.3544, -17.2294])\n",
      "\tGrad:tensor([-0.0023,  0.0128])\n",
      "Epoch 3206,Loss 2.928144\n",
      "\tParams: tensor([  5.3544, -17.2295])\n",
      "\tGrad:tensor([-0.0023,  0.0128])\n",
      "Epoch 3207,Loss 2.928142\n",
      "\tParams: tensor([  5.3544, -17.2297])\n",
      "\tGrad:tensor([-0.0023,  0.0128])\n",
      "Epoch 3208,Loss 2.928140\n",
      "\tParams: tensor([  5.3545, -17.2298])\n",
      "\tGrad:tensor([-0.0022,  0.0128])\n",
      "Epoch 3209,Loss 2.928138\n",
      "\tParams: tensor([  5.3545, -17.2299])\n",
      "\tGrad:tensor([-0.0022,  0.0127])\n",
      "Epoch 3210,Loss 2.928137\n",
      "\tParams: tensor([  5.3545, -17.2300])\n",
      "\tGrad:tensor([-0.0023,  0.0127])\n",
      "Epoch 3211,Loss 2.928135\n",
      "\tParams: tensor([  5.3545, -17.2302])\n",
      "\tGrad:tensor([-0.0023,  0.0127])\n",
      "Epoch 3212,Loss 2.928135\n",
      "\tParams: tensor([  5.3546, -17.2303])\n",
      "\tGrad:tensor([-0.0023,  0.0127])\n",
      "Epoch 3213,Loss 2.928133\n",
      "\tParams: tensor([  5.3546, -17.2304])\n",
      "\tGrad:tensor([-0.0022,  0.0127])\n",
      "Epoch 3214,Loss 2.928131\n",
      "\tParams: tensor([  5.3546, -17.2305])\n",
      "\tGrad:tensor([-0.0022,  0.0126])\n",
      "Epoch 3215,Loss 2.928130\n",
      "\tParams: tensor([  5.3546, -17.2307])\n",
      "\tGrad:tensor([-0.0022,  0.0126])\n",
      "Epoch 3216,Loss 2.928126\n",
      "\tParams: tensor([  5.3546, -17.2308])\n",
      "\tGrad:tensor([-0.0022,  0.0126])\n",
      "Epoch 3217,Loss 2.928125\n",
      "\tParams: tensor([  5.3547, -17.2309])\n",
      "\tGrad:tensor([-0.0022,  0.0126])\n",
      "Epoch 3218,Loss 2.928124\n",
      "\tParams: tensor([  5.3547, -17.2310])\n",
      "\tGrad:tensor([-0.0022,  0.0125])\n",
      "Epoch 3219,Loss 2.928121\n",
      "\tParams: tensor([  5.3547, -17.2312])\n",
      "\tGrad:tensor([-0.0022,  0.0125])\n",
      "Epoch 3220,Loss 2.928121\n",
      "\tParams: tensor([  5.3547, -17.2313])\n",
      "\tGrad:tensor([-0.0022,  0.0125])\n",
      "Epoch 3221,Loss 2.928120\n",
      "\tParams: tensor([  5.3548, -17.2314])\n",
      "\tGrad:tensor([-0.0022,  0.0125])\n",
      "Epoch 3222,Loss 2.928118\n",
      "\tParams: tensor([  5.3548, -17.2315])\n",
      "\tGrad:tensor([-0.0022,  0.0125])\n",
      "Epoch 3223,Loss 2.928117\n",
      "\tParams: tensor([  5.3548, -17.2317])\n",
      "\tGrad:tensor([-0.0022,  0.0124])\n",
      "Epoch 3224,Loss 2.928115\n",
      "\tParams: tensor([  5.3548, -17.2318])\n",
      "\tGrad:tensor([-0.0022,  0.0124])\n",
      "Epoch 3225,Loss 2.928113\n",
      "\tParams: tensor([  5.3548, -17.2319])\n",
      "\tGrad:tensor([-0.0022,  0.0124])\n",
      "Epoch 3226,Loss 2.928110\n",
      "\tParams: tensor([  5.3549, -17.2320])\n",
      "\tGrad:tensor([-0.0022,  0.0124])\n",
      "Epoch 3227,Loss 2.928109\n",
      "\tParams: tensor([  5.3549, -17.2322])\n",
      "\tGrad:tensor([-0.0022,  0.0124])\n",
      "Epoch 3228,Loss 2.928108\n",
      "\tParams: tensor([  5.3549, -17.2323])\n",
      "\tGrad:tensor([-0.0022,  0.0123])\n",
      "Epoch 3229,Loss 2.928105\n",
      "\tParams: tensor([  5.3549, -17.2324])\n",
      "\tGrad:tensor([-0.0022,  0.0123])\n",
      "Epoch 3230,Loss 2.928105\n",
      "\tParams: tensor([  5.3550, -17.2325])\n",
      "\tGrad:tensor([-0.0022,  0.0123])\n",
      "Epoch 3231,Loss 2.928104\n",
      "\tParams: tensor([  5.3550, -17.2327])\n",
      "\tGrad:tensor([-0.0022,  0.0123])\n",
      "Epoch 3232,Loss 2.928102\n",
      "\tParams: tensor([  5.3550, -17.2328])\n",
      "\tGrad:tensor([-0.0021,  0.0123])\n",
      "Epoch 3233,Loss 2.928101\n",
      "\tParams: tensor([  5.3550, -17.2329])\n",
      "\tGrad:tensor([-0.0022,  0.0122])\n",
      "Epoch 3234,Loss 2.928098\n",
      "\tParams: tensor([  5.3550, -17.2330])\n",
      "\tGrad:tensor([-0.0022,  0.0122])\n",
      "Epoch 3235,Loss 2.928097\n",
      "\tParams: tensor([  5.3551, -17.2331])\n",
      "\tGrad:tensor([-0.0022,  0.0122])\n",
      "Epoch 3236,Loss 2.928095\n",
      "\tParams: tensor([  5.3551, -17.2333])\n",
      "\tGrad:tensor([-0.0022,  0.0122])\n",
      "Epoch 3237,Loss 2.928094\n",
      "\tParams: tensor([  5.3551, -17.2334])\n",
      "\tGrad:tensor([-0.0022,  0.0121])\n",
      "Epoch 3238,Loss 2.928093\n",
      "\tParams: tensor([  5.3551, -17.2335])\n",
      "\tGrad:tensor([-0.0022,  0.0121])\n",
      "Epoch 3239,Loss 2.928091\n",
      "\tParams: tensor([  5.3551, -17.2336])\n",
      "\tGrad:tensor([-0.0022,  0.0121])\n",
      "Epoch 3240,Loss 2.928090\n",
      "\tParams: tensor([  5.3552, -17.2338])\n",
      "\tGrad:tensor([-0.0021,  0.0121])\n",
      "Epoch 3241,Loss 2.928088\n",
      "\tParams: tensor([  5.3552, -17.2339])\n",
      "\tGrad:tensor([-0.0021,  0.0121])\n",
      "Epoch 3242,Loss 2.928086\n",
      "\tParams: tensor([  5.3552, -17.2340])\n",
      "\tGrad:tensor([-0.0021,  0.0120])\n",
      "Epoch 3243,Loss 2.928085\n",
      "\tParams: tensor([  5.3552, -17.2341])\n",
      "\tGrad:tensor([-0.0021,  0.0120])\n",
      "Epoch 3244,Loss 2.928084\n",
      "\tParams: tensor([  5.3553, -17.2342])\n",
      "\tGrad:tensor([-0.0021,  0.0120])\n",
      "Epoch 3245,Loss 2.928082\n",
      "\tParams: tensor([  5.3553, -17.2344])\n",
      "\tGrad:tensor([-0.0021,  0.0120])\n",
      "Epoch 3246,Loss 2.928080\n",
      "\tParams: tensor([  5.3553, -17.2345])\n",
      "\tGrad:tensor([-0.0021,  0.0120])\n",
      "Epoch 3247,Loss 2.928079\n",
      "\tParams: tensor([  5.3553, -17.2346])\n",
      "\tGrad:tensor([-0.0021,  0.0119])\n",
      "Epoch 3248,Loss 2.928076\n",
      "\tParams: tensor([  5.3553, -17.2347])\n",
      "\tGrad:tensor([-0.0021,  0.0119])\n",
      "Epoch 3249,Loss 2.928077\n",
      "\tParams: tensor([  5.3554, -17.2348])\n",
      "\tGrad:tensor([-0.0021,  0.0119])\n",
      "Epoch 3250,Loss 2.928075\n",
      "\tParams: tensor([  5.3554, -17.2350])\n",
      "\tGrad:tensor([-0.0021,  0.0119])\n",
      "Epoch 3251,Loss 2.928072\n",
      "\tParams: tensor([  5.3554, -17.2351])\n",
      "\tGrad:tensor([-0.0021,  0.0119])\n",
      "Epoch 3252,Loss 2.928072\n",
      "\tParams: tensor([  5.3554, -17.2352])\n",
      "\tGrad:tensor([-0.0021,  0.0118])\n",
      "Epoch 3253,Loss 2.928071\n",
      "\tParams: tensor([  5.3554, -17.2353])\n",
      "\tGrad:tensor([-0.0021,  0.0118])\n",
      "Epoch 3254,Loss 2.928068\n",
      "\tParams: tensor([  5.3555, -17.2354])\n",
      "\tGrad:tensor([-0.0021,  0.0118])\n",
      "Epoch 3255,Loss 2.928069\n",
      "\tParams: tensor([  5.3555, -17.2355])\n",
      "\tGrad:tensor([-0.0021,  0.0118])\n",
      "Epoch 3256,Loss 2.928066\n",
      "\tParams: tensor([  5.3555, -17.2357])\n",
      "\tGrad:tensor([-0.0021,  0.0118])\n",
      "Epoch 3257,Loss 2.928065\n",
      "\tParams: tensor([  5.3555, -17.2358])\n",
      "\tGrad:tensor([-0.0021,  0.0117])\n",
      "Epoch 3258,Loss 2.928064\n",
      "\tParams: tensor([  5.3555, -17.2359])\n",
      "\tGrad:tensor([-0.0021,  0.0117])\n",
      "Epoch 3259,Loss 2.928061\n",
      "\tParams: tensor([  5.3556, -17.2360])\n",
      "\tGrad:tensor([-0.0021,  0.0117])\n",
      "Epoch 3260,Loss 2.928060\n",
      "\tParams: tensor([  5.3556, -17.2361])\n",
      "\tGrad:tensor([-0.0021,  0.0117])\n",
      "Epoch 3261,Loss 2.928057\n",
      "\tParams: tensor([  5.3556, -17.2362])\n",
      "\tGrad:tensor([-0.0021,  0.0117])\n",
      "Epoch 3262,Loss 2.928058\n",
      "\tParams: tensor([  5.3556, -17.2364])\n",
      "\tGrad:tensor([-0.0021,  0.0116])\n",
      "Epoch 3263,Loss 2.928056\n",
      "\tParams: tensor([  5.3557, -17.2365])\n",
      "\tGrad:tensor([-0.0021,  0.0116])\n",
      "Epoch 3264,Loss 2.928055\n",
      "\tParams: tensor([  5.3557, -17.2366])\n",
      "\tGrad:tensor([-0.0021,  0.0116])\n",
      "Epoch 3265,Loss 2.928052\n",
      "\tParams: tensor([  5.3557, -17.2367])\n",
      "\tGrad:tensor([-0.0021,  0.0116])\n",
      "Epoch 3266,Loss 2.928053\n",
      "\tParams: tensor([  5.3557, -17.2368])\n",
      "\tGrad:tensor([-0.0021,  0.0116])\n",
      "Epoch 3267,Loss 2.928051\n",
      "\tParams: tensor([  5.3557, -17.2369])\n",
      "\tGrad:tensor([-0.0021,  0.0115])\n",
      "Epoch 3268,Loss 2.928050\n",
      "\tParams: tensor([  5.3558, -17.2371])\n",
      "\tGrad:tensor([-0.0021,  0.0115])\n",
      "Epoch 3269,Loss 2.928047\n",
      "\tParams: tensor([  5.3558, -17.2372])\n",
      "\tGrad:tensor([-0.0020,  0.0115])\n",
      "Epoch 3270,Loss 2.928046\n",
      "\tParams: tensor([  5.3558, -17.2373])\n",
      "\tGrad:tensor([-0.0020,  0.0115])\n",
      "Epoch 3271,Loss 2.928046\n",
      "\tParams: tensor([  5.3558, -17.2374])\n",
      "\tGrad:tensor([-0.0020,  0.0115])\n",
      "Epoch 3272,Loss 2.928044\n",
      "\tParams: tensor([  5.3558, -17.2375])\n",
      "\tGrad:tensor([-0.0020,  0.0115])\n",
      "Epoch 3273,Loss 2.928042\n",
      "\tParams: tensor([  5.3559, -17.2376])\n",
      "\tGrad:tensor([-0.0020,  0.0114])\n",
      "Epoch 3274,Loss 2.928040\n",
      "\tParams: tensor([  5.3559, -17.2377])\n",
      "\tGrad:tensor([-0.0020,  0.0114])\n",
      "Epoch 3275,Loss 2.928040\n",
      "\tParams: tensor([  5.3559, -17.2379])\n",
      "\tGrad:tensor([-0.0020,  0.0114])\n",
      "Epoch 3276,Loss 2.928036\n",
      "\tParams: tensor([  5.3559, -17.2380])\n",
      "\tGrad:tensor([-0.0020,  0.0114])\n",
      "Epoch 3277,Loss 2.928036\n",
      "\tParams: tensor([  5.3559, -17.2381])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3278,Loss 2.928037\n",
      "\tParams: tensor([  5.3560, -17.2382])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3279,Loss 2.928034\n",
      "\tParams: tensor([  5.3560, -17.2383])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3280,Loss 2.928034\n",
      "\tParams: tensor([  5.3560, -17.2384])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3281,Loss 2.928031\n",
      "\tParams: tensor([  5.3560, -17.2385])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3282,Loss 2.928032\n",
      "\tParams: tensor([  5.3560, -17.2386])\n",
      "\tGrad:tensor([-0.0020,  0.0113])\n",
      "Epoch 3283,Loss 2.928028\n",
      "\tParams: tensor([  5.3561, -17.2388])\n",
      "\tGrad:tensor([-0.0020,  0.0112])\n",
      "Epoch 3284,Loss 2.928027\n",
      "\tParams: tensor([  5.3561, -17.2389])\n",
      "\tGrad:tensor([-0.0020,  0.0112])\n",
      "Epoch 3285,Loss 2.928026\n",
      "\tParams: tensor([  5.3561, -17.2390])\n",
      "\tGrad:tensor([-0.0020,  0.0112])\n",
      "Epoch 3286,Loss 2.928025\n",
      "\tParams: tensor([  5.3561, -17.2391])\n",
      "\tGrad:tensor([-0.0020,  0.0112])\n",
      "Epoch 3287,Loss 2.928024\n",
      "\tParams: tensor([  5.3561, -17.2392])\n",
      "\tGrad:tensor([-0.0020,  0.0112])\n",
      "Epoch 3288,Loss 2.928022\n",
      "\tParams: tensor([  5.3562, -17.2393])\n",
      "\tGrad:tensor([-0.0020,  0.0111])\n",
      "Epoch 3289,Loss 2.928023\n",
      "\tParams: tensor([  5.3562, -17.2394])\n",
      "\tGrad:tensor([-0.0020,  0.0111])\n",
      "Epoch 3290,Loss 2.928021\n",
      "\tParams: tensor([  5.3562, -17.2395])\n",
      "\tGrad:tensor([-0.0020,  0.0111])\n",
      "Epoch 3291,Loss 2.928019\n",
      "\tParams: tensor([  5.3562, -17.2397])\n",
      "\tGrad:tensor([-0.0020,  0.0111])\n",
      "Epoch 3292,Loss 2.928018\n",
      "\tParams: tensor([  5.3562, -17.2398])\n",
      "\tGrad:tensor([-0.0020,  0.0111])\n",
      "Epoch 3293,Loss 2.928017\n",
      "\tParams: tensor([  5.3563, -17.2399])\n",
      "\tGrad:tensor([-0.0020,  0.0110])\n",
      "Epoch 3294,Loss 2.928015\n",
      "\tParams: tensor([  5.3563, -17.2400])\n",
      "\tGrad:tensor([-0.0020,  0.0110])\n",
      "Epoch 3295,Loss 2.928013\n",
      "\tParams: tensor([  5.3563, -17.2401])\n",
      "\tGrad:tensor([-0.0020,  0.0110])\n",
      "Epoch 3296,Loss 2.928013\n",
      "\tParams: tensor([  5.3563, -17.2402])\n",
      "\tGrad:tensor([-0.0019,  0.0110])\n",
      "Epoch 3297,Loss 2.928011\n",
      "\tParams: tensor([  5.3563, -17.2403])\n",
      "\tGrad:tensor([-0.0019,  0.0110])\n",
      "Epoch 3298,Loss 2.928009\n",
      "\tParams: tensor([  5.3563, -17.2404])\n",
      "\tGrad:tensor([-0.0019,  0.0110])\n",
      "Epoch 3299,Loss 2.928008\n",
      "\tParams: tensor([  5.3564, -17.2405])\n",
      "\tGrad:tensor([-0.0019,  0.0109])\n",
      "Epoch 3300,Loss 2.928006\n",
      "\tParams: tensor([  5.3564, -17.2406])\n",
      "\tGrad:tensor([-0.0019,  0.0109])\n",
      "Epoch 3301,Loss 2.928007\n",
      "\tParams: tensor([  5.3564, -17.2407])\n",
      "\tGrad:tensor([-0.0019,  0.0109])\n",
      "Epoch 3302,Loss 2.928007\n",
      "\tParams: tensor([  5.3564, -17.2409])\n",
      "\tGrad:tensor([-0.0019,  0.0109])\n",
      "Epoch 3303,Loss 2.928004\n",
      "\tParams: tensor([  5.3564, -17.2410])\n",
      "\tGrad:tensor([-0.0019,  0.0109])\n",
      "Epoch 3304,Loss 2.928002\n",
      "\tParams: tensor([  5.3565, -17.2411])\n",
      "\tGrad:tensor([-0.0019,  0.0108])\n",
      "Epoch 3305,Loss 2.928002\n",
      "\tParams: tensor([  5.3565, -17.2412])\n",
      "\tGrad:tensor([-0.0019,  0.0108])\n",
      "Epoch 3306,Loss 2.928000\n",
      "\tParams: tensor([  5.3565, -17.2413])\n",
      "\tGrad:tensor([-0.0019,  0.0108])\n",
      "Epoch 3307,Loss 2.928000\n",
      "\tParams: tensor([  5.3565, -17.2414])\n",
      "\tGrad:tensor([-0.0019,  0.0108])\n",
      "Epoch 3308,Loss 2.927998\n",
      "\tParams: tensor([  5.3565, -17.2415])\n",
      "\tGrad:tensor([-0.0019,  0.0108])\n",
      "Epoch 3309,Loss 2.927995\n",
      "\tParams: tensor([  5.3566, -17.2416])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3310,Loss 2.927995\n",
      "\tParams: tensor([  5.3566, -17.2417])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3311,Loss 2.927994\n",
      "\tParams: tensor([  5.3566, -17.2418])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3312,Loss 2.927994\n",
      "\tParams: tensor([  5.3566, -17.2419])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3313,Loss 2.927991\n",
      "\tParams: tensor([  5.3566, -17.2420])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3314,Loss 2.927991\n",
      "\tParams: tensor([  5.3567, -17.2421])\n",
      "\tGrad:tensor([-0.0019,  0.0107])\n",
      "Epoch 3315,Loss 2.927990\n",
      "\tParams: tensor([  5.3567, -17.2423])\n",
      "\tGrad:tensor([-0.0019,  0.0106])\n",
      "Epoch 3316,Loss 2.927989\n",
      "\tParams: tensor([  5.3567, -17.2424])\n",
      "\tGrad:tensor([-0.0019,  0.0106])\n",
      "Epoch 3317,Loss 2.927988\n",
      "\tParams: tensor([  5.3567, -17.2425])\n",
      "\tGrad:tensor([-0.0019,  0.0106])\n",
      "Epoch 3318,Loss 2.927986\n",
      "\tParams: tensor([  5.3567, -17.2426])\n",
      "\tGrad:tensor([-0.0019,  0.0106])\n",
      "Epoch 3319,Loss 2.927985\n",
      "\tParams: tensor([  5.3567, -17.2427])\n",
      "\tGrad:tensor([-0.0019,  0.0106])\n",
      "Epoch 3320,Loss 2.927983\n",
      "\tParams: tensor([  5.3568, -17.2428])\n",
      "\tGrad:tensor([-0.0018,  0.0106])\n",
      "Epoch 3321,Loss 2.927983\n",
      "\tParams: tensor([  5.3568, -17.2429])\n",
      "\tGrad:tensor([-0.0018,  0.0105])\n",
      "Epoch 3322,Loss 2.927981\n",
      "\tParams: tensor([  5.3568, -17.2430])\n",
      "\tGrad:tensor([-0.0018,  0.0105])\n",
      "Epoch 3323,Loss 2.927980\n",
      "\tParams: tensor([  5.3568, -17.2431])\n",
      "\tGrad:tensor([-0.0018,  0.0105])\n",
      "Epoch 3324,Loss 2.927979\n",
      "\tParams: tensor([  5.3568, -17.2432])\n",
      "\tGrad:tensor([-0.0018,  0.0105])\n",
      "Epoch 3325,Loss 2.927979\n",
      "\tParams: tensor([  5.3569, -17.2433])\n",
      "\tGrad:tensor([-0.0018,  0.0105])\n",
      "Epoch 3326,Loss 2.927977\n",
      "\tParams: tensor([  5.3569, -17.2434])\n",
      "\tGrad:tensor([-0.0018,  0.0104])\n",
      "Epoch 3327,Loss 2.927975\n",
      "\tParams: tensor([  5.3569, -17.2435])\n",
      "\tGrad:tensor([-0.0019,  0.0104])\n",
      "Epoch 3328,Loss 2.927973\n",
      "\tParams: tensor([  5.3569, -17.2436])\n",
      "\tGrad:tensor([-0.0018,  0.0104])\n",
      "Epoch 3329,Loss 2.927974\n",
      "\tParams: tensor([  5.3569, -17.2437])\n",
      "\tGrad:tensor([-0.0018,  0.0104])\n",
      "Epoch 3330,Loss 2.927974\n",
      "\tParams: tensor([  5.3570, -17.2438])\n",
      "\tGrad:tensor([-0.0018,  0.0104])\n",
      "Epoch 3331,Loss 2.927972\n",
      "\tParams: tensor([  5.3570, -17.2439])\n",
      "\tGrad:tensor([-0.0018,  0.0104])\n",
      "Epoch 3332,Loss 2.927972\n",
      "\tParams: tensor([  5.3570, -17.2440])\n",
      "\tGrad:tensor([-0.0018,  0.0103])\n",
      "Epoch 3333,Loss 2.927969\n",
      "\tParams: tensor([  5.3570, -17.2441])\n",
      "\tGrad:tensor([-0.0018,  0.0103])\n",
      "Epoch 3334,Loss 2.927969\n",
      "\tParams: tensor([  5.3570, -17.2442])\n",
      "\tGrad:tensor([-0.0018,  0.0103])\n",
      "Epoch 3335,Loss 2.927967\n",
      "\tParams: tensor([  5.3570, -17.2443])\n",
      "\tGrad:tensor([-0.0018,  0.0103])\n",
      "Epoch 3336,Loss 2.927967\n",
      "\tParams: tensor([  5.3571, -17.2444])\n",
      "\tGrad:tensor([-0.0018,  0.0103])\n",
      "Epoch 3337,Loss 2.927963\n",
      "\tParams: tensor([  5.3571, -17.2446])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3338,Loss 2.927963\n",
      "\tParams: tensor([  5.3571, -17.2447])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3339,Loss 2.927962\n",
      "\tParams: tensor([  5.3571, -17.2448])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3340,Loss 2.927962\n",
      "\tParams: tensor([  5.3571, -17.2449])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3341,Loss 2.927960\n",
      "\tParams: tensor([  5.3572, -17.2450])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3342,Loss 2.927960\n",
      "\tParams: tensor([  5.3572, -17.2451])\n",
      "\tGrad:tensor([-0.0018,  0.0102])\n",
      "Epoch 3343,Loss 2.927959\n",
      "\tParams: tensor([  5.3572, -17.2452])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3344,Loss 2.927958\n",
      "\tParams: tensor([  5.3572, -17.2453])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3345,Loss 2.927956\n",
      "\tParams: tensor([  5.3572, -17.2454])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3346,Loss 2.927956\n",
      "\tParams: tensor([  5.3572, -17.2455])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3347,Loss 2.927955\n",
      "\tParams: tensor([  5.3573, -17.2456])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3348,Loss 2.927953\n",
      "\tParams: tensor([  5.3573, -17.2457])\n",
      "\tGrad:tensor([-0.0018,  0.0101])\n",
      "Epoch 3349,Loss 2.927953\n",
      "\tParams: tensor([  5.3573, -17.2458])\n",
      "\tGrad:tensor([-0.0018,  0.0100])\n",
      "Epoch 3350,Loss 2.927951\n",
      "\tParams: tensor([  5.3573, -17.2459])\n",
      "\tGrad:tensor([-0.0018,  0.0100])\n",
      "Epoch 3351,Loss 2.927950\n",
      "\tParams: tensor([  5.3573, -17.2460])\n",
      "\tGrad:tensor([-0.0018,  0.0100])\n",
      "Epoch 3352,Loss 2.927948\n",
      "\tParams: tensor([  5.3573, -17.2461])\n",
      "\tGrad:tensor([-0.0018,  0.0100])\n",
      "Epoch 3353,Loss 2.927947\n",
      "\tParams: tensor([  5.3574, -17.2462])\n",
      "\tGrad:tensor([-0.0017,  0.0100])\n",
      "Epoch 3354,Loss 2.927948\n",
      "\tParams: tensor([  5.3574, -17.2463])\n",
      "\tGrad:tensor([-0.0018,  0.0100])\n",
      "Epoch 3355,Loss 2.927945\n",
      "\tParams: tensor([  5.3574, -17.2464])\n",
      "\tGrad:tensor([-0.0017,  0.0099])\n",
      "Epoch 3356,Loss 2.927944\n",
      "\tParams: tensor([  5.3574, -17.2465])\n",
      "\tGrad:tensor([-0.0017,  0.0099])\n",
      "Epoch 3357,Loss 2.927943\n",
      "\tParams: tensor([  5.3574, -17.2466])\n",
      "\tGrad:tensor([-0.0018,  0.0099])\n",
      "Epoch 3358,Loss 2.927944\n",
      "\tParams: tensor([  5.3575, -17.2467])\n",
      "\tGrad:tensor([-0.0017,  0.0099])\n",
      "Epoch 3359,Loss 2.927942\n",
      "\tParams: tensor([  5.3575, -17.2468])\n",
      "\tGrad:tensor([-0.0017,  0.0099])\n",
      "Epoch 3360,Loss 2.927941\n",
      "\tParams: tensor([  5.3575, -17.2469])\n",
      "\tGrad:tensor([-0.0018,  0.0099])\n",
      "Epoch 3361,Loss 2.927940\n",
      "\tParams: tensor([  5.3575, -17.2470])\n",
      "\tGrad:tensor([-0.0017,  0.0098])\n",
      "Epoch 3362,Loss 2.927938\n",
      "\tParams: tensor([  5.3575, -17.2471])\n",
      "\tGrad:tensor([-0.0017,  0.0098])\n",
      "Epoch 3363,Loss 2.927938\n",
      "\tParams: tensor([  5.3575, -17.2472])\n",
      "\tGrad:tensor([-0.0018,  0.0098])\n",
      "Epoch 3364,Loss 2.927936\n",
      "\tParams: tensor([  5.3576, -17.2473])\n",
      "\tGrad:tensor([-0.0017,  0.0098])\n",
      "Epoch 3365,Loss 2.927936\n",
      "\tParams: tensor([  5.3576, -17.2474])\n",
      "\tGrad:tensor([-0.0017,  0.0098])\n",
      "Epoch 3366,Loss 2.927937\n",
      "\tParams: tensor([  5.3576, -17.2474])\n",
      "\tGrad:tensor([-0.0017,  0.0098])\n",
      "Epoch 3367,Loss 2.927934\n",
      "\tParams: tensor([  5.3576, -17.2475])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3368,Loss 2.927933\n",
      "\tParams: tensor([  5.3576, -17.2476])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3369,Loss 2.927932\n",
      "\tParams: tensor([  5.3576, -17.2477])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3370,Loss 2.927930\n",
      "\tParams: tensor([  5.3577, -17.2478])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3371,Loss 2.927928\n",
      "\tParams: tensor([  5.3577, -17.2479])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3372,Loss 2.927931\n",
      "\tParams: tensor([  5.3577, -17.2480])\n",
      "\tGrad:tensor([-0.0017,  0.0097])\n",
      "Epoch 3373,Loss 2.927929\n",
      "\tParams: tensor([  5.3577, -17.2481])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3374,Loss 2.927927\n",
      "\tParams: tensor([  5.3577, -17.2482])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3375,Loss 2.927926\n",
      "\tParams: tensor([  5.3577, -17.2483])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3376,Loss 2.927925\n",
      "\tParams: tensor([  5.3578, -17.2484])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3377,Loss 2.927924\n",
      "\tParams: tensor([  5.3578, -17.2485])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3378,Loss 2.927923\n",
      "\tParams: tensor([  5.3578, -17.2486])\n",
      "\tGrad:tensor([-0.0017,  0.0096])\n",
      "Epoch 3379,Loss 2.927924\n",
      "\tParams: tensor([  5.3578, -17.2487])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3380,Loss 2.927922\n",
      "\tParams: tensor([  5.3578, -17.2488])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3381,Loss 2.927922\n",
      "\tParams: tensor([  5.3578, -17.2489])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3382,Loss 2.927920\n",
      "\tParams: tensor([  5.3579, -17.2490])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3383,Loss 2.927918\n",
      "\tParams: tensor([  5.3579, -17.2491])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3384,Loss 2.927917\n",
      "\tParams: tensor([  5.3579, -17.2492])\n",
      "\tGrad:tensor([-0.0017,  0.0095])\n",
      "Epoch 3385,Loss 2.927917\n",
      "\tParams: tensor([  5.3579, -17.2493])\n",
      "\tGrad:tensor([-0.0017,  0.0094])\n",
      "Epoch 3386,Loss 2.927915\n",
      "\tParams: tensor([  5.3579, -17.2494])\n",
      "\tGrad:tensor([-0.0017,  0.0094])\n",
      "Epoch 3387,Loss 2.927915\n",
      "\tParams: tensor([  5.3579, -17.2495])\n",
      "\tGrad:tensor([-0.0017,  0.0094])\n",
      "Epoch 3388,Loss 2.927914\n",
      "\tParams: tensor([  5.3580, -17.2496])\n",
      "\tGrad:tensor([-0.0016,  0.0094])\n",
      "Epoch 3389,Loss 2.927913\n",
      "\tParams: tensor([  5.3580, -17.2496])\n",
      "\tGrad:tensor([-0.0017,  0.0094])\n",
      "Epoch 3390,Loss 2.927911\n",
      "\tParams: tensor([  5.3580, -17.2497])\n",
      "\tGrad:tensor([-0.0016,  0.0094])\n",
      "Epoch 3391,Loss 2.927913\n",
      "\tParams: tensor([  5.3580, -17.2498])\n",
      "\tGrad:tensor([-0.0017,  0.0093])\n",
      "Epoch 3392,Loss 2.927911\n",
      "\tParams: tensor([  5.3580, -17.2499])\n",
      "\tGrad:tensor([-0.0016,  0.0093])\n",
      "Epoch 3393,Loss 2.927910\n",
      "\tParams: tensor([  5.3580, -17.2500])\n",
      "\tGrad:tensor([-0.0016,  0.0093])\n",
      "Epoch 3394,Loss 2.927909\n",
      "\tParams: tensor([  5.3581, -17.2501])\n",
      "\tGrad:tensor([-0.0016,  0.0093])\n",
      "Epoch 3395,Loss 2.927908\n",
      "\tParams: tensor([  5.3581, -17.2502])\n",
      "\tGrad:tensor([-0.0016,  0.0093])\n",
      "Epoch 3396,Loss 2.927907\n",
      "\tParams: tensor([  5.3581, -17.2503])\n",
      "\tGrad:tensor([-0.0017,  0.0093])\n",
      "Epoch 3397,Loss 2.927906\n",
      "\tParams: tensor([  5.3581, -17.2504])\n",
      "\tGrad:tensor([-0.0016,  0.0093])\n",
      "Epoch 3398,Loss 2.927905\n",
      "\tParams: tensor([  5.3581, -17.2505])\n",
      "\tGrad:tensor([-0.0017,  0.0092])\n",
      "Epoch 3399,Loss 2.927905\n",
      "\tParams: tensor([  5.3581, -17.2506])\n",
      "\tGrad:tensor([-0.0016,  0.0092])\n",
      "Epoch 3400,Loss 2.927904\n",
      "\tParams: tensor([  5.3582, -17.2507])\n",
      "\tGrad:tensor([-0.0016,  0.0092])\n",
      "Epoch 3401,Loss 2.927902\n",
      "\tParams: tensor([  5.3582, -17.2508])\n",
      "\tGrad:tensor([-0.0016,  0.0092])\n",
      "Epoch 3402,Loss 2.927902\n",
      "\tParams: tensor([  5.3582, -17.2509])\n",
      "\tGrad:tensor([-0.0016,  0.0092])\n",
      "Epoch 3403,Loss 2.927902\n",
      "\tParams: tensor([  5.3582, -17.2509])\n",
      "\tGrad:tensor([-0.0016,  0.0092])\n",
      "Epoch 3404,Loss 2.927899\n",
      "\tParams: tensor([  5.3582, -17.2510])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3405,Loss 2.927899\n",
      "\tParams: tensor([  5.3582, -17.2511])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3406,Loss 2.927898\n",
      "\tParams: tensor([  5.3583, -17.2512])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3407,Loss 2.927899\n",
      "\tParams: tensor([  5.3583, -17.2513])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3408,Loss 2.927896\n",
      "\tParams: tensor([  5.3583, -17.2514])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3409,Loss 2.927895\n",
      "\tParams: tensor([  5.3583, -17.2515])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3410,Loss 2.927896\n",
      "\tParams: tensor([  5.3583, -17.2516])\n",
      "\tGrad:tensor([-0.0016,  0.0091])\n",
      "Epoch 3411,Loss 2.927894\n",
      "\tParams: tensor([  5.3583, -17.2517])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3412,Loss 2.927892\n",
      "\tParams: tensor([  5.3584, -17.2518])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3413,Loss 2.927892\n",
      "\tParams: tensor([  5.3584, -17.2519])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3414,Loss 2.927891\n",
      "\tParams: tensor([  5.3584, -17.2519])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3415,Loss 2.927891\n",
      "\tParams: tensor([  5.3584, -17.2520])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3416,Loss 2.927890\n",
      "\tParams: tensor([  5.3584, -17.2521])\n",
      "\tGrad:tensor([-0.0016,  0.0090])\n",
      "Epoch 3417,Loss 2.927891\n",
      "\tParams: tensor([  5.3584, -17.2522])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3418,Loss 2.927888\n",
      "\tParams: tensor([  5.3584, -17.2523])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3419,Loss 2.927888\n",
      "\tParams: tensor([  5.3585, -17.2524])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3420,Loss 2.927886\n",
      "\tParams: tensor([  5.3585, -17.2525])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3421,Loss 2.927887\n",
      "\tParams: tensor([  5.3585, -17.2526])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3422,Loss 2.927886\n",
      "\tParams: tensor([  5.3585, -17.2527])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3423,Loss 2.927884\n",
      "\tParams: tensor([  5.3585, -17.2527])\n",
      "\tGrad:tensor([-0.0016,  0.0089])\n",
      "Epoch 3424,Loss 2.927883\n",
      "\tParams: tensor([  5.3585, -17.2528])\n",
      "\tGrad:tensor([-0.0015,  0.0088])\n",
      "Epoch 3425,Loss 2.927881\n",
      "\tParams: tensor([  5.3586, -17.2529])\n",
      "\tGrad:tensor([-0.0015,  0.0088])\n",
      "Epoch 3426,Loss 2.927881\n",
      "\tParams: tensor([  5.3586, -17.2530])\n",
      "\tGrad:tensor([-0.0016,  0.0088])\n",
      "Epoch 3427,Loss 2.927880\n",
      "\tParams: tensor([  5.3586, -17.2531])\n",
      "\tGrad:tensor([-0.0015,  0.0088])\n",
      "Epoch 3428,Loss 2.927880\n",
      "\tParams: tensor([  5.3586, -17.2532])\n",
      "\tGrad:tensor([-0.0016,  0.0088])\n",
      "Epoch 3429,Loss 2.927879\n",
      "\tParams: tensor([  5.3586, -17.2533])\n",
      "\tGrad:tensor([-0.0015,  0.0088])\n",
      "Epoch 3430,Loss 2.927877\n",
      "\tParams: tensor([  5.3586, -17.2534])\n",
      "\tGrad:tensor([-0.0016,  0.0087])\n",
      "Epoch 3431,Loss 2.927876\n",
      "\tParams: tensor([  5.3586, -17.2534])\n",
      "\tGrad:tensor([-0.0015,  0.0087])\n",
      "Epoch 3432,Loss 2.927876\n",
      "\tParams: tensor([  5.3587, -17.2535])\n",
      "\tGrad:tensor([-0.0015,  0.0087])\n",
      "Epoch 3433,Loss 2.927876\n",
      "\tParams: tensor([  5.3587, -17.2536])\n",
      "\tGrad:tensor([-0.0015,  0.0087])\n",
      "Epoch 3434,Loss 2.927876\n",
      "\tParams: tensor([  5.3587, -17.2537])\n",
      "\tGrad:tensor([-0.0016,  0.0087])\n",
      "Epoch 3435,Loss 2.927876\n",
      "\tParams: tensor([  5.3587, -17.2538])\n",
      "\tGrad:tensor([-0.0016,  0.0087])\n",
      "Epoch 3436,Loss 2.927875\n",
      "\tParams: tensor([  5.3587, -17.2539])\n",
      "\tGrad:tensor([-0.0015,  0.0087])\n",
      "Epoch 3437,Loss 2.927873\n",
      "\tParams: tensor([  5.3587, -17.2540])\n",
      "\tGrad:tensor([-0.0015,  0.0087])\n",
      "Epoch 3438,Loss 2.927872\n",
      "\tParams: tensor([  5.3588, -17.2541])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3439,Loss 2.927871\n",
      "\tParams: tensor([  5.3588, -17.2541])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3440,Loss 2.927870\n",
      "\tParams: tensor([  5.3588, -17.2542])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3441,Loss 2.927871\n",
      "\tParams: tensor([  5.3588, -17.2543])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3442,Loss 2.927869\n",
      "\tParams: tensor([  5.3588, -17.2544])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3443,Loss 2.927869\n",
      "\tParams: tensor([  5.3588, -17.2545])\n",
      "\tGrad:tensor([-0.0015,  0.0086])\n",
      "Epoch 3444,Loss 2.927867\n",
      "\tParams: tensor([  5.3588, -17.2546])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3445,Loss 2.927866\n",
      "\tParams: tensor([  5.3589, -17.2547])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3446,Loss 2.927866\n",
      "\tParams: tensor([  5.3589, -17.2547])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3447,Loss 2.927866\n",
      "\tParams: tensor([  5.3589, -17.2548])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3448,Loss 2.927864\n",
      "\tParams: tensor([  5.3589, -17.2549])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3449,Loss 2.927863\n",
      "\tParams: tensor([  5.3589, -17.2550])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3450,Loss 2.927863\n",
      "\tParams: tensor([  5.3589, -17.2551])\n",
      "\tGrad:tensor([-0.0015,  0.0085])\n",
      "Epoch 3451,Loss 2.927862\n",
      "\tParams: tensor([  5.3590, -17.2552])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3452,Loss 2.927863\n",
      "\tParams: tensor([  5.3590, -17.2552])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3453,Loss 2.927860\n",
      "\tParams: tensor([  5.3590, -17.2553])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3454,Loss 2.927860\n",
      "\tParams: tensor([  5.3590, -17.2554])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3455,Loss 2.927860\n",
      "\tParams: tensor([  5.3590, -17.2555])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3456,Loss 2.927859\n",
      "\tParams: tensor([  5.3590, -17.2556])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3457,Loss 2.927858\n",
      "\tParams: tensor([  5.3590, -17.2557])\n",
      "\tGrad:tensor([-0.0015,  0.0084])\n",
      "Epoch 3458,Loss 2.927858\n",
      "\tParams: tensor([  5.3591, -17.2557])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3459,Loss 2.927856\n",
      "\tParams: tensor([  5.3591, -17.2558])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3460,Loss 2.927857\n",
      "\tParams: tensor([  5.3591, -17.2559])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3461,Loss 2.927854\n",
      "\tParams: tensor([  5.3591, -17.2560])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3462,Loss 2.927855\n",
      "\tParams: tensor([  5.3591, -17.2561])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3463,Loss 2.927854\n",
      "\tParams: tensor([  5.3591, -17.2562])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3464,Loss 2.927854\n",
      "\tParams: tensor([  5.3591, -17.2562])\n",
      "\tGrad:tensor([-0.0015,  0.0083])\n",
      "Epoch 3465,Loss 2.927851\n",
      "\tParams: tensor([  5.3592, -17.2563])\n",
      "\tGrad:tensor([-0.0014,  0.0082])\n",
      "Epoch 3466,Loss 2.927853\n",
      "\tParams: tensor([  5.3592, -17.2564])\n",
      "\tGrad:tensor([-0.0015,  0.0082])\n",
      "Epoch 3467,Loss 2.927852\n",
      "\tParams: tensor([  5.3592, -17.2565])\n",
      "\tGrad:tensor([-0.0014,  0.0082])\n",
      "Epoch 3468,Loss 2.927850\n",
      "\tParams: tensor([  5.3592, -17.2566])\n",
      "\tGrad:tensor([-0.0014,  0.0082])\n",
      "Epoch 3469,Loss 2.927849\n",
      "\tParams: tensor([  5.3592, -17.2567])\n",
      "\tGrad:tensor([-0.0015,  0.0082])\n",
      "Epoch 3470,Loss 2.927849\n",
      "\tParams: tensor([  5.3592, -17.2567])\n",
      "\tGrad:tensor([-0.0015,  0.0082])\n",
      "Epoch 3471,Loss 2.927848\n",
      "\tParams: tensor([  5.3592, -17.2568])\n",
      "\tGrad:tensor([-0.0014,  0.0082])\n",
      "Epoch 3472,Loss 2.927848\n",
      "\tParams: tensor([  5.3593, -17.2569])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3473,Loss 2.927846\n",
      "\tParams: tensor([  5.3593, -17.2570])\n",
      "\tGrad:tensor([-0.0015,  0.0081])\n",
      "Epoch 3474,Loss 2.927846\n",
      "\tParams: tensor([  5.3593, -17.2571])\n",
      "\tGrad:tensor([-0.0015,  0.0081])\n",
      "Epoch 3475,Loss 2.927845\n",
      "\tParams: tensor([  5.3593, -17.2571])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3476,Loss 2.927844\n",
      "\tParams: tensor([  5.3593, -17.2572])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3477,Loss 2.927844\n",
      "\tParams: tensor([  5.3593, -17.2573])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3478,Loss 2.927844\n",
      "\tParams: tensor([  5.3593, -17.2574])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3479,Loss 2.927843\n",
      "\tParams: tensor([  5.3594, -17.2575])\n",
      "\tGrad:tensor([-0.0014,  0.0081])\n",
      "Epoch 3480,Loss 2.927843\n",
      "\tParams: tensor([  5.3594, -17.2575])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3481,Loss 2.927842\n",
      "\tParams: tensor([  5.3594, -17.2576])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3482,Loss 2.927840\n",
      "\tParams: tensor([  5.3594, -17.2577])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3483,Loss 2.927842\n",
      "\tParams: tensor([  5.3594, -17.2578])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3484,Loss 2.927839\n",
      "\tParams: tensor([  5.3594, -17.2579])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3485,Loss 2.927838\n",
      "\tParams: tensor([  5.3594, -17.2579])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3486,Loss 2.927839\n",
      "\tParams: tensor([  5.3595, -17.2580])\n",
      "\tGrad:tensor([-0.0014,  0.0080])\n",
      "Epoch 3487,Loss 2.927838\n",
      "\tParams: tensor([  5.3595, -17.2581])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3488,Loss 2.927837\n",
      "\tParams: tensor([  5.3595, -17.2582])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3489,Loss 2.927835\n",
      "\tParams: tensor([  5.3595, -17.2583])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3490,Loss 2.927837\n",
      "\tParams: tensor([  5.3595, -17.2583])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3491,Loss 2.927836\n",
      "\tParams: tensor([  5.3595, -17.2584])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3492,Loss 2.927835\n",
      "\tParams: tensor([  5.3595, -17.2585])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3493,Loss 2.927833\n",
      "\tParams: tensor([  5.3596, -17.2586])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3494,Loss 2.927833\n",
      "\tParams: tensor([  5.3596, -17.2587])\n",
      "\tGrad:tensor([-0.0014,  0.0079])\n",
      "Epoch 3495,Loss 2.927833\n",
      "\tParams: tensor([  5.3596, -17.2587])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3496,Loss 2.927832\n",
      "\tParams: tensor([  5.3596, -17.2588])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3497,Loss 2.927831\n",
      "\tParams: tensor([  5.3596, -17.2589])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3498,Loss 2.927830\n",
      "\tParams: tensor([  5.3596, -17.2590])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3499,Loss 2.927830\n",
      "\tParams: tensor([  5.3596, -17.2590])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3500,Loss 2.927830\n",
      "\tParams: tensor([  5.3597, -17.2591])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3501,Loss 2.927829\n",
      "\tParams: tensor([  5.3597, -17.2592])\n",
      "\tGrad:tensor([-0.0014,  0.0078])\n",
      "Epoch 3502,Loss 2.927828\n",
      "\tParams: tensor([  5.3597, -17.2593])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3503,Loss 2.927828\n",
      "\tParams: tensor([  5.3597, -17.2594])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3504,Loss 2.927827\n",
      "\tParams: tensor([  5.3597, -17.2594])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3505,Loss 2.927825\n",
      "\tParams: tensor([  5.3597, -17.2595])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3506,Loss 2.927827\n",
      "\tParams: tensor([  5.3597, -17.2596])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3507,Loss 2.927825\n",
      "\tParams: tensor([  5.3597, -17.2597])\n",
      "\tGrad:tensor([-0.0014,  0.0077])\n",
      "Epoch 3508,Loss 2.927824\n",
      "\tParams: tensor([  5.3598, -17.2597])\n",
      "\tGrad:tensor([-0.0013,  0.0077])\n",
      "Epoch 3509,Loss 2.927824\n",
      "\tParams: tensor([  5.3598, -17.2598])\n",
      "\tGrad:tensor([-0.0013,  0.0077])\n",
      "Epoch 3510,Loss 2.927824\n",
      "\tParams: tensor([  5.3598, -17.2599])\n",
      "\tGrad:tensor([-0.0013,  0.0076])\n",
      "Epoch 3511,Loss 2.927822\n",
      "\tParams: tensor([  5.3598, -17.2600])\n",
      "\tGrad:tensor([-0.0014,  0.0076])\n",
      "Epoch 3512,Loss 2.927822\n",
      "\tParams: tensor([  5.3598, -17.2600])\n",
      "\tGrad:tensor([-0.0014,  0.0076])\n",
      "Epoch 3513,Loss 2.927821\n",
      "\tParams: tensor([  5.3598, -17.2601])\n",
      "\tGrad:tensor([-0.0013,  0.0076])\n",
      "Epoch 3514,Loss 2.927820\n",
      "\tParams: tensor([  5.3598, -17.2602])\n",
      "\tGrad:tensor([-0.0013,  0.0076])\n",
      "Epoch 3515,Loss 2.927820\n",
      "\tParams: tensor([  5.3599, -17.2603])\n",
      "\tGrad:tensor([-0.0014,  0.0076])\n",
      "Epoch 3516,Loss 2.927821\n",
      "\tParams: tensor([  5.3599, -17.2604])\n",
      "\tGrad:tensor([-0.0014,  0.0076])\n",
      "Epoch 3517,Loss 2.927819\n",
      "\tParams: tensor([  5.3599, -17.2604])\n",
      "\tGrad:tensor([-0.0014,  0.0075])\n",
      "Epoch 3518,Loss 2.927819\n",
      "\tParams: tensor([  5.3599, -17.2605])\n",
      "\tGrad:tensor([-0.0014,  0.0075])\n",
      "Epoch 3519,Loss 2.927819\n",
      "\tParams: tensor([  5.3599, -17.2606])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3520,Loss 2.927817\n",
      "\tParams: tensor([  5.3599, -17.2607])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3521,Loss 2.927817\n",
      "\tParams: tensor([  5.3599, -17.2607])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3522,Loss 2.927816\n",
      "\tParams: tensor([  5.3599, -17.2608])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3523,Loss 2.927815\n",
      "\tParams: tensor([  5.3600, -17.2609])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3524,Loss 2.927816\n",
      "\tParams: tensor([  5.3600, -17.2610])\n",
      "\tGrad:tensor([-0.0013,  0.0075])\n",
      "Epoch 3525,Loss 2.927815\n",
      "\tParams: tensor([  5.3600, -17.2610])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3526,Loss 2.927814\n",
      "\tParams: tensor([  5.3600, -17.2611])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3527,Loss 2.927813\n",
      "\tParams: tensor([  5.3600, -17.2612])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3528,Loss 2.927812\n",
      "\tParams: tensor([  5.3600, -17.2612])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3529,Loss 2.927811\n",
      "\tParams: tensor([  5.3600, -17.2613])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3530,Loss 2.927812\n",
      "\tParams: tensor([  5.3601, -17.2614])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3531,Loss 2.927812\n",
      "\tParams: tensor([  5.3601, -17.2615])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3532,Loss 2.927810\n",
      "\tParams: tensor([  5.3601, -17.2615])\n",
      "\tGrad:tensor([-0.0013,  0.0074])\n",
      "Epoch 3533,Loss 2.927809\n",
      "\tParams: tensor([  5.3601, -17.2616])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3534,Loss 2.927810\n",
      "\tParams: tensor([  5.3601, -17.2617])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3535,Loss 2.927809\n",
      "\tParams: tensor([  5.3601, -17.2618])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3536,Loss 2.927808\n",
      "\tParams: tensor([  5.3601, -17.2618])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3537,Loss 2.927808\n",
      "\tParams: tensor([  5.3601, -17.2619])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3538,Loss 2.927806\n",
      "\tParams: tensor([  5.3602, -17.2620])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3539,Loss 2.927806\n",
      "\tParams: tensor([  5.3602, -17.2621])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3540,Loss 2.927805\n",
      "\tParams: tensor([  5.3602, -17.2621])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3541,Loss 2.927804\n",
      "\tParams: tensor([  5.3602, -17.2622])\n",
      "\tGrad:tensor([-0.0013,  0.0073])\n",
      "Epoch 3542,Loss 2.927805\n",
      "\tParams: tensor([  5.3602, -17.2623])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3543,Loss 2.927804\n",
      "\tParams: tensor([  5.3602, -17.2623])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3544,Loss 2.927805\n",
      "\tParams: tensor([  5.3602, -17.2624])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3545,Loss 2.927804\n",
      "\tParams: tensor([  5.3602, -17.2625])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3546,Loss 2.927804\n",
      "\tParams: tensor([  5.3603, -17.2626])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3547,Loss 2.927803\n",
      "\tParams: tensor([  5.3603, -17.2626])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3548,Loss 2.927802\n",
      "\tParams: tensor([  5.3603, -17.2627])\n",
      "\tGrad:tensor([-0.0013,  0.0072])\n",
      "Epoch 3549,Loss 2.927801\n",
      "\tParams: tensor([  5.3603, -17.2628])\n",
      "\tGrad:tensor([-0.0013,  0.0071])\n",
      "Epoch 3550,Loss 2.927801\n",
      "\tParams: tensor([  5.3603, -17.2628])\n",
      "\tGrad:tensor([-0.0013,  0.0071])\n",
      "Epoch 3551,Loss 2.927799\n",
      "\tParams: tensor([  5.3603, -17.2629])\n",
      "\tGrad:tensor([-0.0012,  0.0071])\n",
      "Epoch 3552,Loss 2.927801\n",
      "\tParams: tensor([  5.3603, -17.2630])\n",
      "\tGrad:tensor([-0.0012,  0.0071])\n",
      "Epoch 3553,Loss 2.927798\n",
      "\tParams: tensor([  5.3603, -17.2631])\n",
      "\tGrad:tensor([-0.0012,  0.0071])\n",
      "Epoch 3554,Loss 2.927798\n",
      "\tParams: tensor([  5.3604, -17.2631])\n",
      "\tGrad:tensor([-0.0012,  0.0071])\n",
      "Epoch 3555,Loss 2.927798\n",
      "\tParams: tensor([  5.3604, -17.2632])\n",
      "\tGrad:tensor([-0.0013,  0.0071])\n",
      "Epoch 3556,Loss 2.927798\n",
      "\tParams: tensor([  5.3604, -17.2633])\n",
      "\tGrad:tensor([-0.0013,  0.0071])\n",
      "Epoch 3557,Loss 2.927798\n",
      "\tParams: tensor([  5.3604, -17.2633])\n",
      "\tGrad:tensor([-0.0013,  0.0071])\n",
      "Epoch 3558,Loss 2.927796\n",
      "\tParams: tensor([  5.3604, -17.2634])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3559,Loss 2.927795\n",
      "\tParams: tensor([  5.3604, -17.2635])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3560,Loss 2.927796\n",
      "\tParams: tensor([  5.3604, -17.2636])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3561,Loss 2.927794\n",
      "\tParams: tensor([  5.3604, -17.2636])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3562,Loss 2.927795\n",
      "\tParams: tensor([  5.3605, -17.2637])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3563,Loss 2.927795\n",
      "\tParams: tensor([  5.3605, -17.2638])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3564,Loss 2.927793\n",
      "\tParams: tensor([  5.3605, -17.2638])\n",
      "\tGrad:tensor([-0.0013,  0.0070])\n",
      "Epoch 3565,Loss 2.927795\n",
      "\tParams: tensor([  5.3605, -17.2639])\n",
      "\tGrad:tensor([-0.0012,  0.0070])\n",
      "Epoch 3566,Loss 2.927791\n",
      "\tParams: tensor([  5.3605, -17.2640])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3567,Loss 2.927791\n",
      "\tParams: tensor([  5.3605, -17.2640])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3568,Loss 2.927791\n",
      "\tParams: tensor([  5.3605, -17.2641])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3569,Loss 2.927790\n",
      "\tParams: tensor([  5.3605, -17.2642])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3570,Loss 2.927790\n",
      "\tParams: tensor([  5.3606, -17.2642])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3571,Loss 2.927789\n",
      "\tParams: tensor([  5.3606, -17.2643])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3572,Loss 2.927790\n",
      "\tParams: tensor([  5.3606, -17.2644])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3573,Loss 2.927789\n",
      "\tParams: tensor([  5.3606, -17.2645])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3574,Loss 2.927789\n",
      "\tParams: tensor([  5.3606, -17.2645])\n",
      "\tGrad:tensor([-0.0012,  0.0069])\n",
      "Epoch 3575,Loss 2.927789\n",
      "\tParams: tensor([  5.3606, -17.2646])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3576,Loss 2.927787\n",
      "\tParams: tensor([  5.3606, -17.2647])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3577,Loss 2.927786\n",
      "\tParams: tensor([  5.3606, -17.2647])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3578,Loss 2.927788\n",
      "\tParams: tensor([  5.3607, -17.2648])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3579,Loss 2.927785\n",
      "\tParams: tensor([  5.3607, -17.2649])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3580,Loss 2.927785\n",
      "\tParams: tensor([  5.3607, -17.2649])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3581,Loss 2.927786\n",
      "\tParams: tensor([  5.3607, -17.2650])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3582,Loss 2.927785\n",
      "\tParams: tensor([  5.3607, -17.2651])\n",
      "\tGrad:tensor([-0.0012,  0.0068])\n",
      "Epoch 3583,Loss 2.927784\n",
      "\tParams: tensor([  5.3607, -17.2651])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3584,Loss 2.927784\n",
      "\tParams: tensor([  5.3607, -17.2652])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3585,Loss 2.927783\n",
      "\tParams: tensor([  5.3607, -17.2653])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3586,Loss 2.927783\n",
      "\tParams: tensor([  5.3607, -17.2653])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3587,Loss 2.927781\n",
      "\tParams: tensor([  5.3608, -17.2654])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3588,Loss 2.927782\n",
      "\tParams: tensor([  5.3608, -17.2655])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3589,Loss 2.927781\n",
      "\tParams: tensor([  5.3608, -17.2655])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3590,Loss 2.927781\n",
      "\tParams: tensor([  5.3608, -17.2656])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3591,Loss 2.927781\n",
      "\tParams: tensor([  5.3608, -17.2657])\n",
      "\tGrad:tensor([-0.0012,  0.0067])\n",
      "Epoch 3592,Loss 2.927780\n",
      "\tParams: tensor([  5.3608, -17.2657])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3593,Loss 2.927780\n",
      "\tParams: tensor([  5.3608, -17.2658])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3594,Loss 2.927778\n",
      "\tParams: tensor([  5.3608, -17.2659])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3595,Loss 2.927779\n",
      "\tParams: tensor([  5.3609, -17.2659])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3596,Loss 2.927778\n",
      "\tParams: tensor([  5.3609, -17.2660])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3597,Loss 2.927778\n",
      "\tParams: tensor([  5.3609, -17.2661])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3598,Loss 2.927779\n",
      "\tParams: tensor([  5.3609, -17.2661])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3599,Loss 2.927777\n",
      "\tParams: tensor([  5.3609, -17.2662])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3600,Loss 2.927776\n",
      "\tParams: tensor([  5.3609, -17.2663])\n",
      "\tGrad:tensor([-0.0012,  0.0066])\n",
      "Epoch 3601,Loss 2.927775\n",
      "\tParams: tensor([  5.3609, -17.2663])\n",
      "\tGrad:tensor([-0.0012,  0.0065])\n",
      "Epoch 3602,Loss 2.927776\n",
      "\tParams: tensor([  5.3609, -17.2664])\n",
      "\tGrad:tensor([-0.0012,  0.0065])\n",
      "Epoch 3603,Loss 2.927773\n",
      "\tParams: tensor([  5.3609, -17.2665])\n",
      "\tGrad:tensor([-0.0012,  0.0065])\n",
      "Epoch 3604,Loss 2.927775\n",
      "\tParams: tensor([  5.3610, -17.2665])\n",
      "\tGrad:tensor([-0.0012,  0.0065])\n",
      "Epoch 3605,Loss 2.927775\n",
      "\tParams: tensor([  5.3610, -17.2666])\n",
      "\tGrad:tensor([-0.0011,  0.0065])\n",
      "Epoch 3606,Loss 2.927775\n",
      "\tParams: tensor([  5.3610, -17.2667])\n",
      "\tGrad:tensor([-0.0011,  0.0065])\n",
      "Epoch 3607,Loss 2.927773\n",
      "\tParams: tensor([  5.3610, -17.2667])\n",
      "\tGrad:tensor([-0.0011,  0.0065])\n",
      "Epoch 3608,Loss 2.927773\n",
      "\tParams: tensor([  5.3610, -17.2668])\n",
      "\tGrad:tensor([-0.0011,  0.0065])\n",
      "Epoch 3609,Loss 2.927773\n",
      "\tParams: tensor([  5.3610, -17.2668])\n",
      "\tGrad:tensor([-0.0011,  0.0065])\n",
      "Epoch 3610,Loss 2.927772\n",
      "\tParams: tensor([  5.3610, -17.2669])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3611,Loss 2.927772\n",
      "\tParams: tensor([  5.3610, -17.2670])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3612,Loss 2.927770\n",
      "\tParams: tensor([  5.3611, -17.2670])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3613,Loss 2.927772\n",
      "\tParams: tensor([  5.3611, -17.2671])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3614,Loss 2.927771\n",
      "\tParams: tensor([  5.3611, -17.2672])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3615,Loss 2.927770\n",
      "\tParams: tensor([  5.3611, -17.2672])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3616,Loss 2.927770\n",
      "\tParams: tensor([  5.3611, -17.2673])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3617,Loss 2.927769\n",
      "\tParams: tensor([  5.3611, -17.2674])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3618,Loss 2.927768\n",
      "\tParams: tensor([  5.3611, -17.2674])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3619,Loss 2.927769\n",
      "\tParams: tensor([  5.3611, -17.2675])\n",
      "\tGrad:tensor([-0.0011,  0.0064])\n",
      "Epoch 3620,Loss 2.927768\n",
      "\tParams: tensor([  5.3611, -17.2675])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3621,Loss 2.927767\n",
      "\tParams: tensor([  5.3612, -17.2676])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3622,Loss 2.927767\n",
      "\tParams: tensor([  5.3612, -17.2677])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3623,Loss 2.927767\n",
      "\tParams: tensor([  5.3612, -17.2677])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3624,Loss 2.927765\n",
      "\tParams: tensor([  5.3612, -17.2678])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3625,Loss 2.927766\n",
      "\tParams: tensor([  5.3612, -17.2679])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3626,Loss 2.927765\n",
      "\tParams: tensor([  5.3612, -17.2679])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3627,Loss 2.927765\n",
      "\tParams: tensor([  5.3612, -17.2680])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3628,Loss 2.927764\n",
      "\tParams: tensor([  5.3612, -17.2681])\n",
      "\tGrad:tensor([-0.0011,  0.0063])\n",
      "Epoch 3629,Loss 2.927764\n",
      "\tParams: tensor([  5.3612, -17.2681])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3630,Loss 2.927764\n",
      "\tParams: tensor([  5.3613, -17.2682])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3631,Loss 2.927762\n",
      "\tParams: tensor([  5.3613, -17.2682])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3632,Loss 2.927763\n",
      "\tParams: tensor([  5.3613, -17.2683])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3633,Loss 2.927763\n",
      "\tParams: tensor([  5.3613, -17.2684])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3634,Loss 2.927762\n",
      "\tParams: tensor([  5.3613, -17.2684])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3635,Loss 2.927761\n",
      "\tParams: tensor([  5.3613, -17.2685])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3636,Loss 2.927762\n",
      "\tParams: tensor([  5.3613, -17.2685])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3637,Loss 2.927759\n",
      "\tParams: tensor([  5.3613, -17.2686])\n",
      "\tGrad:tensor([-0.0011,  0.0062])\n",
      "Epoch 3638,Loss 2.927761\n",
      "\tParams: tensor([  5.3613, -17.2687])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3639,Loss 2.927761\n",
      "\tParams: tensor([  5.3614, -17.2687])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3640,Loss 2.927760\n",
      "\tParams: tensor([  5.3614, -17.2688])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3641,Loss 2.927759\n",
      "\tParams: tensor([  5.3614, -17.2689])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3642,Loss 2.927758\n",
      "\tParams: tensor([  5.3614, -17.2689])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3643,Loss 2.927759\n",
      "\tParams: tensor([  5.3614, -17.2690])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3644,Loss 2.927757\n",
      "\tParams: tensor([  5.3614, -17.2690])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3645,Loss 2.927758\n",
      "\tParams: tensor([  5.3614, -17.2691])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3646,Loss 2.927757\n",
      "\tParams: tensor([  5.3614, -17.2692])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3647,Loss 2.927757\n",
      "\tParams: tensor([  5.3614, -17.2692])\n",
      "\tGrad:tensor([-0.0011,  0.0061])\n",
      "Epoch 3648,Loss 2.927757\n",
      "\tParams: tensor([  5.3614, -17.2693])\n",
      "\tGrad:tensor([-0.0011,  0.0060])\n",
      "Epoch 3649,Loss 2.927756\n",
      "\tParams: tensor([  5.3615, -17.2693])\n",
      "\tGrad:tensor([-0.0011,  0.0060])\n",
      "Epoch 3650,Loss 2.927757\n",
      "\tParams: tensor([  5.3615, -17.2694])\n",
      "\tGrad:tensor([-0.0011,  0.0060])\n",
      "Epoch 3651,Loss 2.927756\n",
      "\tParams: tensor([  5.3615, -17.2695])\n",
      "\tGrad:tensor([-0.0011,  0.0060])\n",
      "Epoch 3652,Loss 2.927756\n",
      "\tParams: tensor([  5.3615, -17.2695])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3653,Loss 2.927755\n",
      "\tParams: tensor([  5.3615, -17.2696])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3654,Loss 2.927755\n",
      "\tParams: tensor([  5.3615, -17.2696])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3655,Loss 2.927754\n",
      "\tParams: tensor([  5.3615, -17.2697])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3656,Loss 2.927754\n",
      "\tParams: tensor([  5.3615, -17.2698])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3657,Loss 2.927755\n",
      "\tParams: tensor([  5.3615, -17.2698])\n",
      "\tGrad:tensor([-0.0010,  0.0060])\n",
      "Epoch 3658,Loss 2.927753\n",
      "\tParams: tensor([  5.3616, -17.2699])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3659,Loss 2.927752\n",
      "\tParams: tensor([  5.3616, -17.2699])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3660,Loss 2.927754\n",
      "\tParams: tensor([  5.3616, -17.2700])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3661,Loss 2.927752\n",
      "\tParams: tensor([  5.3616, -17.2701])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3662,Loss 2.927751\n",
      "\tParams: tensor([  5.3616, -17.2701])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3663,Loss 2.927752\n",
      "\tParams: tensor([  5.3616, -17.2702])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3664,Loss 2.927750\n",
      "\tParams: tensor([  5.3616, -17.2702])\n",
      "\tGrad:tensor([-0.0011,  0.0059])\n",
      "Epoch 3665,Loss 2.927749\n",
      "\tParams: tensor([  5.3616, -17.2703])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3666,Loss 2.927751\n",
      "\tParams: tensor([  5.3616, -17.2703])\n",
      "\tGrad:tensor([-0.0010,  0.0059])\n",
      "Epoch 3667,Loss 2.927750\n",
      "\tParams: tensor([  5.3616, -17.2704])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3668,Loss 2.927750\n",
      "\tParams: tensor([  5.3617, -17.2705])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3669,Loss 2.927747\n",
      "\tParams: tensor([  5.3617, -17.2705])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3670,Loss 2.927749\n",
      "\tParams: tensor([  5.3617, -17.2706])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3671,Loss 2.927747\n",
      "\tParams: tensor([  5.3617, -17.2706])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3672,Loss 2.927748\n",
      "\tParams: tensor([  5.3617, -17.2707])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3673,Loss 2.927748\n",
      "\tParams: tensor([  5.3617, -17.2708])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3674,Loss 2.927747\n",
      "\tParams: tensor([  5.3617, -17.2708])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3675,Loss 2.927747\n",
      "\tParams: tensor([  5.3617, -17.2709])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3676,Loss 2.927748\n",
      "\tParams: tensor([  5.3617, -17.2709])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3677,Loss 2.927747\n",
      "\tParams: tensor([  5.3617, -17.2710])\n",
      "\tGrad:tensor([-0.0010,  0.0058])\n",
      "Epoch 3678,Loss 2.927747\n",
      "\tParams: tensor([  5.3618, -17.2710])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3679,Loss 2.927745\n",
      "\tParams: tensor([  5.3618, -17.2711])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3680,Loss 2.927745\n",
      "\tParams: tensor([  5.3618, -17.2712])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3681,Loss 2.927746\n",
      "\tParams: tensor([  5.3618, -17.2712])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3682,Loss 2.927744\n",
      "\tParams: tensor([  5.3618, -17.2713])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3683,Loss 2.927743\n",
      "\tParams: tensor([  5.3618, -17.2713])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3684,Loss 2.927743\n",
      "\tParams: tensor([  5.3618, -17.2714])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3685,Loss 2.927743\n",
      "\tParams: tensor([  5.3618, -17.2714])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3686,Loss 2.927743\n",
      "\tParams: tensor([  5.3618, -17.2715])\n",
      "\tGrad:tensor([-0.0010,  0.0057])\n",
      "Epoch 3687,Loss 2.927743\n",
      "\tParams: tensor([  5.3618, -17.2716])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3688,Loss 2.927744\n",
      "\tParams: tensor([  5.3619, -17.2716])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3689,Loss 2.927742\n",
      "\tParams: tensor([  5.3619, -17.2717])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3690,Loss 2.927742\n",
      "\tParams: tensor([  5.3619, -17.2717])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3691,Loss 2.927742\n",
      "\tParams: tensor([  5.3619, -17.2718])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3692,Loss 2.927742\n",
      "\tParams: tensor([  5.3619, -17.2718])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3693,Loss 2.927741\n",
      "\tParams: tensor([  5.3619, -17.2719])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3694,Loss 2.927741\n",
      "\tParams: tensor([  5.3619, -17.2719])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3695,Loss 2.927741\n",
      "\tParams: tensor([  5.3619, -17.2720])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3696,Loss 2.927742\n",
      "\tParams: tensor([  5.3619, -17.2721])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3697,Loss 2.927741\n",
      "\tParams: tensor([  5.3619, -17.2721])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3698,Loss 2.927741\n",
      "\tParams: tensor([  5.3620, -17.2722])\n",
      "\tGrad:tensor([-0.0010,  0.0056])\n",
      "Epoch 3699,Loss 2.927740\n",
      "\tParams: tensor([  5.3620, -17.2722])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3700,Loss 2.927739\n",
      "\tParams: tensor([  5.3620, -17.2723])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3701,Loss 2.927738\n",
      "\tParams: tensor([  5.3620, -17.2723])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3702,Loss 2.927738\n",
      "\tParams: tensor([  5.3620, -17.2724])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3703,Loss 2.927737\n",
      "\tParams: tensor([  5.3620, -17.2724])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3704,Loss 2.927737\n",
      "\tParams: tensor([  5.3620, -17.2725])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3705,Loss 2.927738\n",
      "\tParams: tensor([  5.3620, -17.2726])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3706,Loss 2.927737\n",
      "\tParams: tensor([  5.3620, -17.2726])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3707,Loss 2.927736\n",
      "\tParams: tensor([  5.3620, -17.2727])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3708,Loss 2.927737\n",
      "\tParams: tensor([  5.3621, -17.2727])\n",
      "\tGrad:tensor([-0.0010,  0.0055])\n",
      "Epoch 3709,Loss 2.927737\n",
      "\tParams: tensor([  5.3621, -17.2728])\n",
      "\tGrad:tensor([-0.0010,  0.0054])\n",
      "Epoch 3710,Loss 2.927736\n",
      "\tParams: tensor([  5.3621, -17.2728])\n",
      "\tGrad:tensor([-0.0010,  0.0054])\n",
      "Epoch 3711,Loss 2.927734\n",
      "\tParams: tensor([  5.3621, -17.2729])\n",
      "\tGrad:tensor([-0.0010,  0.0054])\n",
      "Epoch 3712,Loss 2.927735\n",
      "\tParams: tensor([  5.3621, -17.2729])\n",
      "\tGrad:tensor([-0.0010,  0.0054])\n",
      "Epoch 3713,Loss 2.927735\n",
      "\tParams: tensor([  5.3621, -17.2730])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3714,Loss 2.927734\n",
      "\tParams: tensor([  5.3621, -17.2730])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3715,Loss 2.927734\n",
      "\tParams: tensor([  5.3621, -17.2731])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3716,Loss 2.927733\n",
      "\tParams: tensor([  5.3621, -17.2732])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3717,Loss 2.927734\n",
      "\tParams: tensor([  5.3621, -17.2732])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3718,Loss 2.927733\n",
      "\tParams: tensor([  5.3622, -17.2733])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3719,Loss 2.927733\n",
      "\tParams: tensor([  5.3622, -17.2733])\n",
      "\tGrad:tensor([-0.0009,  0.0054])\n",
      "Epoch 3720,Loss 2.927733\n",
      "\tParams: tensor([  5.3622, -17.2734])\n",
      "\tGrad:tensor([-0.0010,  0.0053])\n",
      "Epoch 3721,Loss 2.927732\n",
      "\tParams: tensor([  5.3622, -17.2734])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3722,Loss 2.927731\n",
      "\tParams: tensor([  5.3622, -17.2735])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3723,Loss 2.927731\n",
      "\tParams: tensor([  5.3622, -17.2735])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3724,Loss 2.927733\n",
      "\tParams: tensor([  5.3622, -17.2736])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3725,Loss 2.927730\n",
      "\tParams: tensor([  5.3622, -17.2736])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3726,Loss 2.927730\n",
      "\tParams: tensor([  5.3622, -17.2737])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3727,Loss 2.927732\n",
      "\tParams: tensor([  5.3622, -17.2737])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3728,Loss 2.927730\n",
      "\tParams: tensor([  5.3622, -17.2738])\n",
      "\tGrad:tensor([-0.0010,  0.0053])\n",
      "Epoch 3729,Loss 2.927732\n",
      "\tParams: tensor([  5.3623, -17.2738])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3730,Loss 2.927731\n",
      "\tParams: tensor([  5.3623, -17.2739])\n",
      "\tGrad:tensor([-0.0009,  0.0053])\n",
      "Epoch 3731,Loss 2.927730\n",
      "\tParams: tensor([  5.3623, -17.2740])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3732,Loss 2.927728\n",
      "\tParams: tensor([  5.3623, -17.2740])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3733,Loss 2.927729\n",
      "\tParams: tensor([  5.3623, -17.2741])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3734,Loss 2.927729\n",
      "\tParams: tensor([  5.3623, -17.2741])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3735,Loss 2.927728\n",
      "\tParams: tensor([  5.3623, -17.2742])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3736,Loss 2.927728\n",
      "\tParams: tensor([  5.3623, -17.2742])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3737,Loss 2.927728\n",
      "\tParams: tensor([  5.3623, -17.2743])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3738,Loss 2.927728\n",
      "\tParams: tensor([  5.3623, -17.2743])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3739,Loss 2.927727\n",
      "\tParams: tensor([  5.3623, -17.2744])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3740,Loss 2.927728\n",
      "\tParams: tensor([  5.3624, -17.2744])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3741,Loss 2.927728\n",
      "\tParams: tensor([  5.3624, -17.2745])\n",
      "\tGrad:tensor([-0.0009,  0.0052])\n",
      "Epoch 3742,Loss 2.927727\n",
      "\tParams: tensor([  5.3624, -17.2745])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3743,Loss 2.927727\n",
      "\tParams: tensor([  5.3624, -17.2746])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3744,Loss 2.927726\n",
      "\tParams: tensor([  5.3624, -17.2746])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3745,Loss 2.927726\n",
      "\tParams: tensor([  5.3624, -17.2747])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3746,Loss 2.927725\n",
      "\tParams: tensor([  5.3624, -17.2747])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3747,Loss 2.927725\n",
      "\tParams: tensor([  5.3624, -17.2748])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3748,Loss 2.927725\n",
      "\tParams: tensor([  5.3624, -17.2748])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3749,Loss 2.927723\n",
      "\tParams: tensor([  5.3624, -17.2749])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3750,Loss 2.927724\n",
      "\tParams: tensor([  5.3624, -17.2749])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3751,Loss 2.927724\n",
      "\tParams: tensor([  5.3625, -17.2750])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3752,Loss 2.927725\n",
      "\tParams: tensor([  5.3625, -17.2750])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3753,Loss 2.927724\n",
      "\tParams: tensor([  5.3625, -17.2751])\n",
      "\tGrad:tensor([-0.0009,  0.0051])\n",
      "Epoch 3754,Loss 2.927724\n",
      "\tParams: tensor([  5.3625, -17.2751])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3755,Loss 2.927723\n",
      "\tParams: tensor([  5.3625, -17.2752])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3756,Loss 2.927723\n",
      "\tParams: tensor([  5.3625, -17.2752])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3757,Loss 2.927723\n",
      "\tParams: tensor([  5.3625, -17.2753])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3758,Loss 2.927722\n",
      "\tParams: tensor([  5.3625, -17.2753])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3759,Loss 2.927723\n",
      "\tParams: tensor([  5.3625, -17.2754])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3760,Loss 2.927722\n",
      "\tParams: tensor([  5.3625, -17.2754])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3761,Loss 2.927723\n",
      "\tParams: tensor([  5.3625, -17.2755])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3762,Loss 2.927721\n",
      "\tParams: tensor([  5.3626, -17.2755])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3763,Loss 2.927722\n",
      "\tParams: tensor([  5.3626, -17.2756])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3764,Loss 2.927720\n",
      "\tParams: tensor([  5.3626, -17.2756])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3765,Loss 2.927720\n",
      "\tParams: tensor([  5.3626, -17.2757])\n",
      "\tGrad:tensor([-0.0009,  0.0050])\n",
      "Epoch 3766,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2757])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3767,Loss 2.927721\n",
      "\tParams: tensor([  5.3626, -17.2758])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3768,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2758])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3769,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2759])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3770,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2759])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3771,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2760])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3772,Loss 2.927719\n",
      "\tParams: tensor([  5.3626, -17.2760])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3773,Loss 2.927720\n",
      "\tParams: tensor([  5.3626, -17.2761])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3774,Loss 2.927718\n",
      "\tParams: tensor([  5.3627, -17.2761])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3775,Loss 2.927718\n",
      "\tParams: tensor([  5.3627, -17.2762])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3776,Loss 2.927717\n",
      "\tParams: tensor([  5.3627, -17.2762])\n",
      "\tGrad:tensor([-0.0009,  0.0049])\n",
      "Epoch 3777,Loss 2.927718\n",
      "\tParams: tensor([  5.3627, -17.2763])\n",
      "\tGrad:tensor([-0.0008,  0.0049])\n",
      "Epoch 3778,Loss 2.927717\n",
      "\tParams: tensor([  5.3627, -17.2763])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3779,Loss 2.927717\n",
      "\tParams: tensor([  5.3627, -17.2764])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3780,Loss 2.927716\n",
      "\tParams: tensor([  5.3627, -17.2764])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3781,Loss 2.927716\n",
      "\tParams: tensor([  5.3627, -17.2765])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3782,Loss 2.927717\n",
      "\tParams: tensor([  5.3627, -17.2765])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3783,Loss 2.927717\n",
      "\tParams: tensor([  5.3627, -17.2766])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3784,Loss 2.927716\n",
      "\tParams: tensor([  5.3627, -17.2766])\n",
      "\tGrad:tensor([-0.0009,  0.0048])\n",
      "Epoch 3785,Loss 2.927715\n",
      "\tParams: tensor([  5.3627, -17.2767])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3786,Loss 2.927715\n",
      "\tParams: tensor([  5.3628, -17.2767])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3787,Loss 2.927715\n",
      "\tParams: tensor([  5.3628, -17.2767])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3788,Loss 2.927715\n",
      "\tParams: tensor([  5.3628, -17.2768])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3789,Loss 2.927715\n",
      "\tParams: tensor([  5.3628, -17.2768])\n",
      "\tGrad:tensor([-0.0008,  0.0048])\n",
      "Epoch 3790,Loss 2.927715\n",
      "\tParams: tensor([  5.3628, -17.2769])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3791,Loss 2.927714\n",
      "\tParams: tensor([  5.3628, -17.2769])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3792,Loss 2.927714\n",
      "\tParams: tensor([  5.3628, -17.2770])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3793,Loss 2.927714\n",
      "\tParams: tensor([  5.3628, -17.2770])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3794,Loss 2.927714\n",
      "\tParams: tensor([  5.3628, -17.2771])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3795,Loss 2.927713\n",
      "\tParams: tensor([  5.3628, -17.2771])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3796,Loss 2.927714\n",
      "\tParams: tensor([  5.3628, -17.2772])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3797,Loss 2.927713\n",
      "\tParams: tensor([  5.3629, -17.2772])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3798,Loss 2.927712\n",
      "\tParams: tensor([  5.3629, -17.2773])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3799,Loss 2.927712\n",
      "\tParams: tensor([  5.3629, -17.2773])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3800,Loss 2.927713\n",
      "\tParams: tensor([  5.3629, -17.2774])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3801,Loss 2.927711\n",
      "\tParams: tensor([  5.3629, -17.2774])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3802,Loss 2.927712\n",
      "\tParams: tensor([  5.3629, -17.2775])\n",
      "\tGrad:tensor([-0.0008,  0.0047])\n",
      "Epoch 3803,Loss 2.927712\n",
      "\tParams: tensor([  5.3629, -17.2775])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3804,Loss 2.927711\n",
      "\tParams: tensor([  5.3629, -17.2775])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3805,Loss 2.927712\n",
      "\tParams: tensor([  5.3629, -17.2776])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3806,Loss 2.927711\n",
      "\tParams: tensor([  5.3629, -17.2776])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3807,Loss 2.927711\n",
      "\tParams: tensor([  5.3629, -17.2777])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3808,Loss 2.927711\n",
      "\tParams: tensor([  5.3629, -17.2777])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3809,Loss 2.927709\n",
      "\tParams: tensor([  5.3629, -17.2778])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3810,Loss 2.927710\n",
      "\tParams: tensor([  5.3630, -17.2778])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3811,Loss 2.927710\n",
      "\tParams: tensor([  5.3630, -17.2779])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3812,Loss 2.927708\n",
      "\tParams: tensor([  5.3630, -17.2779])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3813,Loss 2.927708\n",
      "\tParams: tensor([  5.3630, -17.2780])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3814,Loss 2.927709\n",
      "\tParams: tensor([  5.3630, -17.2780])\n",
      "\tGrad:tensor([-0.0008,  0.0046])\n",
      "Epoch 3815,Loss 2.927709\n",
      "\tParams: tensor([  5.3630, -17.2781])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3816,Loss 2.927710\n",
      "\tParams: tensor([  5.3630, -17.2781])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3817,Loss 2.927708\n",
      "\tParams: tensor([  5.3630, -17.2781])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3818,Loss 2.927708\n",
      "\tParams: tensor([  5.3630, -17.2782])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3819,Loss 2.927706\n",
      "\tParams: tensor([  5.3630, -17.2782])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3820,Loss 2.927707\n",
      "\tParams: tensor([  5.3630, -17.2783])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3821,Loss 2.927708\n",
      "\tParams: tensor([  5.3630, -17.2783])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3822,Loss 2.927707\n",
      "\tParams: tensor([  5.3631, -17.2784])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3823,Loss 2.927707\n",
      "\tParams: tensor([  5.3631, -17.2784])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3824,Loss 2.927707\n",
      "\tParams: tensor([  5.3631, -17.2785])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3825,Loss 2.927708\n",
      "\tParams: tensor([  5.3631, -17.2785])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3826,Loss 2.927707\n",
      "\tParams: tensor([  5.3631, -17.2786])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3827,Loss 2.927706\n",
      "\tParams: tensor([  5.3631, -17.2786])\n",
      "\tGrad:tensor([-0.0008,  0.0045])\n",
      "Epoch 3828,Loss 2.927707\n",
      "\tParams: tensor([  5.3631, -17.2786])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3829,Loss 2.927705\n",
      "\tParams: tensor([  5.3631, -17.2787])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3830,Loss 2.927706\n",
      "\tParams: tensor([  5.3631, -17.2787])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3831,Loss 2.927706\n",
      "\tParams: tensor([  5.3631, -17.2788])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3832,Loss 2.927705\n",
      "\tParams: tensor([  5.3631, -17.2788])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3833,Loss 2.927705\n",
      "\tParams: tensor([  5.3631, -17.2789])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3834,Loss 2.927705\n",
      "\tParams: tensor([  5.3631, -17.2789])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3835,Loss 2.927705\n",
      "\tParams: tensor([  5.3632, -17.2789])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3836,Loss 2.927705\n",
      "\tParams: tensor([  5.3632, -17.2790])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3837,Loss 2.927705\n",
      "\tParams: tensor([  5.3632, -17.2790])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3838,Loss 2.927704\n",
      "\tParams: tensor([  5.3632, -17.2791])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3839,Loss 2.927704\n",
      "\tParams: tensor([  5.3632, -17.2791])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3840,Loss 2.927704\n",
      "\tParams: tensor([  5.3632, -17.2792])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3841,Loss 2.927703\n",
      "\tParams: tensor([  5.3632, -17.2792])\n",
      "\tGrad:tensor([-0.0008,  0.0044])\n",
      "Epoch 3842,Loss 2.927702\n",
      "\tParams: tensor([  5.3632, -17.2793])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3843,Loss 2.927703\n",
      "\tParams: tensor([  5.3632, -17.2793])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3844,Loss 2.927703\n",
      "\tParams: tensor([  5.3632, -17.2793])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3845,Loss 2.927704\n",
      "\tParams: tensor([  5.3632, -17.2794])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3846,Loss 2.927702\n",
      "\tParams: tensor([  5.3632, -17.2794])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3847,Loss 2.927701\n",
      "\tParams: tensor([  5.3632, -17.2795])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3848,Loss 2.927703\n",
      "\tParams: tensor([  5.3633, -17.2795])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3849,Loss 2.927702\n",
      "\tParams: tensor([  5.3633, -17.2796])\n",
      "\tGrad:tensor([-0.0008,  0.0043])\n",
      "Epoch 3850,Loss 2.927701\n",
      "\tParams: tensor([  5.3633, -17.2796])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3851,Loss 2.927701\n",
      "\tParams: tensor([  5.3633, -17.2796])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3852,Loss 2.927703\n",
      "\tParams: tensor([  5.3633, -17.2797])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3853,Loss 2.927700\n",
      "\tParams: tensor([  5.3633, -17.2797])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3854,Loss 2.927701\n",
      "\tParams: tensor([  5.3633, -17.2798])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3855,Loss 2.927701\n",
      "\tParams: tensor([  5.3633, -17.2798])\n",
      "\tGrad:tensor([-0.0007,  0.0043])\n",
      "Epoch 3856,Loss 2.927700\n",
      "\tParams: tensor([  5.3633, -17.2799])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3857,Loss 2.927700\n",
      "\tParams: tensor([  5.3633, -17.2799])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3858,Loss 2.927700\n",
      "\tParams: tensor([  5.3633, -17.2799])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3859,Loss 2.927701\n",
      "\tParams: tensor([  5.3633, -17.2800])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3860,Loss 2.927699\n",
      "\tParams: tensor([  5.3633, -17.2800])\n",
      "\tGrad:tensor([-0.0008,  0.0042])\n",
      "Epoch 3861,Loss 2.927699\n",
      "\tParams: tensor([  5.3634, -17.2801])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3862,Loss 2.927700\n",
      "\tParams: tensor([  5.3634, -17.2801])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3863,Loss 2.927699\n",
      "\tParams: tensor([  5.3634, -17.2801])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3864,Loss 2.927698\n",
      "\tParams: tensor([  5.3634, -17.2802])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3865,Loss 2.927699\n",
      "\tParams: tensor([  5.3634, -17.2802])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3866,Loss 2.927697\n",
      "\tParams: tensor([  5.3634, -17.2803])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3867,Loss 2.927700\n",
      "\tParams: tensor([  5.3634, -17.2803])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3868,Loss 2.927699\n",
      "\tParams: tensor([  5.3634, -17.2804])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3869,Loss 2.927698\n",
      "\tParams: tensor([  5.3634, -17.2804])\n",
      "\tGrad:tensor([-0.0007,  0.0042])\n",
      "Epoch 3870,Loss 2.927697\n",
      "\tParams: tensor([  5.3634, -17.2804])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3871,Loss 2.927698\n",
      "\tParams: tensor([  5.3634, -17.2805])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3872,Loss 2.927696\n",
      "\tParams: tensor([  5.3634, -17.2805])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3873,Loss 2.927699\n",
      "\tParams: tensor([  5.3634, -17.2806])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3874,Loss 2.927698\n",
      "\tParams: tensor([  5.3634, -17.2806])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3875,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2806])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3876,Loss 2.927698\n",
      "\tParams: tensor([  5.3635, -17.2807])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3877,Loss 2.927697\n",
      "\tParams: tensor([  5.3635, -17.2807])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3878,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2808])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3879,Loss 2.927697\n",
      "\tParams: tensor([  5.3635, -17.2808])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3880,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2808])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3881,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2809])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3882,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2809])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3883,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2810])\n",
      "\tGrad:tensor([-0.0007,  0.0041])\n",
      "Epoch 3884,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2810])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3885,Loss 2.927695\n",
      "\tParams: tensor([  5.3635, -17.2810])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3886,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2811])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3887,Loss 2.927696\n",
      "\tParams: tensor([  5.3635, -17.2811])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3888,Loss 2.927695\n",
      "\tParams: tensor([  5.3635, -17.2812])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3889,Loss 2.927695\n",
      "\tParams: tensor([  5.3636, -17.2812])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3890,Loss 2.927694\n",
      "\tParams: tensor([  5.3636, -17.2812])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3891,Loss 2.927693\n",
      "\tParams: tensor([  5.3636, -17.2813])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3892,Loss 2.927693\n",
      "\tParams: tensor([  5.3636, -17.2813])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3893,Loss 2.927695\n",
      "\tParams: tensor([  5.3636, -17.2814])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3894,Loss 2.927695\n",
      "\tParams: tensor([  5.3636, -17.2814])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3895,Loss 2.927694\n",
      "\tParams: tensor([  5.3636, -17.2815])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3896,Loss 2.927696\n",
      "\tParams: tensor([  5.3636, -17.2815])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3897,Loss 2.927693\n",
      "\tParams: tensor([  5.3636, -17.2815])\n",
      "\tGrad:tensor([-0.0007,  0.0040])\n",
      "Epoch 3898,Loss 2.927693\n",
      "\tParams: tensor([  5.3636, -17.2816])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3899,Loss 2.927694\n",
      "\tParams: tensor([  5.3636, -17.2816])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3900,Loss 2.927693\n",
      "\tParams: tensor([  5.3636, -17.2817])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3901,Loss 2.927692\n",
      "\tParams: tensor([  5.3636, -17.2817])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3902,Loss 2.927694\n",
      "\tParams: tensor([  5.3636, -17.2817])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3903,Loss 2.927692\n",
      "\tParams: tensor([  5.3637, -17.2818])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3904,Loss 2.927693\n",
      "\tParams: tensor([  5.3637, -17.2818])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3905,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2818])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3906,Loss 2.927692\n",
      "\tParams: tensor([  5.3637, -17.2819])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3907,Loss 2.927692\n",
      "\tParams: tensor([  5.3637, -17.2819])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3908,Loss 2.927692\n",
      "\tParams: tensor([  5.3637, -17.2820])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3909,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2820])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3910,Loss 2.927692\n",
      "\tParams: tensor([  5.3637, -17.2820])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3911,Loss 2.927690\n",
      "\tParams: tensor([  5.3637, -17.2821])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3912,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2821])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3913,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2822])\n",
      "\tGrad:tensor([-0.0007,  0.0039])\n",
      "Epoch 3914,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2822])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3915,Loss 2.927690\n",
      "\tParams: tensor([  5.3637, -17.2822])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3916,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2823])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3917,Loss 2.927691\n",
      "\tParams: tensor([  5.3637, -17.2823])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3918,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2823])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3919,Loss 2.927690\n",
      "\tParams: tensor([  5.3638, -17.2824])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3920,Loss 2.927690\n",
      "\tParams: tensor([  5.3638, -17.2824])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3921,Loss 2.927690\n",
      "\tParams: tensor([  5.3638, -17.2825])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3922,Loss 2.927690\n",
      "\tParams: tensor([  5.3638, -17.2825])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3923,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2825])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3924,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2826])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3925,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2826])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3926,Loss 2.927688\n",
      "\tParams: tensor([  5.3638, -17.2826])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3927,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2827])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3928,Loss 2.927689\n",
      "\tParams: tensor([  5.3638, -17.2827])\n",
      "\tGrad:tensor([-0.0007,  0.0038])\n",
      "Epoch 3929,Loss 2.927688\n",
      "\tParams: tensor([  5.3638, -17.2828])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3930,Loss 2.927688\n",
      "\tParams: tensor([  5.3638, -17.2828])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3931,Loss 2.927688\n",
      "\tParams: tensor([  5.3638, -17.2828])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3932,Loss 2.927688\n",
      "\tParams: tensor([  5.3638, -17.2829])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3933,Loss 2.927687\n",
      "\tParams: tensor([  5.3639, -17.2829])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3934,Loss 2.927689\n",
      "\tParams: tensor([  5.3639, -17.2829])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3935,Loss 2.927688\n",
      "\tParams: tensor([  5.3639, -17.2830])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3936,Loss 2.927687\n",
      "\tParams: tensor([  5.3639, -17.2830])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3937,Loss 2.927687\n",
      "\tParams: tensor([  5.3639, -17.2831])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3938,Loss 2.927687\n",
      "\tParams: tensor([  5.3639, -17.2831])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3939,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2831])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3940,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2832])\n",
      "\tGrad:tensor([-0.0007,  0.0037])\n",
      "Epoch 3941,Loss 2.927687\n",
      "\tParams: tensor([  5.3639, -17.2832])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3942,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2832])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3943,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2833])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3944,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2833])\n",
      "\tGrad:tensor([-0.0006,  0.0037])\n",
      "Epoch 3945,Loss 2.927686\n",
      "\tParams: tensor([  5.3639, -17.2833])\n",
      "\tGrad:tensor([-0.0007,  0.0036])\n",
      "Epoch 3946,Loss 2.927685\n",
      "\tParams: tensor([  5.3639, -17.2834])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3947,Loss 2.927685\n",
      "\tParams: tensor([  5.3639, -17.2834])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3948,Loss 2.927686\n",
      "\tParams: tensor([  5.3640, -17.2835])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3949,Loss 2.927685\n",
      "\tParams: tensor([  5.3640, -17.2835])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3950,Loss 2.927686\n",
      "\tParams: tensor([  5.3640, -17.2835])\n",
      "\tGrad:tensor([-0.0007,  0.0036])\n",
      "Epoch 3951,Loss 2.927686\n",
      "\tParams: tensor([  5.3640, -17.2836])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3952,Loss 2.927687\n",
      "\tParams: tensor([  5.3640, -17.2836])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3953,Loss 2.927685\n",
      "\tParams: tensor([  5.3640, -17.2836])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3954,Loss 2.927686\n",
      "\tParams: tensor([  5.3640, -17.2837])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3955,Loss 2.927686\n",
      "\tParams: tensor([  5.3640, -17.2837])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3956,Loss 2.927685\n",
      "\tParams: tensor([  5.3640, -17.2837])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3957,Loss 2.927683\n",
      "\tParams: tensor([  5.3640, -17.2838])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3958,Loss 2.927684\n",
      "\tParams: tensor([  5.3640, -17.2838])\n",
      "\tGrad:tensor([-0.0007,  0.0036])\n",
      "Epoch 3959,Loss 2.927685\n",
      "\tParams: tensor([  5.3640, -17.2839])\n",
      "\tGrad:tensor([-0.0006,  0.0036])\n",
      "Epoch 3960,Loss 2.927684\n",
      "\tParams: tensor([  5.3640, -17.2839])\n",
      "\tGrad:tensor([-0.0007,  0.0036])\n",
      "Epoch 3961,Loss 2.927684\n",
      "\tParams: tensor([  5.3640, -17.2839])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3962,Loss 2.927684\n",
      "\tParams: tensor([  5.3640, -17.2840])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3963,Loss 2.927685\n",
      "\tParams: tensor([  5.3640, -17.2840])\n",
      "\tGrad:tensor([-0.0007,  0.0035])\n",
      "Epoch 3964,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2840])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3965,Loss 2.927685\n",
      "\tParams: tensor([  5.3641, -17.2841])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3966,Loss 2.927685\n",
      "\tParams: tensor([  5.3641, -17.2841])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3967,Loss 2.927684\n",
      "\tParams: tensor([  5.3641, -17.2841])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3968,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2842])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3969,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2842])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3970,Loss 2.927682\n",
      "\tParams: tensor([  5.3641, -17.2842])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3971,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2843])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3972,Loss 2.927684\n",
      "\tParams: tensor([  5.3641, -17.2843])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3973,Loss 2.927682\n",
      "\tParams: tensor([  5.3641, -17.2843])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3974,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2844])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3975,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2844])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3976,Loss 2.927683\n",
      "\tParams: tensor([  5.3641, -17.2844])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3977,Loss 2.927682\n",
      "\tParams: tensor([  5.3641, -17.2845])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3978,Loss 2.927682\n",
      "\tParams: tensor([  5.3641, -17.2845])\n",
      "\tGrad:tensor([-0.0006,  0.0035])\n",
      "Epoch 3979,Loss 2.927682\n",
      "\tParams: tensor([  5.3641, -17.2845])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3980,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2846])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3981,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2846])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3982,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2847])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3983,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2847])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3984,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2847])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3985,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2848])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3986,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2848])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3987,Loss 2.927680\n",
      "\tParams: tensor([  5.3642, -17.2848])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3988,Loss 2.927682\n",
      "\tParams: tensor([  5.3642, -17.2849])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3989,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2849])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3990,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2849])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3991,Loss 2.927680\n",
      "\tParams: tensor([  5.3642, -17.2850])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3992,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2850])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3993,Loss 2.927680\n",
      "\tParams: tensor([  5.3642, -17.2850])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3994,Loss 2.927680\n",
      "\tParams: tensor([  5.3642, -17.2851])\n",
      "\tGrad:tensor([-0.0006,  0.0034])\n",
      "Epoch 3995,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2851])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 3996,Loss 2.927681\n",
      "\tParams: tensor([  5.3642, -17.2851])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 3997,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2852])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 3998,Loss 2.927680\n",
      "\tParams: tensor([  5.3643, -17.2852])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 3999,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2852])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4000,Loss 2.927680\n",
      "\tParams: tensor([  5.3643, -17.2853])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4001,Loss 2.927680\n",
      "\tParams: tensor([  5.3643, -17.2853])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4002,Loss 2.927681\n",
      "\tParams: tensor([  5.3643, -17.2853])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4003,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2854])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4004,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2854])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4005,Loss 2.927680\n",
      "\tParams: tensor([  5.3643, -17.2854])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4006,Loss 2.927680\n",
      "\tParams: tensor([  5.3643, -17.2855])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4007,Loss 2.927677\n",
      "\tParams: tensor([  5.3643, -17.2855])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4008,Loss 2.927678\n",
      "\tParams: tensor([  5.3643, -17.2855])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4009,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2856])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4010,Loss 2.927678\n",
      "\tParams: tensor([  5.3643, -17.2856])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4011,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2856])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4012,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2857])\n",
      "\tGrad:tensor([-0.0006,  0.0033])\n",
      "Epoch 4013,Loss 2.927679\n",
      "\tParams: tensor([  5.3643, -17.2857])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4014,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2857])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4015,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2857])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4016,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2858])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4017,Loss 2.927679\n",
      "\tParams: tensor([  5.3644, -17.2858])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4018,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2858])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4019,Loss 2.927678\n",
      "\tParams: tensor([  5.3644, -17.2859])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4020,Loss 2.927678\n",
      "\tParams: tensor([  5.3644, -17.2859])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4021,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2859])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4022,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2860])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4023,Loss 2.927678\n",
      "\tParams: tensor([  5.3644, -17.2860])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4024,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2860])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4025,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2861])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4026,Loss 2.927676\n",
      "\tParams: tensor([  5.3644, -17.2861])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4027,Loss 2.927676\n",
      "\tParams: tensor([  5.3644, -17.2861])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4028,Loss 2.927675\n",
      "\tParams: tensor([  5.3644, -17.2862])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4029,Loss 2.927677\n",
      "\tParams: tensor([  5.3644, -17.2862])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4030,Loss 2.927674\n",
      "\tParams: tensor([  5.3644, -17.2862])\n",
      "\tGrad:tensor([-0.0006,  0.0032])\n",
      "Epoch 4031,Loss 2.927676\n",
      "\tParams: tensor([  5.3644, -17.2863])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4032,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2863])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4033,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2863])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4034,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2864])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4035,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2864])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4036,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2864])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4037,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2865])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4038,Loss 2.927677\n",
      "\tParams: tensor([  5.3645, -17.2865])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4039,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2865])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4040,Loss 2.927676\n",
      "\tParams: tensor([  5.3645, -17.2865])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4041,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2866])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4042,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2866])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4043,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2866])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4044,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2867])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4045,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2867])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4046,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2867])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4047,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2868])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4048,Loss 2.927674\n",
      "\tParams: tensor([  5.3645, -17.2868])\n",
      "\tGrad:tensor([-0.0006,  0.0031])\n",
      "Epoch 4049,Loss 2.927675\n",
      "\tParams: tensor([  5.3645, -17.2868])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4050,Loss 2.927672\n",
      "\tParams: tensor([  5.3646, -17.2868])\n",
      "\tGrad:tensor([-0.0005,  0.0031])\n",
      "Epoch 4051,Loss 2.927675\n",
      "\tParams: tensor([  5.3646, -17.2869])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4052,Loss 2.927675\n",
      "\tParams: tensor([  5.3646, -17.2869])\n",
      "\tGrad:tensor([-0.0006,  0.0030])\n",
      "Epoch 4053,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2869])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4054,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2870])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4055,Loss 2.927674\n",
      "\tParams: tensor([  5.3646, -17.2870])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4056,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2870])\n",
      "\tGrad:tensor([-0.0006,  0.0030])\n",
      "Epoch 4057,Loss 2.927674\n",
      "\tParams: tensor([  5.3646, -17.2871])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4058,Loss 2.927672\n",
      "\tParams: tensor([  5.3646, -17.2871])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4059,Loss 2.927674\n",
      "\tParams: tensor([  5.3646, -17.2871])\n",
      "\tGrad:tensor([-0.0006,  0.0030])\n",
      "Epoch 4060,Loss 2.927675\n",
      "\tParams: tensor([  5.3646, -17.2872])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4061,Loss 2.927672\n",
      "\tParams: tensor([  5.3646, -17.2872])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4062,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2872])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4063,Loss 2.927675\n",
      "\tParams: tensor([  5.3646, -17.2872])\n",
      "\tGrad:tensor([-0.0006,  0.0030])\n",
      "Epoch 4064,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2873])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4065,Loss 2.927674\n",
      "\tParams: tensor([  5.3646, -17.2873])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4066,Loss 2.927672\n",
      "\tParams: tensor([  5.3646, -17.2873])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4067,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2874])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4068,Loss 2.927673\n",
      "\tParams: tensor([  5.3646, -17.2874])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4069,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2874])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4070,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2875])\n",
      "\tGrad:tensor([-0.0005,  0.0030])\n",
      "Epoch 4071,Loss 2.927673\n",
      "\tParams: tensor([  5.3647, -17.2875])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4072,Loss 2.927673\n",
      "\tParams: tensor([  5.3647, -17.2875])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4073,Loss 2.927671\n",
      "\tParams: tensor([  5.3647, -17.2875])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4074,Loss 2.927673\n",
      "\tParams: tensor([  5.3647, -17.2876])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4075,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2876])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4076,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2876])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4077,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2877])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4078,Loss 2.927670\n",
      "\tParams: tensor([  5.3647, -17.2877])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4079,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2877])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4080,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2877])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4081,Loss 2.927671\n",
      "\tParams: tensor([  5.3647, -17.2878])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4082,Loss 2.927670\n",
      "\tParams: tensor([  5.3647, -17.2878])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4083,Loss 2.927673\n",
      "\tParams: tensor([  5.3647, -17.2878])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4084,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2879])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4085,Loss 2.927670\n",
      "\tParams: tensor([  5.3647, -17.2879])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4086,Loss 2.927670\n",
      "\tParams: tensor([  5.3647, -17.2879])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4087,Loss 2.927670\n",
      "\tParams: tensor([  5.3647, -17.2879])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4088,Loss 2.927672\n",
      "\tParams: tensor([  5.3647, -17.2880])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4089,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2880])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4090,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2880])\n",
      "\tGrad:tensor([-0.0005,  0.0029])\n",
      "Epoch 4091,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2881])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4092,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2881])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4093,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2881])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4094,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2881])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4095,Loss 2.927669\n",
      "\tParams: tensor([  5.3648, -17.2882])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4096,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2882])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4097,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2882])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4098,Loss 2.927671\n",
      "\tParams: tensor([  5.3648, -17.2883])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4099,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2883])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4100,Loss 2.927671\n",
      "\tParams: tensor([  5.3648, -17.2883])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4101,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2883])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4102,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2884])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4103,Loss 2.927671\n",
      "\tParams: tensor([  5.3648, -17.2884])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4104,Loss 2.927669\n",
      "\tParams: tensor([  5.3648, -17.2884])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4105,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2885])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4106,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2885])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4107,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2885])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4108,Loss 2.927670\n",
      "\tParams: tensor([  5.3648, -17.2885])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4109,Loss 2.927668\n",
      "\tParams: tensor([  5.3649, -17.2886])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4110,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2886])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4111,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2886])\n",
      "\tGrad:tensor([-0.0005,  0.0028])\n",
      "Epoch 4112,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2886])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4113,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2887])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4114,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2887])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4115,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2887])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4116,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2887])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4117,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2888])\n",
      "\tGrad:tensor([-0.0004,  0.0027])\n",
      "Epoch 4118,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2888])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4119,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2888])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4120,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2889])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4121,Loss 2.927668\n",
      "\tParams: tensor([  5.3649, -17.2889])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4122,Loss 2.927668\n",
      "\tParams: tensor([  5.3649, -17.2889])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4123,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2889])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4124,Loss 2.927668\n",
      "\tParams: tensor([  5.3649, -17.2890])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4125,Loss 2.927670\n",
      "\tParams: tensor([  5.3649, -17.2890])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4126,Loss 2.927666\n",
      "\tParams: tensor([  5.3649, -17.2890])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4127,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2890])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4128,Loss 2.927668\n",
      "\tParams: tensor([  5.3649, -17.2891])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4129,Loss 2.927669\n",
      "\tParams: tensor([  5.3649, -17.2891])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4130,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2891])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4131,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2892])\n",
      "\tGrad:tensor([-0.0004,  0.0027])\n",
      "Epoch 4132,Loss 2.927668\n",
      "\tParams: tensor([  5.3650, -17.2892])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4133,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2892])\n",
      "\tGrad:tensor([-0.0005,  0.0027])\n",
      "Epoch 4134,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2892])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4135,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2893])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4136,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2893])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4137,Loss 2.927669\n",
      "\tParams: tensor([  5.3650, -17.2893])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4138,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2893])\n",
      "\tGrad:tensor([-0.0004,  0.0026])\n",
      "Epoch 4139,Loss 2.927668\n",
      "\tParams: tensor([  5.3650, -17.2894])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4140,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2894])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4141,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2894])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4142,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2894])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4143,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2895])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4144,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2895])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4145,Loss 2.927666\n",
      "\tParams: tensor([  5.3650, -17.2895])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4146,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2896])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4147,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2896])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4148,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2896])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4149,Loss 2.927667\n",
      "\tParams: tensor([  5.3650, -17.2896])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4150,Loss 2.927665\n",
      "\tParams: tensor([  5.3650, -17.2897])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4151,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2897])\n",
      "\tGrad:tensor([-0.0004,  0.0026])\n",
      "Epoch 4152,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2897])\n",
      "\tGrad:tensor([-0.0004,  0.0026])\n",
      "Epoch 4153,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2897])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4154,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2898])\n",
      "\tGrad:tensor([-0.0005,  0.0026])\n",
      "Epoch 4155,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2898])\n",
      "\tGrad:tensor([-0.0004,  0.0026])\n",
      "Epoch 4156,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2898])\n",
      "\tGrad:tensor([-0.0004,  0.0026])\n",
      "Epoch 4157,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2898])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4158,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2899])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4159,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2899])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4160,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2899])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4161,Loss 2.927664\n",
      "\tParams: tensor([  5.3651, -17.2899])\n",
      "\tGrad:tensor([-0.0005,  0.0025])\n",
      "Epoch 4162,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2900])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4163,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2900])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4164,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2900])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4165,Loss 2.927664\n",
      "\tParams: tensor([  5.3651, -17.2900])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4166,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2901])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4167,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2901])\n",
      "\tGrad:tensor([-0.0005,  0.0025])\n",
      "Epoch 4168,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2901])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4169,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2901])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4170,Loss 2.927664\n",
      "\tParams: tensor([  5.3651, -17.2902])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4171,Loss 2.927665\n",
      "\tParams: tensor([  5.3651, -17.2902])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4172,Loss 2.927666\n",
      "\tParams: tensor([  5.3651, -17.2902])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4173,Loss 2.927663\n",
      "\tParams: tensor([  5.3651, -17.2902])\n",
      "\tGrad:tensor([-0.0005,  0.0025])\n",
      "Epoch 4174,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2903])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4175,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2903])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4176,Loss 2.927665\n",
      "\tParams: tensor([  5.3652, -17.2903])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4177,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2903])\n",
      "\tGrad:tensor([-0.0004,  0.0025])\n",
      "Epoch 4178,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2903])\n",
      "\tGrad:tensor([-0.0005,  0.0025])\n",
      "Epoch 4179,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2904])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4180,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2904])\n",
      "\tGrad:tensor([-0.0005,  0.0024])\n",
      "Epoch 4181,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2904])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4182,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2904])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4183,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2905])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4184,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2905])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4185,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2905])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4186,Loss 2.927662\n",
      "\tParams: tensor([  5.3652, -17.2905])\n",
      "\tGrad:tensor([-0.0005,  0.0024])\n",
      "Epoch 4187,Loss 2.927665\n",
      "\tParams: tensor([  5.3652, -17.2906])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4188,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2906])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4189,Loss 2.927662\n",
      "\tParams: tensor([  5.3652, -17.2906])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4190,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2906])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4191,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2907])\n",
      "\tGrad:tensor([-0.0005,  0.0024])\n",
      "Epoch 4192,Loss 2.927664\n",
      "\tParams: tensor([  5.3652, -17.2907])\n",
      "\tGrad:tensor([-0.0005,  0.0024])\n",
      "Epoch 4193,Loss 2.927662\n",
      "\tParams: tensor([  5.3652, -17.2907])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4194,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2907])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4195,Loss 2.927663\n",
      "\tParams: tensor([  5.3652, -17.2908])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4196,Loss 2.927665\n",
      "\tParams: tensor([  5.3652, -17.2908])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4197,Loss 2.927664\n",
      "\tParams: tensor([  5.3653, -17.2908])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4198,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2908])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4199,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2909])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4200,Loss 2.927664\n",
      "\tParams: tensor([  5.3653, -17.2909])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4201,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2909])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4202,Loss 2.927661\n",
      "\tParams: tensor([  5.3653, -17.2909])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4203,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2910])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4204,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2910])\n",
      "\tGrad:tensor([-0.0004,  0.0024])\n",
      "Epoch 4205,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2910])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4206,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2910])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4207,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2910])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4208,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2911])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4209,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2911])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4210,Loss 2.927664\n",
      "\tParams: tensor([  5.3653, -17.2911])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4211,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2911])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4212,Loss 2.927660\n",
      "\tParams: tensor([  5.3653, -17.2912])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4213,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2912])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4214,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2912])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4215,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2912])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4216,Loss 2.927661\n",
      "\tParams: tensor([  5.3653, -17.2913])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4217,Loss 2.927660\n",
      "\tParams: tensor([  5.3653, -17.2913])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4218,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2913])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4219,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2913])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4220,Loss 2.927663\n",
      "\tParams: tensor([  5.3653, -17.2913])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4221,Loss 2.927662\n",
      "\tParams: tensor([  5.3653, -17.2914])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4222,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2914])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4223,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2914])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4224,Loss 2.927663\n",
      "\tParams: tensor([  5.3654, -17.2914])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4225,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2915])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4226,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2915])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4227,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2915])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4228,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2915])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4229,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2915])\n",
      "\tGrad:tensor([-0.0004,  0.0023])\n",
      "Epoch 4230,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2916])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4231,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2916])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4232,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2916])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4233,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2916])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4234,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2917])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4235,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2917])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4236,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2917])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4237,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2917])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4238,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2918])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4239,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2918])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4240,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2918])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4241,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2918])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4242,Loss 2.927660\n",
      "\tParams: tensor([  5.3654, -17.2918])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4243,Loss 2.927659\n",
      "\tParams: tensor([  5.3654, -17.2919])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4244,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2919])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4245,Loss 2.927661\n",
      "\tParams: tensor([  5.3654, -17.2919])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4246,Loss 2.927662\n",
      "\tParams: tensor([  5.3654, -17.2919])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4247,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2920])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4248,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2920])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4249,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2920])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4250,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2920])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4251,Loss 2.927662\n",
      "\tParams: tensor([  5.3655, -17.2920])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4252,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2921])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4253,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2921])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4254,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2921])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4255,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2921])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4256,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2921])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4257,Loss 2.927661\n",
      "\tParams: tensor([  5.3655, -17.2922])\n",
      "\tGrad:tensor([-0.0004,  0.0022])\n",
      "Epoch 4258,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2922])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4259,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2922])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4260,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2922])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4261,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2922])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4262,Loss 2.927662\n",
      "\tParams: tensor([  5.3655, -17.2923])\n",
      "\tGrad:tensor([-0.0003,  0.0021])\n",
      "Epoch 4263,Loss 2.927658\n",
      "\tParams: tensor([  5.3655, -17.2923])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4264,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2923])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4265,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2923])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4266,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2924])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4267,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2924])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4268,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2924])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4269,Loss 2.927659\n",
      "\tParams: tensor([  5.3655, -17.2924])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4270,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2924])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4271,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2925])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4272,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2925])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4273,Loss 2.927660\n",
      "\tParams: tensor([  5.3655, -17.2925])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4274,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2925])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4275,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2925])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4276,Loss 2.927660\n",
      "\tParams: tensor([  5.3656, -17.2926])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4277,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2926])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4278,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2926])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4279,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2926])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4280,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2926])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4281,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2927])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4282,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2927])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4283,Loss 2.927660\n",
      "\tParams: tensor([  5.3656, -17.2927])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4284,Loss 2.927660\n",
      "\tParams: tensor([  5.3656, -17.2927])\n",
      "\tGrad:tensor([-0.0004,  0.0021])\n",
      "Epoch 4285,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2927])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4286,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2928])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4287,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2928])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4288,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2928])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4289,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2928])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4290,Loss 2.927657\n",
      "\tParams: tensor([  5.3656, -17.2929])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4291,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2929])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4292,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2929])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4293,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2929])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4294,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2929])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4295,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2930])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4296,Loss 2.927657\n",
      "\tParams: tensor([  5.3656, -17.2930])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4297,Loss 2.927657\n",
      "\tParams: tensor([  5.3656, -17.2930])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4298,Loss 2.927657\n",
      "\tParams: tensor([  5.3656, -17.2930])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4299,Loss 2.927658\n",
      "\tParams: tensor([  5.3656, -17.2930])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4300,Loss 2.927659\n",
      "\tParams: tensor([  5.3656, -17.2931])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4301,Loss 2.927660\n",
      "\tParams: tensor([  5.3657, -17.2931])\n",
      "\tGrad:tensor([-0.0004,  0.0020])\n",
      "Epoch 4302,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2931])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4303,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2931])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4304,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2931])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4305,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4306,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4307,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4308,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4309,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4310,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2932])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4311,Loss 2.927660\n",
      "\tParams: tensor([  5.3657, -17.2933])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4312,Loss 2.927659\n",
      "\tParams: tensor([  5.3657, -17.2933])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4313,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2933])\n",
      "\tGrad:tensor([-0.0003,  0.0020])\n",
      "Epoch 4314,Loss 2.927656\n",
      "\tParams: tensor([  5.3657, -17.2933])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4315,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2933])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4316,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2934])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4317,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2934])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4318,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2934])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4319,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2934])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4320,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2934])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4321,Loss 2.927656\n",
      "\tParams: tensor([  5.3657, -17.2935])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4322,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2935])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4323,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2935])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4324,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2935])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4325,Loss 2.927658\n",
      "\tParams: tensor([  5.3657, -17.2935])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4326,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4327,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4328,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4329,Loss 2.927656\n",
      "\tParams: tensor([  5.3657, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4330,Loss 2.927657\n",
      "\tParams: tensor([  5.3657, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4331,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2936])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4332,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2937])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4333,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2937])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4334,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2937])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4335,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2937])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4336,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2937])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4337,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2938])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4338,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2938])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4339,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2938])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4340,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2938])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4341,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2938])\n",
      "\tGrad:tensor([-0.0003,  0.0019])\n",
      "Epoch 4342,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2939])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4343,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2939])\n",
      "\tGrad:tensor([-0.0004,  0.0019])\n",
      "Epoch 4344,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2939])\n",
      "\tGrad:tensor([-0.0004,  0.0018])\n",
      "Epoch 4345,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2939])\n",
      "\tGrad:tensor([-0.0004,  0.0018])\n",
      "Epoch 4346,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2939])\n",
      "\tGrad:tensor([-0.0004,  0.0018])\n",
      "Epoch 4347,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2940])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4348,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2940])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4349,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2940])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4350,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2940])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4351,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2940])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4352,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4353,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4354,Loss 2.927655\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4355,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4356,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4357,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2941])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4358,Loss 2.927657\n",
      "\tParams: tensor([  5.3658, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4359,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4360,Loss 2.927656\n",
      "\tParams: tensor([  5.3658, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4361,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4362,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4363,Loss 2.927657\n",
      "\tParams: tensor([  5.3659, -17.2942])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4364,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4365,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4366,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4367,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4368,Loss 2.927654\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4369,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2943])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4370,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2944])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4371,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2944])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4372,Loss 2.927657\n",
      "\tParams: tensor([  5.3659, -17.2944])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4373,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2944])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4374,Loss 2.927657\n",
      "\tParams: tensor([  5.3659, -17.2944])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4375,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4376,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4377,Loss 2.927654\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4378,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0018])\n",
      "Epoch 4379,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4380,Loss 2.927654\n",
      "\tParams: tensor([  5.3659, -17.2945])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4381,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4382,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4383,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4384,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4385,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4386,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2946])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4387,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4388,Loss 2.927653\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4389,Loss 2.927654\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4390,Loss 2.927654\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4391,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4392,Loss 2.927656\n",
      "\tParams: tensor([  5.3659, -17.2947])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4393,Loss 2.927655\n",
      "\tParams: tensor([  5.3659, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4394,Loss 2.927656\n",
      "\tParams: tensor([  5.3660, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4395,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4396,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4397,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4398,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2948])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4399,Loss 2.927656\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4400,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4401,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4402,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4403,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4404,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2949])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4405,Loss 2.927656\n",
      "\tParams: tensor([  5.3660, -17.2950])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4406,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2950])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4407,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2950])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4408,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2950])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4409,Loss 2.927653\n",
      "\tParams: tensor([  5.3660, -17.2950])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4410,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4411,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0017])\n",
      "Epoch 4412,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4413,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4414,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4415,Loss 2.927653\n",
      "\tParams: tensor([  5.3660, -17.2951])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4416,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4417,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4418,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4419,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4420,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4421,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2952])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4422,Loss 2.927653\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4423,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4424,Loss 2.927653\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4425,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4426,Loss 2.927655\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4427,Loss 2.927654\n",
      "\tParams: tensor([  5.3660, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4428,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2953])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4429,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4430,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4431,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4432,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4433,Loss 2.927655\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4434,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2954])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4435,Loss 2.927655\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4436,Loss 2.927652\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4437,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4438,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4439,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4440,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4441,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2955])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4442,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4443,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0002,  0.0016])\n",
      "Epoch 4444,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4445,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4446,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4447,Loss 2.927652\n",
      "\tParams: tensor([  5.3661, -17.2956])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4448,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4449,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0016])\n",
      "Epoch 4450,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4451,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4452,Loss 2.927651\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4453,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4454,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2957])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4455,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4456,Loss 2.927655\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4457,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4458,Loss 2.927652\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4459,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4460,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2958])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4461,Loss 2.927654\n",
      "\tParams: tensor([  5.3661, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4462,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4463,Loss 2.927652\n",
      "\tParams: tensor([  5.3661, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4464,Loss 2.927653\n",
      "\tParams: tensor([  5.3661, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4465,Loss 2.927654\n",
      "\tParams: tensor([  5.3662, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4466,Loss 2.927654\n",
      "\tParams: tensor([  5.3662, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4467,Loss 2.927654\n",
      "\tParams: tensor([  5.3662, -17.2959])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4468,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4469,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4470,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4471,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4472,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4473,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4474,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2960])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4475,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4476,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4477,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4478,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4479,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4480,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2961])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4481,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4482,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4483,Loss 2.927654\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4484,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4485,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4486,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4487,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2962])\n",
      "\tGrad:tensor([-0.0003,  0.0015])\n",
      "Epoch 4488,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4489,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4490,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4491,Loss 2.927651\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4492,Loss 2.927651\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4493,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2963])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4494,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4495,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4496,Loss 2.927651\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0003,  0.0014])\n",
      "Epoch 4497,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4498,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4499,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4500,Loss 2.927651\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4501,Loss 2.927652\n",
      "\tParams: tensor([  5.3662, -17.2964])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4502,Loss 2.927653\n",
      "\tParams: tensor([  5.3662, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4503,Loss 2.927653\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4504,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4505,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4506,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4507,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4508,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2965])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4509,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4510,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4511,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4512,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4513,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4514,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4515,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4516,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2966])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4517,Loss 2.927653\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4518,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4519,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4520,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4521,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4522,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4523,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2967])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4524,Loss 2.927653\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4525,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4526,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4527,Loss 2.927653\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4528,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4529,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4530,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2968])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4531,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0014])\n",
      "Epoch 4532,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4533,Loss 2.927652\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4534,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4535,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4536,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4537,Loss 2.927653\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4538,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2969])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4539,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4540,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4541,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4542,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4543,Loss 2.927651\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4544,Loss 2.927650\n",
      "\tParams: tensor([  5.3663, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4545,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2970])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4546,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4547,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4548,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4549,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4550,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4551,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4552,Loss 2.927653\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4553,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2971])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4554,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4555,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4556,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4557,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4558,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4559,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4560,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2972])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4561,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4562,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4563,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4564,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4565,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4566,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4567,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4568,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2973])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4569,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4570,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4571,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4572,Loss 2.927649\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4573,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0013])\n",
      "Epoch 4574,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4575,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2974])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4576,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4577,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4578,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4579,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4580,Loss 2.927652\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4581,Loss 2.927649\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4582,Loss 2.927653\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4583,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2975])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4584,Loss 2.927649\n",
      "\tParams: tensor([  5.3664, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4585,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4586,Loss 2.927650\n",
      "\tParams: tensor([  5.3664, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4587,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4588,Loss 2.927651\n",
      "\tParams: tensor([  5.3664, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4589,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4590,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4591,Loss 2.927652\n",
      "\tParams: tensor([  5.3665, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4592,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2976])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4593,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4594,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4595,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4596,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4597,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4598,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4599,Loss 2.927652\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4600,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4601,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2977])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4602,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4603,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4604,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4605,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4606,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4607,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4608,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4609,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4610,Loss 2.927649\n",
      "\tParams: tensor([  5.3665, -17.2978])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4611,Loss 2.927649\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4612,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4613,Loss 2.927649\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4614,Loss 2.927649\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4615,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4616,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4617,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4618,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2979])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4619,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4620,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4621,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4622,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4623,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4624,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4625,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4626,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0012])\n",
      "Epoch 4627,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2980])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4628,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4629,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4630,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4631,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4632,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4633,Loss 2.927651\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4634,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4635,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4636,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2981])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4637,Loss 2.927649\n",
      "\tParams: tensor([  5.3665, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4638,Loss 2.927650\n",
      "\tParams: tensor([  5.3665, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4639,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4640,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4641,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4642,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4643,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4644,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4645,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2982])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4646,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4647,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4648,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4649,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4650,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4651,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4652,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4653,Loss 2.927651\n",
      "\tParams: tensor([  5.3666, -17.2983])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4654,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4655,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4656,Loss 2.927651\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4657,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4658,Loss 2.927651\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4659,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4660,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4661,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4662,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2984])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4663,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4664,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4665,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4666,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4667,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4668,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4669,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4670,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4671,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2985])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4672,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4673,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4674,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4675,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0011])\n",
      "Epoch 4676,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4677,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4678,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4679,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4680,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2986])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4681,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4682,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4683,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4684,Loss 2.927648\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4685,Loss 2.927650\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4686,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4687,Loss 2.927651\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4688,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4689,Loss 2.927649\n",
      "\tParams: tensor([  5.3666, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4690,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4691,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2987])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4692,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4693,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4694,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4695,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4696,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4697,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4698,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4699,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4700,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4701,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2988])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4702,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4703,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4704,Loss 2.927647\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4705,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4706,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4707,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4708,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4709,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4710,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4711,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4712,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2989])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4713,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4714,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4715,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4716,Loss 2.927647\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4717,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4718,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4719,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4720,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4721,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4722,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2990])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4723,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4724,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4725,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4726,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4727,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4728,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4729,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4730,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4731,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4732,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4733,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2991])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4734,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4735,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4736,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4737,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4738,Loss 2.927650\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0010])\n",
      "Epoch 4739,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4740,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4741,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4742,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4743,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2992])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4744,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4745,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4746,Loss 2.927648\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4747,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4748,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4749,Loss 2.927649\n",
      "\tParams: tensor([  5.3667, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4750,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4751,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4752,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4753,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4754,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2993])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4755,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4756,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4757,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4758,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4759,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4760,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4761,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4762,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4763,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4764,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2994])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4765,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4766,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4767,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4768,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4769,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4770,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4771,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4772,Loss 2.927648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4773,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4774,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4775,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2995])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4776,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4777,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4778,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4779,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4780,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4781,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4782,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4783,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4784,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4785,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2996])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4786,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4787,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4788,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4789,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4790,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4791,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4792,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4793,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4794,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4795,Loss 2.927650\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4796,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4797,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2997])\n",
      "\tGrad:tensor([-0.0002,  0.0009])\n",
      "Epoch 4798,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4799,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4800,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4801,Loss 2.927646\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4802,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4803,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0009])\n",
      "Epoch 4804,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4805,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4806,Loss 2.927647\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4807,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4808,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4809,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4810,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2998])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4811,Loss 2.927648\n",
      "\tParams: tensor([  5.3668, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4812,Loss 2.927649\n",
      "\tParams: tensor([  5.3668, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4813,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4814,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4815,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4816,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4817,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4818,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4819,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4820,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4821,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4822,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4823,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.2999])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4824,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4825,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4826,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4827,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4828,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4829,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4830,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4831,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4832,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4833,Loss 2.927646\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4834,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4835,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4836,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3000])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4837,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4838,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4839,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4840,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4841,Loss 2.927650\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4842,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4843,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4844,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4845,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4846,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4847,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4848,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4849,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3001])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4850,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4851,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4852,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4853,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4854,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4855,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4856,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4857,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4858,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4859,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4860,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4861,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4862,Loss 2.927645\n",
      "\tParams: tensor([  5.3669, -17.3002])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4863,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4864,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4865,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4866,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4867,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4868,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4869,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4870,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4871,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4872,Loss 2.927646\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4873,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4874,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4875,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3003])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4876,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4877,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4878,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4879,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4880,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4881,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0008])\n",
      "Epoch 4882,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4883,Loss 2.927648\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4884,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4885,Loss 2.927647\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4886,Loss 2.927649\n",
      "\tParams: tensor([  5.3669, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4887,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4888,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3004])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4889,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4890,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4891,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4892,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4893,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4894,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4895,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4896,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4897,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4898,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4899,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4900,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4901,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3005])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4902,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4903,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4904,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4905,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4906,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4907,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4908,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4909,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4910,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4911,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4912,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4913,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4914,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4915,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3006])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4916,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4917,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4918,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4919,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4920,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4921,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4922,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4923,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4924,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4925,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4926,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4927,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4928,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3007])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4929,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4930,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4931,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4932,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4933,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4934,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4935,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4936,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4937,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4938,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4939,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4940,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4941,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3008])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4942,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4943,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4944,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4945,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4946,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4947,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4948,Loss 2.927649\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4949,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4950,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-9.9361e-05,  6.6355e-04])\n",
      "Epoch 4951,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-9.5367e-05,  6.6292e-04])\n",
      "Epoch 4952,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-9.6858e-05,  6.6188e-04])\n",
      "Epoch 4953,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4954,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4955,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4956,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4957,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4958,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3009])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4959,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-9.8228e-05,  6.5479e-04])\n",
      "Epoch 4960,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4961,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-9.8288e-05,  6.5270e-04])\n",
      "Epoch 4962,Loss 2.927646\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0007])\n",
      "Epoch 4963,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4964,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4965,Loss 2.927647\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4966,Loss 2.927648\n",
      "\tParams: tensor([  5.3670, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4967,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-9.7990e-05,  6.4683e-04])\n",
      "Epoch 4968,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-9.7573e-05,  6.4588e-04])\n",
      "Epoch 4969,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4970,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4971,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4972,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4973,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4974,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4975,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3010])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4976,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4977,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-9.5606e-05,  6.3694e-04])\n",
      "Epoch 4978,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-9.8169e-05,  6.3545e-04])\n",
      "Epoch 4979,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4980,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4981,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4982,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4983,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4984,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4985,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4986,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-9.6440e-05,  6.2808e-04])\n",
      "Epoch 4987,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4988,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4989,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4990,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4991,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4992,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4993,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3011])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4994,Loss 2.927646\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-9.2626e-05,  6.2042e-04])\n",
      "Epoch 4995,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-9.8884e-05,  6.1837e-04])\n",
      "Epoch 4996,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4997,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4998,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 4999,Loss 2.927647\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n",
      "Epoch 5000,Loss 2.927648\n",
      "\tParams: tensor([  5.3671, -17.3012])\n",
      "\tGrad:tensor([-0.0001,  0.0006])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = training_loop(n_epochs = 5000, learning_rate = 1e-2, params = torch.tensor([1.0, 0]), t_u = t_nu, t_c = t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13b4a2-d663-43ca-91f1-ea7f00534495",
   "metadata": {},
   "source": [
    "当我们沿着梯度下降的方向改变参数时，我们的损失减小了，但它不会完全的趋近于零，这意味着迭代次数不足以使其收敛到零，或者数据点完全不在同一条线上。正如我们所预期的，开始的数据不是完全准确的，或者读数中有噪音。\n",
    "\n",
    "接下来绘制数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20addebe-a9c8-4957-9804-f92aae1eb303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x129330520>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3840x2880 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t_p = model(t_nu, *params)\n",
    "fig = plt.figure(dpi=600)\n",
    "plt.xlabel(\"Temperature(F)\")\n",
    "plt.ylabel(\"Temperature(C)\")\n",
    "plt.plot(t_u.numpy(), t_p.detach().numpy())\n",
    "plt.plot(t_u.numpy(), t_c.numpy(), 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a61f60-26ed-4418-8c8b-21253eceb02a",
   "metadata": {},
   "source": [
    "上图实线为线性拟合模型的数据，圆点为输入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded8bf85-abb5-4e12-aada-a47d2f1a2e86",
   "metadata": {},
   "source": [
    "### PyTorch 自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a09c5c-9560-4d23-ad15-d66bb88b95a6",
   "metadata": {},
   "source": [
    "在之前的例子中，我们看到了一个简单的反向传播的例子：通过使用链式规则反向传播导数，我们计算了模型和损失的复合函数关于其内部参数 `w` 和 `b` 的梯度。这里的基本要求是我们处理的所有函数都是可微的。\n",
    "\n",
    "即使我们有一个包含数百万个参数的复杂模型，只要我们的模型是可微的，计算关于参数的损失梯度就是写出导数的解析表达式并计算一次，当然这个方法并不会很有趣。这时 `PyTorch` 张量就会发挥作用。\n",
    "\n",
    "`PyTorch` 可以记住它们从何而来，根据产生它们的操作和父张量，它们可以根据输入自动提供这些操作的导数链。这意味着我们不需要手动推导模型，给定一个前向表达式，无论嵌套如何， `PtTorch` 都会自动提供表达式相对其输入参数的梯度。\n",
    "\n",
    "接下来尝试使用自动求导来替换之前的逻辑，首先初始化一个参数张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d5dbc32-748b-49fa-83fc-7e494d8b0f7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f811f-ba9e-48d4-9a24-a953fb35f69c",
   "metadata": {},
   "source": [
    "注意传入的 `requires_grad=True` ，这个参数告诉 `PyTorch` 跟踪由对 `params` 张量进行操作后产生的整个系谱图。换句话说，任何将 `params` 作为祖先的张量都可以访问从 `params` 到那个张量的函数链。如果说这些函数是可微的，导数的值将自动填充为 `params` 张量的 `grad` 属性。\n",
    "\n",
    "通常情况下，该属性值为 `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f637d30-8e40-4f35-9f6e-205825715f05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad is None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8919fd6-5cd3-44de-a80d-fe42805553ab",
   "metadata": {},
   "source": [
    "我们现在需要做的是从一个 `requires_grad` 为 `True` 的张量开始，调用模型并计算损失，然后反向调用损失张量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd1d480b-498d-47e3-8a44-54b76f0e587a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ae0c4-e96a-42cd-857a-ff49b0387a51",
   "metadata": {},
   "source": [
    "此时， `params` 的 `grad` 属性包含关于 `params` 的每个元素的损失的导数。\n",
    "\n",
    "我们可以有任意数量的 `requires_grad` 为 `True` 的张量和任意组合的函数。在这种情况下， `PyTorch` 将计算整个函数调用链中损失的导数，并将它们的值累加到这些张量的 `grad` 函数中。\n",
    "\n",
    "需要注意的是，调用 `backward()` 方法将导致导数在叶节点上累加。使用梯度进行更新后，我们需要显式的将梯度归零："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8360bcc-3adb-4db8-8307-a69265800d1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcc5dd-d731-4e83-acbc-a84eac0fa825",
   "metadata": {},
   "source": [
    "接下来看看自动求导的训练代码从头到尾的样子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38527ca7-d6d1-4d25-83bc-2d738778f707",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epochs in range(1, n_epochs+1):\n",
    "        if params.grad is not None:\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            params -= learning_rate * params.grad\n",
    "        \n",
    "        if epochs % 500 == 0:\n",
    "            print('Epoch %d, Loss %f'%(epochs, float(loss)))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752202db-21b6-469e-9401-d5feb0f69315",
   "metadata": {},
   "source": [
    "在上面的代码中需要注意的是：\n",
    "\n",
    "    1. 使用 `Python` 的 `with` 语句将更新操作封装在非梯度上下文中，这意味着在 `with` 语句块中， `PyTorch` 自动求导机制将不生效：也就是说不向前向图中添加边。\n",
    "    \n",
    "    2. 我们在适当的地方更新 `params` 张量，这意味着我们使用相同的 `params` 张量，但从中减去更新的值。当使用自动求导时，我们通常避免就地更新，因为 `PyTorch` 的自动求导引擎可能需要我们修改反向传播的值。\n",
    "\n",
    "接下来看看新的训练函数是否有效："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fa2c0c7-ac1c-4ff4-8dad-be76f7f96e68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(n_epochs = 5000, learning_rate = 1e-2, params = torch.tensor([1.0, 0], requires_grad=True), t_u = t_nu, t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af74cd-e3e4-4c8c-ac55-b8201b890088",
   "metadata": {},
   "source": [
    "结果和之前得到的一样。这意味着虽然我们有能力手动计算导数，但我们不再需要这样做了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c88a0-581a-46a1-91d3-49dd1fad8969",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d108e86-6645-4511-a60f-ce5ecb11b47c",
   "metadata": {},
   "source": [
    "在之前的实例代码中，我们使用了批量梯度下降进行优化，这在这个简单例子中运行良好。当然，有一些优化策略和技巧可以帮助收敛，特别是当模型变得复杂时。接下来介绍 `PyTorch` 从用户代码中提取的优化策略，这就避免了我们必须自己更新模型中的每一个参数的繁琐工作。\n",
    "\n",
    "`torch` 模块中有一个 `optim` 子模块，我们可以在其中找到实现不同优化算法的类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67bfb2af-ac30-4121-9fd0-7418adb6e530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80a283-ea3b-4e14-b935-9ee416552572",
   "metadata": {},
   "source": [
    "每个优化器构造函数都接受一个参数列表（ `PyTorch` 张量，通常将 `requires_grad` 设置为 `True` ）作为第一个输入。传递给优化器的所有参数都会被保留在优化器对象中，这样优化器就可以更新它们的值并访问它们的 `grad` 属性。\n",
    "\n",
    "每个优化器都暴露两个方法： `zero_grad()` 和 `step()` ， `zero_grad()` 将在构造函数中传入的张量的 `grad` 属性归零， `step()` 根据特定优化器实现优化策略更新这些参数的值。\n",
    "\n",
    "接下来使用梯度下降优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71345da8-6919-4d43-8deb-f551124b8a1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9.5483e-01, -8.2600e-04], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-5\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7db0b-85c3-43df-abcc-b26bcb6d04eb",
   "metadata": {},
   "source": [
    "`SGD` 是一个随机梯度下降优化。在调用 `backward()` 方法前调用 `zero_grad()` 方法初始化梯度，调用完 `backward()` 方法后再调用 `step()` 方法更新参数。\n",
    "\n",
    "接下来更新循环训练函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b523b12a-17de-4616-9baa-4c64bd0e47c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epochs in range(1, n_epochs+1):\n",
    "        \n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epochs % 500 == 0:\n",
    "            print('Epoch %d, Loss %f'%(epochs, float(loss)))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15c5daed-2f11-487c-9e16-65c634a7147e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "params = training_loop(n_epochs = 5000, optimizer=optimizer, params=params, t_u = t_nu, t_c = t_c)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bc22b8-f20e-4a6a-8f3e-49d74fcb7d3b",
   "metadata": {},
   "source": [
    "同样，我们也得到了和之前的一样的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

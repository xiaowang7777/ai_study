{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25eb1cf",
   "metadata": {},
   "source": [
    "《PyTorch 实战》第六章习题2，加载葡萄酒数据集，并使用适当数量的输入参数创建新的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe867fd0",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f433b",
   "metadata": {},
   "source": [
    "导入葡萄酒数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414a19f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/p1ch4/tabular-wine/winequality-white.csv\",\n",
    "                  skiprows=1,\n",
    "                  delimiter=\";\",\n",
    "                  dtype=np.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ddbd7",
   "metadata": {},
   "source": [
    "拆分出最后一列作为目标结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754b81e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7.  ,  0.27,  0.36, ...,  3.  ,  0.45,  8.8 ],\n",
       "        [ 6.3 ,  0.3 ,  0.34, ...,  3.3 ,  0.49,  9.5 ],\n",
       "        [ 8.1 ,  0.28,  0.4 , ...,  3.26,  0.44, 10.1 ],\n",
       "        ...,\n",
       "        [ 6.5 ,  0.24,  0.19, ...,  2.99,  0.46,  9.4 ],\n",
       "        [ 5.5 ,  0.29,  0.3 , ...,  3.34,  0.38, 12.8 ],\n",
       "        [ 6.  ,  0.21,  0.38, ...,  3.26,  0.32, 11.8 ]], dtype=float32),\n",
       " array([6, 6, 6, ..., 6, 7, 6]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data = data[:, :-1]\n",
    "target_data = np.array(data[:, -1],dtype=int)\n",
    "src_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961eaaa",
   "metadata": {},
   "source": [
    "加载到 `pytorch` 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a515dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " tensor([6, 6, 6,  ..., 6, 7, 6]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data = torch.from_numpy(src_data)\n",
    "target_data = torch.from_numpy(target_data)\n",
    "src_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54af86",
   "metadata": {},
   "source": [
    "对目标进行独热编码，需要先对 `target_data` 升维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9afd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = target_data.unsqueeze(1)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881fab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_target = torch.zeros(target_data.shape[0],10)\n",
    "onehot_target.scatter_(1,target_data,1.0)\n",
    "onehot_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc279",
   "metadata": {},
   "source": [
    "`scatter_(dim, index, src, reduce=None)` 函数执行逻辑：\n",
    "> self[index[ i ] [ j ] [ k ]] [ j ] [ k ] = src [ i ] [ j ] [ k ]  # if dim == 0\n",
    ">\n",
    "> self[ i ][index[ i ][ j ][ k ]][ k ] = src[ i ][ j ][ k ]  # if dim == 1\n",
    ">\n",
    "> self[ i ][ j ][index[ i ][ j ][ k ]] = src[ i ][ j ][ k ]  # if dim == 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8bea3",
   "metadata": {},
   "source": [
    "接下来查看一下输入和输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b6f6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]), torch.Size([4898, 1]), torch.Size([4898, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, target_data.shape, onehot_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b28d0d",
   "metadata": {},
   "source": [
    "接下来对输出进行归一化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9c6c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7208e-01, -8.1761e-02,  2.1326e-01,  ..., -1.2468e+00,\n",
       "         -3.4915e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7996e-02,  ...,  7.3995e-01,\n",
       "          1.3422e-03, -8.2419e-01],\n",
       "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  ...,  4.7505e-01,\n",
       "         -4.3677e-01, -3.3663e-01],\n",
       "        ...,\n",
       "        [-4.2043e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3130e+00,\n",
       "         -2.6153e-01, -9.0545e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0049e+00,\n",
       "         -9.6251e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7505e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data_normal = (src_data - torch.mean(src_data,dim=0))/torch.sqrt(src_data.var(dim=0))\n",
    "src_data_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03388a7",
   "metadata": {},
   "source": [
    "定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317baa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e_epchons, model, loss_fn, optimizer, train_src_data,\n",
    "                  train_target_data, val_src_data, val_target_data):\n",
    "    for i in range(1, e_epchons + 1):\n",
    "        # 训练集处理\n",
    "        train_res = model(train_src_data)\n",
    "        train_loss = loss_fn(train_res, train_target_data)\n",
    "\n",
    "        # 验证集处理\n",
    "        val_res = model(val_src_data)\n",
    "        val_loss = loss_fn(val_res, val_target_data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 1 or i % 100 == 0:\n",
    "            print(f'Epchon: {i}  Train Loss :{train_loss}'\n",
    "                  f' Val Loss :{train_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72f271",
   "metadata": {},
   "source": [
    "拆分数据集为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bb79c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1721,  0.3151,  0.1306,  ...,  0.2102,  0.0890,  0.1509],\n",
       "         [ 1.3571, -0.0818, -0.6957,  ..., -0.9157,  0.0890, -1.0680],\n",
       "         [ 0.4091, -1.4707,  1.0396,  ..., -1.4455, -0.3491,  0.4760],\n",
       "         ...,\n",
       "         [ 0.8831, -0.4786,  1.2875,  ..., -0.7170,  0.7899, -0.7429],\n",
       "         [ 1.7126, -0.6770,  0.6264,  ..., -2.2402, -0.3491, -0.8242],\n",
       "         [ 1.5941,  0.0175,  0.7090,  ..., -0.6508, -0.8749,  0.7197]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[ 8.6000,  0.1600,  0.4900,  ...,  3.1300,  0.5900, 10.5000],\n",
       "         [ 6.4000,  0.2600,  0.2100,  ...,  3.2300,  0.4800,  9.5000],\n",
       "         [ 7.0000,  0.1300,  0.3000,  ...,  3.4700,  0.4200, 10.5000],\n",
       "         ...,\n",
       "         [ 8.2000,  0.3700,  0.2700,  ...,  2.9700,  0.4800, 10.4000],\n",
       "         [ 6.2000,  0.2800,  0.3300,  ...,  3.2400,  0.5000, 12.1000],\n",
       "         [ 6.9000,  0.4400,  0.1800,  ...,  3.2300,  0.4800,  9.1000]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = src_data_normal.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "suffled_indices = torch.randperm(n_samples)\n",
    "train_indices = suffled_indices[:-n_val]\n",
    "val_indices = suffled_indices[-n_val:]\n",
    "\n",
    "train_src_data = src_data_normal[train_indices, :]\n",
    "train_target_data = onehot_target[train_indices, :]\n",
    "val_src_data = src_data[val_indices, :]\n",
    "val_target_data = onehot_target[val_indices, :]\n",
    "\n",
    "train_target_img = target_data[train_indices,:]\n",
    "val_target_img = target_data[val_indices,:]\n",
    "\n",
    "train_src_data, train_target_data, val_src_data, val_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc54e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3919, 11]),\n",
       " torch.Size([3919, 10]),\n",
       " torch.Size([979, 11]),\n",
       " torch.Size([979, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src_data.shape, train_target_data.shape, val_src_data.shape, val_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db3e23",
   "metadata": {},
   "source": [
    "定义模型：\n",
    "    \n",
    "    1. 从整体数据上看输入的特征有 11 个，而输出的特征为一个分数，在这个例子中使用的独热编码把10分分到了一维向量中，因此总体而言是输入了 11 个特征输出了 10 个特征，先使用单个 `nn.Linear` 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62270240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=11, out_features=10, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(11,10)\n",
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e582e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epchon: 1  Train Loss :0.3732781708240509 Val Loss :0.3732781708240509\n",
      "Epchon: 100  Train Loss :0.2577877640724182 Val Loss :0.2577877640724182\n",
      "Epchon: 200  Train Loss :0.19319289922714233 Val Loss :0.19319289922714233\n",
      "Epchon: 300  Train Loss :0.15368379652500153 Val Loss :0.15368379652500153\n",
      "Epchon: 400  Train Loss :0.12793876230716705 Val Loss :0.12793876230716705\n",
      "Epchon: 500  Train Loss :0.11048222333192825 Val Loss :0.11048222333192825\n",
      "Epchon: 600  Train Loss :0.09831090271472931 Val Loss :0.09831090271472931\n",
      "Epchon: 700  Train Loss :0.0896342396736145 Val Loss :0.0896342396736145\n",
      "Epchon: 800  Train Loss :0.08332838863134384 Val Loss :0.08332838863134384\n",
      "Epchon: 900  Train Loss :0.0786643698811531 Val Loss :0.0786643698811531\n",
      "Epchon: 1000  Train Loss :0.0751580148935318 Val Loss :0.0751580148935318\n",
      "Epchon: 1100  Train Loss :0.07248163223266602 Val Loss :0.07248163223266602\n",
      "Epchon: 1200  Train Loss :0.07040974497795105 Val Loss :0.07040974497795105\n",
      "Epchon: 1300  Train Loss :0.06878475099802017 Val Loss :0.06878475099802017\n",
      "Epchon: 1400  Train Loss :0.06749486923217773 Val Loss :0.06749486923217773\n",
      "Epchon: 1500  Train Loss :0.06645972281694412 Val Loss :0.06645972281694412\n",
      "Epchon: 1600  Train Loss :0.06562069803476334 Val Loss :0.06562069803476334\n",
      "Epchon: 1700  Train Loss :0.06493446975946426 Val Loss :0.06493446975946426\n",
      "Epchon: 1800  Train Loss :0.06436864286661148 Val Loss :0.06436864286661148\n",
      "Epchon: 1900  Train Loss :0.06389864534139633 Val Loss :0.06389864534139633\n",
      "Epchon: 2000  Train Loss :0.06350565701723099 Val Loss :0.06350565701723099\n",
      "Epchon: 2100  Train Loss :0.06317508965730667 Val Loss :0.06317508965730667\n",
      "Epchon: 2200  Train Loss :0.0628955289721489 Val Loss :0.0628955289721489\n",
      "Epchon: 2300  Train Loss :0.062657929956913 Val Loss :0.062657929956913\n",
      "Epchon: 2400  Train Loss :0.06245509162545204 Val Loss :0.06245509162545204\n",
      "Epchon: 2500  Train Loss :0.062281202524900436 Val Loss :0.062281202524900436\n",
      "Epchon: 2600  Train Loss :0.06213155388832092 Val Loss :0.06213155388832092\n",
      "Epchon: 2700  Train Loss :0.06200230494141579 Val Loss :0.06200230494141579\n",
      "Epchon: 2800  Train Loss :0.06189030781388283 Val Loss :0.06189030781388283\n",
      "Epchon: 2900  Train Loss :0.06179293990135193 Val Loss :0.06179293990135193\n",
      "Epchon: 3000  Train Loss :0.06170802190899849 Val Loss :0.06170802190899849\n",
      "Epchon: 3100  Train Loss :0.061633750796318054 Val Loss :0.061633750796318054\n",
      "Epchon: 3200  Train Loss :0.06156860291957855 Val Loss :0.06156860291957855\n",
      "Epchon: 3300  Train Loss :0.061511289328336716 Val Loss :0.061511289328336716\n",
      "Epchon: 3400  Train Loss :0.061460722237825394 Val Loss :0.061460722237825394\n",
      "Epchon: 3500  Train Loss :0.06141600012779236 Val Loss :0.06141600012779236\n",
      "Epchon: 3600  Train Loss :0.06137630343437195 Val Loss :0.06137630343437195\n",
      "Epchon: 3700  Train Loss :0.06134098768234253 Val Loss :0.06134098768234253\n",
      "Epchon: 3800  Train Loss :0.06130947917699814 Val Loss :0.06130947917699814\n",
      "Epchon: 3900  Train Loss :0.06128127872943878 Val Loss :0.06128127872943878\n",
      "Epchon: 4000  Train Loss :0.061255961656570435 Val Loss :0.061255961656570435\n",
      "Epchon: 4100  Train Loss :0.06123315542936325 Val Loss :0.06123315542936325\n",
      "Epchon: 4200  Train Loss :0.061212558299303055 Val Loss :0.061212558299303055\n",
      "Epchon: 4300  Train Loss :0.06119389459490776 Val Loss :0.06119389459490776\n",
      "Epchon: 4400  Train Loss :0.06117692589759827 Val Loss :0.06117692589759827\n",
      "Epchon: 4500  Train Loss :0.06116143614053726 Val Loss :0.06116143614053726\n",
      "Epchon: 4600  Train Loss :0.06114726513624191 Val Loss :0.06114726513624191\n",
      "Epchon: 4700  Train Loss :0.06113424524664879 Val Loss :0.06113424524664879\n",
      "Epchon: 4800  Train Loss :0.061122242361307144 Val Loss :0.061122242361307144\n",
      "Epchon: 4900  Train Loss :0.06111113354563713 Val Loss :0.06111113354563713\n",
      "Epchon: 5000  Train Loss :0.061100829392671585 Val Loss :0.061100829392671585\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(params=linear_model.parameters(), lr=1e-2)\n",
    "training_loop(e_epchons=5000,\n",
    "              model=linear_model,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=nn.MSELoss(),\n",
    "              train_target_data=train_target_data,\n",
    "              train_src_data=train_src_data,\n",
    "              val_target_data=val_target_data,\n",
    "              val_src_data=val_src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c0b83b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[ 1.9296e-02,  2.1554e-03,  1.3300e-03,  5.7504e-02,  2.6363e-03,\n",
       "            -4.3672e-03,  4.6035e-03, -8.8152e-02,  1.6266e-02,  4.8714e-03,\n",
       "            -4.1611e-02],\n",
       "           [ 1.3836e-02,  2.2479e-03,  4.1404e-04,  3.7250e-02,  2.1007e-03,\n",
       "             6.9256e-04, -1.3130e-03, -5.6452e-02,  1.1445e-02,  3.5433e-03,\n",
       "            -2.7445e-02],\n",
       "           [ 4.8188e-03,  3.1775e-04,  1.4656e-04,  1.3480e-02,  5.2682e-04,\n",
       "            -1.8245e-03,  2.1897e-03, -2.1231e-02,  4.0268e-03,  9.2488e-04,\n",
       "            -1.0015e-02],\n",
       "           [ 6.3420e-02,  1.1306e-02,  1.6715e-03,  1.5932e-01,  1.0142e-02,\n",
       "             5.9910e-03, -5.1129e-04, -2.4639e-01,  5.0371e-02,  1.3349e-02,\n",
       "            -1.1843e-01],\n",
       "           [ 1.2953e-02,  3.2373e-02, -4.4939e-03, -2.3537e-02,  2.4322e-03,\n",
       "            -1.6669e-02, -8.8622e-03,  1.8363e-02,  5.0166e-03, -2.0306e-03,\n",
       "            -1.0272e-02],\n",
       "           [-1.8727e-03,  8.4702e-02,  6.5577e-03, -4.3964e-02,  5.2286e-03,\n",
       "            -1.7841e-02,  3.1435e-02, -5.6737e-03, -1.5081e-02, -2.2575e-02,\n",
       "            -1.8391e-01],\n",
       "           [-6.4198e-02, -8.9022e-02, -3.6854e-03, -1.1948e-01,  3.8909e-04,\n",
       "             1.0189e-02, -2.3248e-02,  2.3943e-01, -4.8723e-02, -1.1937e-02,\n",
       "             1.4765e-01],\n",
       "           [ 2.8406e-02, -2.6665e-02, -3.1055e-04,  1.0250e-01, -1.1583e-02,\n",
       "             2.4536e-02, -2.3796e-02, -1.3106e-01,  3.5543e-02,  3.3851e-02,\n",
       "             5.7984e-02],\n",
       "           [-1.3514e-02, -8.0910e-03, -1.2312e-03, -1.3275e-02,  7.7619e-04,\n",
       "             4.0139e-03,  1.1173e-02,  3.9801e-02, -6.1482e-03, -3.8268e-03,\n",
       "             6.6809e-02],\n",
       "           [ 3.1205e-02,  6.4599e-03,  3.0913e-03,  8.3603e-02,  2.7738e-03,\n",
       "             2.5959e-03, -4.1014e-03, -1.2921e-01,  2.6217e-02,  7.5418e-03,\n",
       "            -6.4363e-02]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([ 4.0387e-05, -2.0476e-05,  2.4994e-05,  3.7858e-03,  3.3292e-02,\n",
       "            2.9007e-01,  4.5290e-01,  1.8133e-01,  3.7357e-02,  9.9338e-04],\n",
       "          requires_grad=True)],\n",
       "  'lr': 0.01,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbfeeb",
   "metadata": {},
   "source": [
    "然后重新定义模型，是用多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "660a9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epchon: 1  Train Loss :0.16350282728672028 Val Loss :0.16350282728672028\n",
      "Epchon: 100  Train Loss :0.12053274363279343 Val Loss :0.12053274363279343\n",
      "Epchon: 200  Train Loss :0.09907392412424088 Val Loss :0.09907392412424088\n",
      "Epchon: 300  Train Loss :0.08757535368204117 Val Loss :0.08757535368204117\n",
      "Epchon: 400  Train Loss :0.08087292313575745 Val Loss :0.08087292313575745\n",
      "Epchon: 500  Train Loss :0.07663048803806305 Val Loss :0.07663048803806305\n",
      "Epchon: 600  Train Loss :0.07373721152544022 Val Loss :0.07373721152544022\n",
      "Epchon: 700  Train Loss :0.07163814455270767 Val Loss :0.07163814455270767\n",
      "Epchon: 800  Train Loss :0.07004065811634064 Val Loss :0.07004065811634064\n",
      "Epchon: 900  Train Loss :0.06878093630075455 Val Loss :0.06878093630075455\n",
      "Epchon: 1000  Train Loss :0.06776142865419388 Val Loss :0.06776142865419388\n",
      "Epchon: 1100  Train Loss :0.06692034006118774 Val Loss :0.06692034006118774\n",
      "Epchon: 1200  Train Loss :0.066216379404068 Val Loss :0.066216379404068\n",
      "Epchon: 1300  Train Loss :0.06562051177024841 Val Loss :0.06562051177024841\n",
      "Epchon: 1400  Train Loss :0.06511159986257553 Val Loss :0.06511159986257553\n",
      "Epchon: 1500  Train Loss :0.06467372179031372 Val Loss :0.06467372179031372\n",
      "Epchon: 1600  Train Loss :0.06429459154605865 Val Loss :0.06429459154605865\n",
      "Epchon: 1700  Train Loss :0.06396454572677612 Val Loss :0.06396454572677612\n",
      "Epchon: 1800  Train Loss :0.06367582827806473 Val Loss :0.06367582827806473\n",
      "Epchon: 1900  Train Loss :0.06342216581106186 Val Loss :0.06342216581106186\n",
      "Epchon: 2000  Train Loss :0.06319843232631683 Val Loss :0.06319843232631683\n",
      "Epchon: 2100  Train Loss :0.06300037354230881 Val Loss :0.06300037354230881\n",
      "Epchon: 2200  Train Loss :0.06282442808151245 Val Loss :0.06282442808151245\n",
      "Epchon: 2300  Train Loss :0.06266763061285019 Val Loss :0.06266763061285019\n",
      "Epchon: 2400  Train Loss :0.06252746284008026 Val Loss :0.06252746284008026\n",
      "Epchon: 2500  Train Loss :0.06240179017186165 Val Loss :0.06240179017186165\n",
      "Epchon: 2600  Train Loss :0.06228877604007721 Val Loss :0.06228877604007721\n",
      "Epchon: 2700  Train Loss :0.062186866998672485 Val Loss :0.062186866998672485\n",
      "Epchon: 2800  Train Loss :0.06209471821784973 Val Loss :0.06209471821784973\n",
      "Epchon: 2900  Train Loss :0.06201116368174553 Val Loss :0.06201116368174553\n",
      "Epchon: 3000  Train Loss :0.06193520128726959 Val Loss :0.06193520128726959\n",
      "Epchon: 3100  Train Loss :0.06186594441533089 Val Loss :0.06186594441533089\n",
      "Epchon: 3200  Train Loss :0.06180265173316002 Val Loss :0.06180265173316002\n",
      "Epchon: 3300  Train Loss :0.061744626611471176 Val Loss :0.061744626611471176\n",
      "Epchon: 3400  Train Loss :0.06169131398200989 Val Loss :0.06169131398200989\n",
      "Epchon: 3500  Train Loss :0.061642199754714966 Val Loss :0.061642199754714966\n",
      "Epchon: 3600  Train Loss :0.0615968182682991 Val Loss :0.0615968182682991\n",
      "Epchon: 3700  Train Loss :0.06155477836728096 Val Loss :0.06155477836728096\n",
      "Epchon: 3800  Train Loss :0.06151573732495308 Val Loss :0.06151573732495308\n",
      "Epchon: 3900  Train Loss :0.06147938221693039 Val Loss :0.06147938221693039\n",
      "Epchon: 4000  Train Loss :0.0614454410970211 Val Loss :0.0614454410970211\n",
      "Epchon: 4100  Train Loss :0.06141367182135582 Val Loss :0.06141367182135582\n",
      "Epchon: 4200  Train Loss :0.06138385459780693 Val Loss :0.06138385459780693\n",
      "Epchon: 4300  Train Loss :0.06135580316185951 Val Loss :0.06135580316185951\n",
      "Epchon: 4400  Train Loss :0.06132933497428894 Val Loss :0.06132933497428894\n",
      "Epchon: 4500  Train Loss :0.06130431592464447 Val Loss :0.06130431592464447\n",
      "Epchon: 4600  Train Loss :0.06128060072660446 Val Loss :0.06128060072660446\n",
      "Epchon: 4700  Train Loss :0.061258066445589066 Val Loss :0.061258066445589066\n",
      "Epchon: 4800  Train Loss :0.06123661249876022 Val Loss :0.06123661249876022\n",
      "Epchon: 4900  Train Loss :0.06121612340211868 Val Loss :0.06121612340211868\n",
      "Epchon: 5000  Train Loss :0.06119653582572937 Val Loss :0.06119653582572937\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Sequential(nn.Linear(11, 30), nn.Tanh(), nn.Linear(30, 10))\n",
    "optimizer = optim.SGD(params=linear_model.parameters(), lr=1e-2)\n",
    "training_loop(e_epchons=5000,\n",
    "              model=linear_model,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=nn.MSELoss(),\n",
    "              train_target_data=train_target_data,\n",
    "              train_src_data=train_src_data,\n",
    "              val_target_data=val_target_data,\n",
    "              val_src_data=val_src_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

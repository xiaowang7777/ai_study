{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c25eb1cf",
   "metadata": {},
   "source": [
    "《PyTorch 实战》第六章习题2，加载葡萄酒数据集，并使用适当数量的输入参数创建新的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe867fd0",
   "metadata": {},
   "source": [
    "导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "769965bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f433b",
   "metadata": {},
   "source": [
    "导入葡萄酒数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414a19f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.  ,  0.27,  0.36, ...,  0.45,  8.8 ,  6.  ],\n",
       "       [ 6.3 ,  0.3 ,  0.34, ...,  0.49,  9.5 ,  6.  ],\n",
       "       [ 8.1 ,  0.28,  0.4 , ...,  0.44, 10.1 ,  6.  ],\n",
       "       ...,\n",
       "       [ 6.5 ,  0.24,  0.19, ...,  0.46,  9.4 ,  6.  ],\n",
       "       [ 5.5 ,  0.29,  0.3 , ...,  0.38, 12.8 ,  7.  ],\n",
       "       [ 6.  ,  0.21,  0.38, ...,  0.32, 11.8 ,  6.  ]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"data/p1ch4/tabular-wine/winequality-white.csv\",\n",
    "                  skiprows=1,\n",
    "                  delimiter=\";\",\n",
    "                  dtype=np.float32)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ddbd7",
   "metadata": {},
   "source": [
    "拆分出最后一列作为目标结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754b81e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7.  ,  0.27,  0.36, ...,  3.  ,  0.45,  8.8 ],\n",
       "        [ 6.3 ,  0.3 ,  0.34, ...,  3.3 ,  0.49,  9.5 ],\n",
       "        [ 8.1 ,  0.28,  0.4 , ...,  3.26,  0.44, 10.1 ],\n",
       "        ...,\n",
       "        [ 6.5 ,  0.24,  0.19, ...,  2.99,  0.46,  9.4 ],\n",
       "        [ 5.5 ,  0.29,  0.3 , ...,  3.34,  0.38, 12.8 ],\n",
       "        [ 6.  ,  0.21,  0.38, ...,  3.26,  0.32, 11.8 ]], dtype=float32),\n",
       " array([6, 6, 6, ..., 6, 7, 6]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data = data[:, :-1]\n",
    "target_data = np.array(data[:, -1],dtype=int)\n",
    "src_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961eaaa",
   "metadata": {},
   "source": [
    "加载到 `pytorch` 张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a515dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7.0000,  0.2700,  0.3600,  ...,  3.0000,  0.4500,  8.8000],\n",
       "         [ 6.3000,  0.3000,  0.3400,  ...,  3.3000,  0.4900,  9.5000],\n",
       "         [ 8.1000,  0.2800,  0.4000,  ...,  3.2600,  0.4400, 10.1000],\n",
       "         ...,\n",
       "         [ 6.5000,  0.2400,  0.1900,  ...,  2.9900,  0.4600,  9.4000],\n",
       "         [ 5.5000,  0.2900,  0.3000,  ...,  3.3400,  0.3800, 12.8000],\n",
       "         [ 6.0000,  0.2100,  0.3800,  ...,  3.2600,  0.3200, 11.8000]]),\n",
       " tensor([6, 6, 6,  ..., 6, 7, 6]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data = torch.from_numpy(src_data)\n",
    "target_data = torch.from_numpy(target_data)\n",
    "src_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54af86",
   "metadata": {},
   "source": [
    "对目标进行独热编码，需要先对 `target_data` 升维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9afd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6],\n",
       "        [6],\n",
       "        [6],\n",
       "        ...,\n",
       "        [6],\n",
       "        [7],\n",
       "        [6]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = target_data.unsqueeze(1)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "881fab3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_target = torch.zeros(target_data.shape[0],10)\n",
    "onehot_target.scatter_(1,target_data,1.0)\n",
    "onehot_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccccc279",
   "metadata": {},
   "source": [
    "`scatter_(dim, index, src, reduce=None)` 函数执行逻辑：\n",
    "> self[index[ i ] [ j ] [ k ]] [ j ] [ k ] = src [ i ] [ j ] [ k ]  # if dim == 0\n",
    ">\n",
    "> self[ i ][index[ i ][ j ][ k ]][ k ] = src[ i ][ j ][ k ]  # if dim == 1\n",
    ">\n",
    "> self[ i ][ j ][index[ i ][ j ][ k ]] = src[ i ][ j ][ k ]  # if dim == 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8bea3",
   "metadata": {},
   "source": [
    "接下来查看一下输入和输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b6f6e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4898, 11]), torch.Size([4898, 1]), torch.Size([4898, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data.shape, target_data.shape, onehot_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b28d0d",
   "metadata": {},
   "source": [
    "接下来对输出进行归一化操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b9c6c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7208e-01, -8.1761e-02,  2.1326e-01,  ..., -1.2468e+00,\n",
       "         -3.4915e-01, -1.3930e+00],\n",
       "        [-6.5743e-01,  2.1587e-01,  4.7996e-02,  ...,  7.3995e-01,\n",
       "          1.3422e-03, -8.2419e-01],\n",
       "        [ 1.4756e+00,  1.7450e-02,  5.4378e-01,  ...,  4.7505e-01,\n",
       "         -4.3677e-01, -3.3663e-01],\n",
       "        ...,\n",
       "        [-4.2043e-01, -3.7940e-01, -1.1915e+00,  ..., -1.3130e+00,\n",
       "         -2.6153e-01, -9.0545e-01],\n",
       "        [-1.6054e+00,  1.1666e-01, -2.8253e-01,  ...,  1.0049e+00,\n",
       "         -9.6251e-01,  1.8574e+00],\n",
       "        [-1.0129e+00, -6.7703e-01,  3.7852e-01,  ...,  4.7505e-01,\n",
       "         -1.4882e+00,  1.0448e+00]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_data_normal = (src_data - torch.mean(src_data,dim=0))/torch.sqrt(src_data.var(dim=0))\n",
    "src_data_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03388a7",
   "metadata": {},
   "source": [
    "定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317baa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(e_epchons, model, loss_fn, optimizer, train_src_data,\n",
    "                  train_target_data, val_src_data, val_target_data):\n",
    "    for i in range(1, e_epchons + 1):\n",
    "        # 训练集处理\n",
    "        train_res = model(train_src_data)\n",
    "        train_loss = loss_fn(train_res, train_target_data)\n",
    "\n",
    "        # 验证集处理\n",
    "        val_res = model(val_src_data)\n",
    "        val_loss = loss_fn(val_res, val_target_data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i == 1 or i % 100 == 0:\n",
    "            print(f'Epchon: {i}  Train Loss :{train_loss}'\n",
    "                  f' Val Loss :{val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72f271",
   "metadata": {},
   "source": [
    "拆分数据集为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bb79c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.3019,  0.0175, -0.8609,  ..., -0.4521,  0.1766, -1.3930],\n",
       "         [ 0.8831,  0.0671, -0.1173,  ..., -1.2468, -0.3491, -1.0680],\n",
       "         [ 0.0536, -0.8755, -0.1999,  ..., -1.7104,  0.2642, -1.1492],\n",
       "         ...,\n",
       "         [ 1.5941, -0.1810,  0.8743,  ..., -0.3196,  1.1404, -0.2554],\n",
       "         [-0.4204,  0.8111, -1.4394,  ...,  0.0115, -0.8749, -1.6368],\n",
       "         [ 2.1866, -1.2723, -0.2825,  ...,  0.2102, -0.9625, -0.5804]]),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([[ 6.6000,  0.2200,  0.3700,  ...,  3.3700,  0.5500, 10.3000],\n",
       "         [ 6.5000,  0.4600,  0.2400,  ...,  3.0800,  0.5600,  9.8000],\n",
       "         [ 6.5000,  0.2800,  0.3300,  ...,  3.2200,  0.5100,  9.7000],\n",
       "         ...,\n",
       "         [ 5.8000,  0.3600,  0.2600,  ...,  3.3400,  0.5500, 11.3000],\n",
       "         [ 6.0000,  0.2000,  0.2600,  ...,  3.1400,  0.3800, 11.5000],\n",
       "         [ 6.3000,  0.2600,  0.4900,  ...,  2.9900,  0.6100,  9.8000]]),\n",
       " tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = src_data_normal.shape[0]\n",
    "n_val = int(0.2 * n_samples)\n",
    "suffled_indices = torch.randperm(n_samples)\n",
    "train_indices = suffled_indices[:-n_val]\n",
    "val_indices = suffled_indices[-n_val:]\n",
    "\n",
    "train_src_data = src_data_normal[train_indices, :]\n",
    "train_target_data = onehot_target[train_indices, :]\n",
    "val_src_data = src_data[val_indices, :]\n",
    "val_target_data = onehot_target[val_indices, :]\n",
    "\n",
    "train_target_img = target_data[train_indices,:]\n",
    "val_target_img = target_data[val_indices,:]\n",
    "\n",
    "train_src_data, train_target_data, val_src_data, val_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc54e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3919, 11]),\n",
       " torch.Size([3919, 10]),\n",
       " torch.Size([979, 11]),\n",
       " torch.Size([979, 10]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src_data.shape, train_target_data.shape, val_src_data.shape, val_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db3e23",
   "metadata": {},
   "source": [
    "定义模型：\n",
    "    \n",
    "    1. 从整体数据上看输入的特征有 11 个，而输出的特征为一个分数，在这个例子中使用的独热编码把10分分到了一维向量中，因此总体而言是输入了 11 个特征输出了 10 个特征，先使用单个 `nn.Linear` 进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62270240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=11, out_features=10, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model = nn.Linear(11,10)\n",
    "linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e582e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epchon: 1  Train Loss :0.43926993012428284 Val Loss :711.9766845703125\n",
      "Epchon: 100  Train Loss :0.2930648624897003 Val Loss :571.354248046875\n",
      "Epchon: 200  Train Loss :0.2123730331659317 Val Loss :457.7567443847656\n",
      "Epchon: 300  Train Loss :0.16375622153282166 Val Loss :367.0937805175781\n",
      "Epchon: 400  Train Loss :0.1326463222503662 Val Loss :296.0086975097656\n",
      "Epchon: 500  Train Loss :0.11201494932174683 Val Loss :240.7967071533203\n",
      "Epchon: 600  Train Loss :0.09800571948289871 Val Loss :197.9016876220703\n",
      "Epchon: 700  Train Loss :0.08831840008497238 Val Loss :164.34112548828125\n",
      "Epchon: 800  Train Loss :0.08151186257600784 Val Loss :137.79823303222656\n",
      "Epchon: 900  Train Loss :0.07665643841028214 Val Loss :116.54292297363281\n",
      "Epchon: 1000  Train Loss :0.07314075529575348 Val Loss :99.30558013916016\n",
      "Epchon: 1100  Train Loss :0.07055693119764328 Val Loss :85.15858459472656\n",
      "Epchon: 1200  Train Loss :0.06862953305244446 Val Loss :73.4212875366211\n",
      "Epchon: 1300  Train Loss :0.06717044860124588 Val Loss :63.58953094482422\n",
      "Epchon: 1400  Train Loss :0.06604983657598495 Val Loss :55.28538513183594\n",
      "Epchon: 1500  Train Loss :0.06517709791660309 Val Loss :48.22157669067383\n",
      "Epchon: 1600  Train Loss :0.06448830664157867 Val Loss :42.17658233642578\n",
      "Epchon: 1700  Train Loss :0.06393788009881973 Val Loss :36.97713088989258\n",
      "Epchon: 1800  Train Loss :0.06349290907382965 Val Loss :32.48577880859375\n",
      "Epchon: 1900  Train Loss :0.06312935799360275 Val Loss :28.592153549194336\n",
      "Epchon: 2000  Train Loss :0.06282946467399597 Val Loss :25.20650863647461\n",
      "Epchon: 2100  Train Loss :0.06257995218038559 Val Loss :22.255104064941406\n",
      "Epchon: 2200  Train Loss :0.06237073987722397 Val Loss :19.67676544189453\n",
      "Epchon: 2300  Train Loss :0.062194112688302994 Val Loss :17.420330047607422\n",
      "Epchon: 2400  Train Loss :0.06204407662153244 Val Loss :15.442651748657227\n",
      "Epchon: 2500  Train Loss :0.06191593036055565 Val Loss :13.707139015197754\n",
      "Epchon: 2600  Train Loss :0.061805952340364456 Val Loss :12.182549476623535\n",
      "Epchon: 2700  Train Loss :0.061711136251688004 Val Loss :10.842100143432617\n",
      "Epchon: 2800  Train Loss :0.061629075556993484 Val Loss :9.662705421447754\n",
      "Epchon: 2900  Train Loss :0.061557795852422714 Val Loss :8.624422073364258\n",
      "Epchon: 3000  Train Loss :0.06149565428495407 Val Loss :7.709948539733887\n",
      "Epchon: 3100  Train Loss :0.0614413246512413 Val Loss :6.90422248840332\n",
      "Epchon: 3200  Train Loss :0.06139366701245308 Val Loss :6.194127559661865\n",
      "Epchon: 3300  Train Loss :0.061351753771305084 Val Loss :5.5681891441345215\n",
      "Epchon: 3400  Train Loss :0.06131478026509285 Val Loss :5.01637077331543\n",
      "Epchon: 3500  Train Loss :0.06128208339214325 Val Loss :4.529867649078369\n",
      "Epchon: 3600  Train Loss :0.06125307455658913 Val Loss :4.10095739364624\n",
      "Epchon: 3700  Train Loss :0.061227280646562576 Val Loss :3.722841501235962\n",
      "Epchon: 3800  Train Loss :0.06120428442955017 Val Loss :3.389545202255249\n",
      "Epchon: 3900  Train Loss :0.061183709651231766 Val Loss :3.095804214477539\n",
      "Epchon: 4000  Train Loss :0.06116526201367378 Val Loss :2.836982488632202\n",
      "Epchon: 4100  Train Loss :0.06114867329597473 Val Loss :2.6089882850646973\n",
      "Epchon: 4200  Train Loss :0.06113370507955551 Val Loss :2.4082088470458984\n",
      "Epchon: 4300  Train Loss :0.0611201673746109 Val Loss :2.2314629554748535\n",
      "Epchon: 4400  Train Loss :0.06110787391662598 Val Loss :2.0759356021881104\n",
      "Epchon: 4500  Train Loss :0.0610966794192791 Val Loss :1.939141869544983\n",
      "Epchon: 4600  Train Loss :0.06108644977211952 Val Loss :1.8188860416412354\n",
      "Epchon: 4700  Train Loss :0.06107708066701889 Val Loss :1.7132288217544556\n",
      "Epchon: 4800  Train Loss :0.061068449169397354 Val Loss :1.6204530000686646\n",
      "Epchon: 4900  Train Loss :0.06106048449873924 Val Loss :1.53904390335083\n",
      "Epchon: 5000  Train Loss :0.06105312332510948 Val Loss :1.4676614999771118\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(params=linear_model.parameters(), lr=1e-2)\n",
    "training_loop(e_epchons=5000,\n",
    "              model=linear_model,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=nn.MSELoss(),\n",
    "              train_target_data=train_target_data,\n",
    "              train_src_data=train_src_data,\n",
    "              val_target_data=val_target_data,\n",
    "              val_src_data=val_src_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c0b83b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'params': [Parameter containing:\n",
       "   tensor([[-2.3885e-02, -4.5027e-03, -2.0844e-03, -6.7103e-02, -2.5406e-03,\n",
       "            -2.1463e-04,  1.3225e-03,  1.0413e-01, -2.0044e-02, -6.3890e-03,\n",
       "             5.2217e-02],\n",
       "           [-4.2957e-03,  1.9599e-03, -4.8351e-04, -1.9511e-02, -4.5820e-04,\n",
       "             1.0259e-02, -1.2614e-02,  3.1502e-02, -3.6593e-03, -5.1320e-04,\n",
       "             1.2296e-02],\n",
       "           [-3.9516e-03,  5.8391e-04, -4.2394e-04, -1.5416e-02, -4.0370e-04,\n",
       "             5.0048e-03, -6.0408e-03,  2.4518e-02, -3.2661e-03, -9.3686e-04,\n",
       "             1.0606e-02],\n",
       "           [-1.4989e-02, -1.6429e-03, -2.5878e-03, -5.2741e-02,  4.1427e-04,\n",
       "             1.4428e-03,  6.2988e-03,  7.9819e-02, -1.4691e-02, -6.6840e-03,\n",
       "             4.3272e-02],\n",
       "           [ 4.3082e-02,  3.4047e-02, -3.3426e-03,  6.4325e-02,  6.9212e-03,\n",
       "            -2.0240e-02, -5.2955e-03, -1.1992e-01,  3.0313e-02,  4.9883e-03,\n",
       "            -8.0951e-02],\n",
       "           [-3.3940e-02,  7.7269e-02,  3.4240e-03, -1.2078e-01, -5.7052e-04,\n",
       "            -2.1781e-02,  3.4103e-02,  1.1992e-01, -4.0527e-02, -2.3492e-02,\n",
       "            -1.2502e-01],\n",
       "           [-7.0270e-03, -8.4660e-02,  2.3108e-03,  2.4205e-02,  8.0801e-03,\n",
       "             4.2031e-04,  1.8046e-03,  3.5928e-03, -4.3737e-03,  6.4486e-04,\n",
       "             4.2564e-02],\n",
       "           [ 5.5792e-02, -2.6360e-02, -2.6625e-03,  1.8637e-01, -3.7075e-03,\n",
       "             1.1349e-02, -1.5344e-02, -2.4828e-01,  5.9308e-02,  3.1924e-02,\n",
       "             1.0147e-02],\n",
       "           [ 3.4864e-02,  5.7062e-03,  3.3823e-03,  1.1716e-01,  6.0835e-03,\n",
       "             1.3625e-02, -4.9158e-03, -1.6203e-01,  3.4543e-02,  9.7799e-03,\n",
       "            -3.6721e-02],\n",
       "           [ 7.8845e-03, -1.2480e-03,  8.2799e-04,  2.4229e-02,  1.2958e-03,\n",
       "            -7.5604e-03,  9.4622e-03, -3.6130e-02,  6.9147e-03,  7.4470e-04,\n",
       "            -1.1885e-02]], requires_grad=True),\n",
       "   Parameter containing:\n",
       "   tensor([4.4405e-05, 1.9890e-04, 8.9395e-05, 4.5597e-03, 3.1598e-02, 2.9882e-01,\n",
       "           4.4674e-01, 1.7970e-01, 3.6936e-02, 1.1329e-03], requires_grad=True)],\n",
       "  'lr': 0.01,\n",
       "  'momentum': 0,\n",
       "  'dampening': 0,\n",
       "  'weight_decay': 0,\n",
       "  'nesterov': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'differentiable': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.param_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbfeeb",
   "metadata": {},
   "source": [
    "然后重新定义模型，是用多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660a9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epchon: 1  Train Loss :0.1685260683298111 Val Loss :0.38332706689834595\n",
      "Epchon: 100  Train Loss :0.12446407228708267 Val Loss :0.3292483985424042\n",
      "Epchon: 200  Train Loss :0.10116351395845413 Val Loss :0.29695621132850647\n",
      "Epchon: 300  Train Loss :0.08816388249397278 Val Loss :0.27459096908569336\n",
      "Epchon: 400  Train Loss :0.0804595947265625 Val Loss :0.25797101855278015\n",
      "Epchon: 500  Train Loss :0.0756184458732605 Val Loss :0.24513232707977295\n",
      "Epchon: 600  Train Loss :0.07240107655525208 Val Loss :0.23493944108486176\n",
      "Epchon: 700  Train Loss :0.07015135884284973 Val Loss :0.2266739457845688\n",
      "Epchon: 800  Train Loss :0.06850852072238922 Val Loss :0.2198581099510193\n",
      "Epchon: 900  Train Loss :0.06726600974798203 Val Loss :0.2141614705324173\n",
      "Epchon: 1000  Train Loss :0.06630011647939682 Val Loss :0.2093481868505478\n",
      "Epchon: 1100  Train Loss :0.06553322821855545 Val Loss :0.20524461567401886\n",
      "Epchon: 1200  Train Loss :0.06491425633430481 Val Loss :0.20171941816806793\n",
      "Epchon: 1300  Train Loss :0.06440818309783936 Val Loss :0.1986708790063858\n",
      "Epchon: 1400  Train Loss :0.0639900267124176 Val Loss :0.19601866602897644\n",
      "Epchon: 1500  Train Loss :0.06364142149686813 Val Loss :0.19369852542877197\n",
      "Epchon: 1600  Train Loss :0.06334855407476425 Val Loss :0.1916583925485611\n",
      "Epchon: 1700  Train Loss :0.06310079246759415 Val Loss :0.1898556500673294\n",
      "Epchon: 1800  Train Loss :0.06288984417915344 Val Loss :0.18825547397136688\n",
      "Epchon: 1900  Train Loss :0.06270917505025864 Val Loss :0.18682919442653656\n",
      "Epchon: 2000  Train Loss :0.06255354732275009 Val Loss :0.18555289506912231\n",
      "Epchon: 2100  Train Loss :0.06241877004504204 Val Loss :0.18440674245357513\n",
      "Epchon: 2200  Train Loss :0.06230144947767258 Val Loss :0.18337403237819672\n",
      "Epchon: 2300  Train Loss :0.062198806554079056 Val Loss :0.18244077265262604\n",
      "Epchon: 2400  Train Loss :0.06210857629776001 Val Loss :0.18159490823745728\n",
      "Epchon: 2500  Train Loss :0.062028881162405014 Val Loss :0.18082627654075623\n",
      "Epchon: 2600  Train Loss :0.06195816397666931 Val Loss :0.1801259070634842\n",
      "Epchon: 2700  Train Loss :0.061895135790109634 Val Loss :0.1794862151145935\n",
      "Epchon: 2800  Train Loss :0.061838727444410324 Val Loss :0.17890062928199768\n",
      "Epchon: 2900  Train Loss :0.06178802251815796 Val Loss :0.1783633977174759\n",
      "Epchon: 3000  Train Loss :0.06174226850271225 Val Loss :0.17786960303783417\n",
      "Epchon: 3100  Train Loss :0.06170079484581947 Val Loss :0.17741484940052032\n",
      "Epchon: 3200  Train Loss :0.061663076281547546 Val Loss :0.17699550092220306\n",
      "Epchon: 3300  Train Loss :0.06162863224744797 Val Loss :0.17660823464393616\n",
      "Epchon: 3400  Train Loss :0.0615970604121685 Val Loss :0.17625023424625397\n",
      "Epchon: 3500  Train Loss :0.06156802922487259 Val Loss :0.17591901123523712\n",
      "Epchon: 3600  Train Loss :0.06154122203588486 Val Loss :0.17561252415180206\n",
      "Epchon: 3700  Train Loss :0.06151639670133591 Val Loss :0.17532899975776672\n",
      "Epchon: 3800  Train Loss :0.061493322253227234 Val Loss :0.1750665158033371\n",
      "Epchon: 3900  Train Loss :0.06147180497646332 Val Loss :0.17482413351535797\n",
      "Epchon: 4000  Train Loss :0.061451684683561325 Val Loss :0.1746005266904831\n",
      "Epchon: 4100  Train Loss :0.06143280118703842 Val Loss :0.174394428730011\n",
      "Epchon: 4200  Train Loss :0.06141502782702446 Val Loss :0.17420509457588196\n",
      "Epchon: 4300  Train Loss :0.06139826029539108 Val Loss :0.17403186857700348\n",
      "Epchon: 4400  Train Loss :0.06138238683342934 Val Loss :0.1738736480474472\n",
      "Epchon: 4500  Train Loss :0.06136731430888176 Val Loss :0.17373022437095642\n",
      "Epchon: 4600  Train Loss :0.06135297566652298 Val Loss :0.17360065877437592\n",
      "Epchon: 4700  Train Loss :0.06133929640054703 Val Loss :0.1734844148159027\n",
      "Epchon: 4800  Train Loss :0.06132621690630913 Val Loss :0.17338114976882935\n",
      "Epchon: 4900  Train Loss :0.061313677579164505 Val Loss :0.17329023778438568\n",
      "Epchon: 5000  Train Loss :0.061301637440919876 Val Loss :0.1732112318277359\n"
     ]
    }
   ],
   "source": [
    "linear_model = nn.Sequential(nn.Linear(11, 30), nn.Tanh(), nn.Linear(30, 10))\n",
    "optimizer = optim.SGD(params=linear_model.parameters(), lr=1e-2)\n",
    "training_loop(e_epchons=5000,\n",
    "              model=linear_model,\n",
    "              optimizer=optimizer,\n",
    "              loss_fn=nn.MSELoss(),\n",
    "              train_target_data=train_target_data,\n",
    "              train_src_data=train_src_data,\n",
    "              val_target_data=val_target_data,\n",
    "              val_src_data=val_src_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
